<!DOCTYPE html>
<html lang="en-us">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <meta http-equiv="cache-control" content="max-age=0" />
    <meta http-equiv="cache-control" content="no-cache" />
    <meta http-equiv="expires" content="0" />
    <meta http-equiv="expires" content="Tue, 01 Jan 1980 1:00:00 GMT" />
    <meta http-equiv="pragma" content="no-cache" />

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Gowun+Batang&family=Noto+Sans+KR:wght@100..900&display=swap" rel="stylesheet">

    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"}>
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">

    <meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff" />
    <meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1b" />

    <meta name="description" content="비지도 학습">

    <link rel="alternate" type="application/rss+xml" href="//localhost:1313/tags/%EB%B9%84%EC%A7%80%EB%8F%84-%ED%95%99%EC%8A%B5/index.xml" title="평범한 개발자" />
    
        <title>비지도 학습 | 평범한 개발자</title>
    

    
    <style>
        :root {
          --background: #ffffff;
        }
        @media (prefers-color-scheme: dark) {
          :root {
            --background: #1b1b1b;
          }
        }
        html {
            background-color: var(--background);
        }
        body {
            background-color: var(--background);
        }
    </style>

    
    <link rel="stylesheet" type="text/css" href="/style.min.c8c448788222a35adf7ac27ff97b4517910e918cfc75b2ba3ebd473830b7202c.css" media="all">

    
    
    <link rel="stylesheet" href="//localhost:1313/css/sidenote.edbf363f8c0a6c846181a432f684e56035bc25fc39d7702f1999c4127115857e.css" integrity="sha256-7b82P4wKbIRhgaQy9oTlYDW8Jfw513AvGZnEEnEVhX4=">
  </head>

  <body>
        
        <nav>
          <ul class="menu">
            
                <li><a tabindex="-1" class="menu-link" href="/">글 목록</a></li>
            
                <li><a tabindex="-1" class="menu-link" href="/tags">태그</a></li>
            
                <li><a tabindex="-1" class="menu-link" href="/about">소개</a></li>
            
          </ul>
        </nav>
        


<h1>비지도 학습</h1>
<ul class="post-list">
    

    <li>

    <div style="display: grid; grid-template-columns: 7em auto">
      <span class="date">May 6, 2019</span>
      
      <a href="/posts/%EB%B9%84%EC%A7%80%EB%8F%84-%ED%95%99%EC%8A%B5-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98%EA%B3%BC-%EC%A7%80%EB%8F%84-%ED%95%99%EC%8A%B5/">비지도 학습 알고리즘과 지도 학습</a>
      
    </div>

</li>



    
  </ul>


<br>

<footer>

<script defer>
  document.addEventListener("keydown", function (e) {
    if (document.activeElement.isContentEditable) {
      return false;
    }
    if (document.activeElement.tagName == "INPUT") {
      return false;
    }
    if (e.altKey || e.ctrlKey || e.shiftKey) {
      return false;
    }
    var key = e.key;
    if (key === "h") {
      e.preventDefault();
      e.stopPropagation();
      window.location.href = "/";
    } else if (key === "t") {
      e.preventDefault();
      e.stopPropagation();
      window.location.href = `https://${location.hostname}/tags`;
    } else if (key === "i") {
      e.preventDefault();
      e.stopPropagation();
      const inputs = document.querySelectorAll("input");
      for (let i = 0; i < inputs.length; i++) {
        if (inputs[i].offsetParent !== null) {
          inputs[i].selectionStart = inputs[i].selectionEnd =
            inputs[i].value.length;
          inputs[i].focus();
          break;
        }
      }
    }
    return false;
  });
</script>


<script defer>
  function throttle(fn, wait) {
    var time = Date.now();
    return function () {
      var now = Date.now()
      if (time + wait - now < 0) {
        fn();
        time = now;
      }
    };
  }

  function scrollHandler() {
    const anchors = Array.from(document.querySelectorAll("body h2, body h3"));

    function scrollCallback() {
      var scrollTop = window.pageYOffset || document.documentElement.scrollTop;

      
      for (var i = 0; i < anchors.length; i++) {
        var anchorId = anchors[i].getAttribute("id");
        var link = document.querySelector(
          'nav ul li a[href="#' + anchorId + '"]',
        );
        if (link) {
          link.classList.remove("active-toc");
        }
      }

      
      for (var i = anchors.length - 1; i >= 0; i--) {
        var offsetTop = anchors[i].offsetTop;
        if (scrollTop > offsetTop - 75) {
          var anchorId = anchors[i].getAttribute("id");
          var link = document.querySelector(
            'nav ul li a[href="#' + anchorId + '"]',
          );
          if (link) {
            link.classList.add("active-toc");
            break;
          }
        }
      }
    }

    window.addEventListener(
      "scroll",
      throttle(scrollCallback, 300),
    );
  }
  setTimeout(scrollHandler, 100);
</script>

<script defer>
  function addCopyButtonToCodeBlocks() {
    
    const codeBlocks = document.querySelectorAll('code[class^="language-"]');

    codeBlocks.forEach((codeBlock) => {
      const copyButton = document.createElement("button");
      copyButton.classList.add("copy-code-button");
      copyButton.innerHTML = "copy";

      
      copyButton.addEventListener("click", () => {
        
        const elements = codeBlock.querySelectorAll(".cl");
        let codeToCopy = "";
        elements.forEach((element) => {
          codeToCopy += element.innerText;
        });
        navigator.clipboard.writeText(codeToCopy);

        
        copyButton.innerHTML = "copied!";
        setTimeout(() => {
          copyButton.innerHTML = "copy";
        }, 1500);
      });

      
      codeBlock.parentNode.before(copyButton);
    });
  }
  setTimeout(function () {
    addCopyButtonToCodeBlocks();
  }, 100);
</script>

<script>
window.store = {
    
    "\/\/localhost:1313\/tags\/emacs\/": {
        "title": "Emacs",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/emacs\/"
    },
    
    "\/\/localhost:1313\/tags\/open-source\/": {
        "title": "Open-Source",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/open-source\/"
    },
    
    "\/\/localhost:1313\/posts\/": {
        "title": "Posts",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/posts\/"
    },
    
    "\/\/localhost:1313\/tags\/": {
        "title": "Tags",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/"
    },
    
    "\/\/localhost:1313\/posts\/%EC%9D%B4%EB%A7%A5%EC%8A%A4-%EB%A9%94%EC%9D%B8%ED%85%8C%EC%9D%B4%EB%84%88-%EC%82%AC%EC%9E%84\/": {
        "title": "이맥스 메인테이너 사임",
        "tags": ["emacs","open-source",],
        "content": "CC Mode 얼마 전 이맥스 커뮤니티를 뜨겁게 달군 메일 이 있다. cc-mode cc-mode: C와 C++을 지원하는 이맥스 레거시 패키지로 오랫동안 유지보수 되어왔다. 로 유명한 메인테이너 Alan Mackenzie 가 개발팀에서 나가겠다고 메일링 리스트를 통해 선언한 것이다.\nTree-sitter vs. Legacy Package 최근 이맥스는 tree-sitter Syntax Parsing Library로써 런타임에 코드를 파싱하여 syntax를 구조적으로 액세스할 수 있도록 인터페이스를 제공한다. 를 적극적으로 사용하도록 코드를 수정하고 있다. 하지만 문제는 tree-sitter 이전에 작성된 레거시 패키지들을 사용하지 못하게 되는 경우다. 개발팀은 이를 위해 major-mode-remap-defaults 라는 변수로써 사용자가 원하는 경우 레거시 패키지를 사용할 수 있도록 하고 있다.\n참고로 이맥스는 파일 확장자를 모드 패키지에 매핑시키는 구조로 되어 있다. auto-mode-alist 변수를 살펴보면 어떠한 모드 패키지가 실행될 것인지 확장자에 따라 정의되어 있다.\n;; ... 생략 (\u0026#34;\\\\.lex\\\\\u0026#39;\u0026#34; . c-mode) (\u0026#34;\\\\.y\\\\(acc\\\\)?\\\\\u0026#39;\u0026#34; . c-mode) (\u0026#34;\\\\.h\\\\\u0026#39;\u0026#34; . c-or-c++-mode) (\u0026#34;\\\\.c\\\\\u0026#39;\u0026#34; . c-mode) (\u0026#34;\\\\.\\\\(CC?\\\\|HH?\\\\)\\\\\u0026#39;\u0026#34; . c++-mode) (\u0026#34;\\\\.[ch]\\\\(pp\\\\|xx\\\\|\\\\+\\\\+\\\\)\\\\\u0026#39;\u0026#34; . c++-mode) (\u0026#34;\\\\.\\\\(cc\\\\|hh\\\\)\\\\\u0026#39;\u0026#34; . c++-mode) ;; ... 생략 해당 메인테이너에 따르면, cc-mode의 주요 심볼인 c-mode, c++-mode, c-or-c++-mode는 오래전부터 사용자들에게 사용되어온 심볼임에도 불구하고 아무런 공지 없이 개발팀이 tree-sitter 지원을 위한 목적으로 변경되었다는 것이다. 패키지 내부에는 아래와 같이 major-mode-remap-defaults를 이용해 cc-mode를 사용하고자 하는 사용자들을 위해 임시로 수정 사항을 적용한 것을 볼 수 있는데 제3자 입장에서 봐도 화가 날 만하다.\n(when (boundp \u0026#39;major-mode-remap-defaults) (add-to-list \u0026#39;major-mode-remap-defaults \u0026#39;(c++-mode . c++-ts-mode)) (add-to-list \u0026#39;major-mode-remap-defaults \u0026#39;(c-mode . c-ts-mode)) (add-to-list \u0026#39;major-mode-remap-defaults \u0026#39;(c-or-c++-mode . c-or-c++-ts-mode)) (let (entry) (dolist (mode \u0026#39;(c-mode c++-mode c-or-c++-mode)) (if (and (setq entry (assq mode major-mode-remap-defaults)) (null (cdr entry))) (setq major-mode-remap-defaults (delq entry major-mode-remap-defaults))) (push (cons mode nil) major-mode-remap-defaults)))) 커뮤니티 반응 해당 메인테이너의 메일로 인해 커뮤니티에서는 주요 변경이 있을 때마다 논의하는 시간을 갖자는 의견이 나왔다. \u0026lsquo;결국 사람이 일하는 것인데\u0026hellip;\u0026rsquo;. 해당 의견은 이상적으로만 느껴졌다. 이에 리처드 스톨만은 적극적으로 판단하고 결정하기보다 커뮤니티의 분위기를 다소 온화한 방향으로 이끄는데만 주력했다. 독단적이지만 체계/논리적인 개발 프로세스를 구축한 리눅스 커널의 리누즈와 비교되는 부분이었다.\n비현실적 고찰 오픈소스 프로젝트는 사람들을 이끌고 그들로부터 자발적인 협업을 이끌어낸다. 비지니스 영역은 잘 모르지만 그들이 중요하게 여기는 빠른 결정과 리더쉽이 이 곳에도 동일하게 적용된다. 온화한 성격은 리더쉽이 약하다고 오해되기도 하는데 여기에 이유가 있지 않나 싶다. 빠른 결정을 내릴 수 있으면서도 독단적으로 보이지 않을 수 있는 정치력, 그리고 구성원들이 자발적으로 참여하도록 만들기 위한 기술적 역량이 프로젝트 리더에 정말 중요한 것이라 생각한다.\n생각해보니\u0026hellip; 그런 사람이 있을까?\n", 
        "url": "\/\/localhost:1313\/posts\/%EC%9D%B4%EB%A7%A5%EC%8A%A4-%EB%A9%94%EC%9D%B8%ED%85%8C%EC%9D%B4%EB%84%88-%EC%82%AC%EC%9E%84\/"
    },
    
    "\/\/localhost:1313\/": {
        "title": "평범한 개발자",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/"
    },
    
    "\/\/localhost:1313\/categories\/": {
        "title": "Categories",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/categories\/"
    },
    
    "\/\/localhost:1313\/categories\/emacs\/": {
        "title": "Emacs",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/categories\/emacs\/"
    },
    
    "\/\/localhost:1313\/tags\/tree-sitter\/": {
        "title": "Tree-Sitter",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/tree-sitter\/"
    },
    
    "\/\/localhost:1313\/posts\/%EC%9D%B4%EB%A7%A5%EC%8A%A4-tree-sitter-major-mode-%EA%B5%AC%ED%98%84\/": {
        "title": "이맥스 Tree-sitter Major Mode 구현",
        "tags": ["tree-sitter","emacs",],
        "content": "이 글에서는 이맥스에서 bitbake tree-sitter parser를 사용하기 위해 major mode를 구현하는 방법을 소개한다.\nSyntax Tree Generator 출현 Tree-sitter1 는 소스 코드를 syntax tree로 만드는 \u0026lsquo;parser generator tool\u0026rsquo;이다. Emacs, Neovim, Zed 등의 편집기에 기본으로 builtin 되어 제공된다. 편집기에서 지원하는 건 별개이지만 2018년에 처음 릴리즈 되었다고 하니 벌써 6년이나 된 기능이다. 처음 tree-sitter 출현을 접했을 때 들었던 생각은 \u0026lsquo;대체 왜 만든걸까?\u0026rsquo; 였다. 이미 syntax highlighting 기능은 오래 전부터 제공되던 기능이었기에 실시간으로 소스 코드로부터 syntax tree를 만들 수 있다는 게 어떤 장점을 가질지 예상하기 힘들었다.\n레거시 방식 Emacs는 버퍼 확장자에 따라 언어 지원을 위한 major mode가 로드된다. 예를 들어, C언어(*.c, *.h)는 c-mode 패키지가 자동으로 로드되며 해당 패키지를 통해 syntax highlighting 기능이 제공된다. C-h v auto-mode-alist 명령어를 통해 auto-mode-alist를 확인하면 아래와 같이 c-mode가 할당된 확장자를 확인할 수 있다.\n(\u0026#34;\\\\.ii\\\\\u0026#39;\u0026#34; . c++-mode) (\u0026#34;\\\\.i\\\\\u0026#39;\u0026#34; . c-mode) (\u0026#34;\\\\.lex\\\\\u0026#39;\u0026#34; . c-mode) (\u0026#34;\\\\.y\\\\(acc\\\\)?\\\\\u0026#39;\u0026#34; . c-mode) (\u0026#34;\\\\.h\\\\\u0026#39;\u0026#34; . c-or-c++-mode) (\u0026#34;\\\\.c\\\\\u0026#39;\u0026#34; . c-mode) (\u0026#34;\\\\.\\\\(CC?\\\\|HH?\\\\)\\\\\u0026#39;\u0026#34; . c++-mode) (\u0026#34;\\\\.[ch]\\\\(pp\\\\|xx\\\\|\\\\+\\\\+\\\\)\\\\\u0026#39;\u0026#34; . c++-mode) (\u0026#34;\\\\.\\\\(cc\\\\|hh\\\\)\\\\\u0026#39;\u0026#34; . c++-mode) 그렇다면 이러한 Major Mode 패키지가 있는데도 불구하고 tree-sitter를 사용하는 이유는 무엇일까? 훨씬 빠르기 때문이다. tree-sitter 이전까지 이맥스 패키지 대부분은 syntax highlighting을 위해 정규식을 사용했다. cc-mode 코드를 살펴보면 아래와 같이 아름다운 정규식으로 짜여져 있는 것을 볼 수 있다.\n(defconst c-or-c++-mode--regexp (eval-when-compile (let ((id \u0026#34;[a-zA-Z_][a-zA-Z0-9_]*\u0026#34;) (ws \u0026#34;[ \\t]+\u0026#34;) (ws-maybe \u0026#34;[ \\t]*\u0026#34;) (headers \u0026#39;(\u0026#34;string\u0026#34; \u0026#34;string_view\u0026#34; \u0026#34;iostream\u0026#34; \u0026#34;map\u0026#34; \u0026#34;unordered_map\u0026#34; \u0026#34;set\u0026#34; \u0026#34;unordered_set\u0026#34; \u0026#34;vector\u0026#34; \u0026#34;tuple\u0026#34;))) (concat \u0026#34;^\u0026#34; ws-maybe \u0026#34;\\\\(?:\u0026#34; \u0026#34;using\u0026#34; ws \u0026#34;\\\\(?:namespace\u0026#34; ws \u0026#34;\\\\|\u0026#34; id \u0026#34;::\u0026#34; \u0026#34;\\\\|\u0026#34; id ws-maybe \u0026#34;=\\\\)\u0026#34; \u0026#34;\\\\|\u0026#34; \u0026#34;\\\\(?:inline\u0026#34; ws \u0026#34;\\\\)?namespace\u0026#34; \u0026#34;\\\\(:?\u0026#34; ws \u0026#34;\\\\(?:\u0026#34; id \u0026#34;::\\\\)*\u0026#34; id \u0026#34;\\\\)?\u0026#34; ws-maybe \u0026#34;{\u0026#34; \u0026#34;\\\\|\u0026#34; \u0026#34;class\u0026#34; ws id \u0026#34;\\\\(?:\u0026#34; ws \u0026#34;final\u0026#34; \u0026#34;\\\\)?\u0026#34; ws-maybe \u0026#34;[:{;\\n]\u0026#34; \u0026#34;\\\\|\u0026#34; \u0026#34;struct\u0026#34; ws id \u0026#34;\\\\(?:\u0026#34; ws \u0026#34;final\u0026#34; ws-maybe \u0026#34;[:{\\n]\u0026#34; \u0026#34;\\\\|\u0026#34; ws-maybe \u0026#34;:\\\\)\u0026#34; \u0026#34;\\\\|\u0026#34; \u0026#34;template\u0026#34; ws-maybe \u0026#34;\u0026lt;.*?\u0026gt;\u0026#34; \u0026#34;\\\\|\u0026#34; \u0026#34;#include\u0026#34; ws-maybe \u0026#34;\u0026lt;\u0026#34; (regexp-opt headers) \u0026#34;\u0026gt;\u0026#34; \u0026#34;\\\\)\u0026#34;))) \u0026#34;A regexp applied to C header files to check if they are really C++.\u0026#34;) 이렇게 성능이 떨어지는 방식으로 syntax highlighting 기능을 꾸역꾸역 제공하는 대신 tree-sitter를 이용하면 내가 원하는 파일의 syntax를 실시간으로 parser를 통해 얻어올 수 있고 indentation rule 이나 특정 키워드를 통해 각 노드들의 카테고리를 만들어 가져올 수 있다.\nBitbake syntax highlighting 구현 이미 수많은 tree-sitter parser 들이 공개되어 있다. Github의 tree-sitter-grammars2 를 살펴보면 다양한 언어들에 대해 지원하고 있음을 알 수 있다. 그런데 이러한 parser가 있다고 해서 곧바로 이맥스에서 사용할 수 있는 것은 아니다. 현재 열려있는 버퍼에 대해 매핑되는 major mode를 정의하고 해당 mode에서 tree-sitter 파서가 주는 syntax tree를 이용해 노드 별 카테고리를 만들고 syntax highlighting 컬러에 할당해줘야 한다.\nYocto의 bitbake를 파싱하는 parser는 이미 공개되어 있지만 이를 사용할 수 있는 major mode는 구현되어 있지 않은 상태이다. 필요한데 없으면 만들어야지 어쩌겠는가. 이제부터 bitbake-ts-mode3 코드를 구현하는 과정을 소개한다.\ntree-sitter-bitbake 다운로드 기본적인 parser 조차 없었다면 절망적이었겠지만 다행이도 tree-sitter-bitbake 패키지가 이미 구현되어 공개되어 있었다. 먼저 아래와 같이 이맥스의 tree-sitter 리스트에 tree-sitter-bitbake를 등록한다.\n(setq treesit-language-source-alist \u0026#39;((devicetree \u0026#34;https://github.com/joelspadin/tree-sitter-devicetree\u0026#34;) (bitbake \u0026#34;https://github.com/tree-sitter-grammars/tree-sitter-bitbake\u0026#34;) (c \u0026#34;https://github.com/tree-sitter/tree-sitter-c\u0026#34;) (cpp \u0026#34;https://github.com/tree-sitter/tree-sitter-cpp\u0026#34;))) 이제 M-x treesit-install-language-grammar 함수를 이용해 bitbake treesit을 설치한다. 이제 기본적인 parser까지 준비가 됐으니 major mode를 만들고 확장자에 연결할 차례다.\nbitbake-ts-mode 생성 Major mode를 만들기 위해 처음부터 모드를 전부 작성할 필요 없이 define-derived-mode 함수를 이용해 기본적인 부분들을 상속하는 형태로 만들 수 있다. 여기서는 programming mode를 상속해서 모드를 정의하였다.\n(define-derived-mode bitbake-ts-mode prog-mode \u0026#34;bitbake\u0026#34; (when (treesit-ready-p \u0026#39;bitbake) (treesit-parser-create \u0026#39;bitbake) (treesit-major-mode-setup)) ) mode를 만들었으니 확장자에 맵핑할 차례다.\n(add-to-list \u0026#39;auto-mode-alist \u0026#39;(\u0026#34;\\\\.bb?\\\\\u0026#39;\u0026#34; . bitbake-ts-mode)) (add-to-list \u0026#39;auto-mode-alist \u0026#39;(\u0026#34;\\\\.bbappend?\\\\\u0026#39;\u0026#34; . bitbake-ts-mode)) 이제 parser가 제대로 동작하는지 테스트할 차례다. bb 파일을 열어본 뒤에 M-x treesit-install-language-grammar 함수를 실행해 커서에 위치한 노드 정보를 정상적으로 파싱해오는지 확인한다. 만약 제대로 설치 및 모드가 연결이 되었다면 아래와 같이 커서 위치의 syntax tree 정보가 보일 것이다.\nsyntax highlighting Syntax Tree를 이용해 아래와 같이 font-locking 룰셋을 정의할 것이다. Font-lock4 이란 이맥스 버퍼 내에서 특정 텍스트에 대한 폰트를 지정할 수 있게 해주는 기능이다. 주로 syntax highlighting을 위해 사용한다. Syntax Tree를 얻었으니 이를 이용해 각 노드별로 매칭되는 face를 지정해준다.\n(defvar bitbake-ts-font-lock-rules `(:language bitbake :override t :feature attribute ((attribute) @font-lock-builtin-face) :language bitbake :override t :feature identifier ((identifier) @font-lock-variable-keyword-face) :language bitbake :override t :feature string_content ((string_content) @font-lock-string-face) :language bitbake :override t :feature comment ((comment) @font-lock-comment-face) :language bitbake :override t :feature keyword ([,@bitbake-ts-mode--treesit-keywords] @font-lock-keyword-face))) 여기서 정의된 feature 들은 아래와 같이 treesit-font-lock-feature-list 에 등록해줘야 한다.\n(when (treesit-ready-p \u0026#39;bitbake) (treesit-parser-create \u0026#39;bitbake) ;; Define a list of features of what it is going to be highlighted (setq-local treesit-font-lock-feature-list \u0026#39;((comment) (keyword) (identifier attribute string_content))) ... feature들을 조합해서 (identifier attribute string_content)로 나타낸 것을 볼 수 있는데 syntax tree가 그러하기 때문이다. 예를 들어, 아래와 같은 라인이 있다고 하면\nDEPENDS += \u0026#34;${@bb.utils.contains(\u0026#39;ARCH\u0026#39;, \u0026#39;x86\u0026#39;, \u0026#39;elfutils-native\u0026#39;, \u0026#39;\u0026#39;, d)}\u0026#34; 위 라인은 syntax tree에서 아래와 같이 표현되는데 identifier, attribute, string_content feature 들이 한 라인에 담겨있는 것을 볼 수 있다. 만약 attribute는 syntax highlighting에서 제거하고 싶다면 (identifier string_content) 로 나타내도 무방하다.\n(variable_assignment (identifier) operator: += (literal (string \u0026#34; (inline_python ${@ (call function: (attribute object: (attribute object: (python_identifier) . attribute: (python_identifier)) . attribute: (python_identifier)) arguments: (argument_list ( (python_string (string_start) (string_content) (string_end)) , (python_string (string_start) (string_content) (string_end)) , (python_string (string_start) (string_content) (string_end)) , (python_string (string_start) (string_end)) , (python_identifier) ))) }) \u0026#34;))) imenu 연동 imenu는 이맥스에서 제공하는 frontend 패키지 중 하나로 원하는 정보를 보여주기 위해 사용한다. 여기서는 yocto recipe의 directive와 identifier 위주로 볼 수 있도록 함수를 구현할 것이다. 사실 이 부분도 syntax highlighting 과 마찬가지로 매우 간단하다.\n(setq-local treesit-simple-imenu-settings `((\u0026#34;Directive\u0026#34; bitbake-ts-imenu-directive-node-p nil bitbake-ts-imenu-directive-name-function) (\u0026#34;Identifier\u0026#34; bitbake-ts-imenu-identifier-node-p nil bitbake-ts-imenu-identifier-name-function))) directive를 예로 살펴보면, 해당 노드가 directive인지 판단하기 위한 bitbake-ts-imenu-directive-node-p 함수와 노드로부터 어떻게 출력할 것인지 문자열을 반환하는 bitbake-ts-imenu-directive-name-function 을 이용하고 있는 것을 볼 수 있다. 각각의 함수들은 정규식과 treesit-node-text 함수를 이용해 아래와 같이 간단하게 구현할 수 있다.\n(defun bitbake-ts-imenu-identifier-name-function (node) \u0026#34;A function to return the name of identifier `NODE\u0026#39;.\u0026#34; (treesit-node-text node)) (defun bitbake-ts-imenu-identifier-node-p (node) \u0026#34;A function to check whether the `NODE\u0026#39; is identifier.\u0026#34; (string-match-p \u0026#34;^identifier$\u0026#34; (treesit-node-type node))) (defun bitbake-ts-imenu-directive-name-function (node) \u0026#34;A function to obtain the name of directive `NODE\u0026#39;.\u0026#34; (treesit-node-text node)) (defun bitbake-ts-imenu-directive-node-p (node) \u0026#34;A function to check whether the `NODE\u0026#39; is directive.\u0026#34; (string-match-p \u0026#34;_directive$\u0026#34; (treesit-node-type node))) 마무리 전체 코드는 https://github.com/seokbeomKim/bitbake-ts-mode repo를 참고한다. 한국에 얼마 남지 않은 이맥스 유저들에게 도움이 되길 바라며 글을 마무리한다.\nhttps://tree-sitter.github.io/tree-sitter \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/tree-sitter-grammars \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/seokbeomKim/bitbake-ts-mode \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.gnu.org/software/emacs/manual/html_node/emacs/Font-Lock.html \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n", 
        "url": "\/\/localhost:1313\/posts\/%EC%9D%B4%EB%A7%A5%EC%8A%A4-tree-sitter-major-mode-%EA%B5%AC%ED%98%84\/"
    },
    
    "\/\/localhost:1313\/posts\/vterm-in-emacs\/": {
        "title": "Vterm In Emacs",
        "tags": [],
        "content": " Overview Using Emacs in daily life, I rely on vterm terminal emulator instead of eshell. However, I\u0026#39;ve noticed that there are certain limitations in terms of integration between vterm and Emacs. While the package supports some user-accessible functions, they are not sufficient. I\u0026#39;ve always wanted the integration level as VSCode, and at least it should be able to open files from the terminal interface. Well, this is one of essential features of the terminal emulator running on editors, so I thought that having this kind of issue was ridiculous. So I tried to find solutions by googling about it, but none of them had a one-shot method to achieve this. So, I made up my mind to write functions by myself.\nIn this article, I am going to describe the following things:\nA callback function to open files from vterm Functions to manage vterm session Note that since I am using Doomemacs right now, the keymap setting could differ from yours. If you do not want to read any details about functions that I wrote, just use the following settings.\n(require \u0026#39;filenotify) (defvar my:get-vterm--backup nil) (defun my:vterm-new () (interactive) (if (not (string-match-p \u0026#34;vterm\u0026#34; (buffer-name (current-buffer)))) (setq my:get-vterm--backup (current-buffer))) (call-interactively #\u0026#39;+vterm/here)) (defun my:vterm-toggle () (interactive) (let ((cnt (cl-remove-if #\u0026#39;null (mapcar (lambda (x) (and (string-match-p \u0026#34;vterm\u0026#34; (buffer-name x)) (buffer-name x))) (buffer-list))))) (if (null cnt) (progn (setq my:get-vterm--backup (current-buffer)) (call-interactively #\u0026#39;+vterm/here)) (if (and (string-match-p \u0026#34;vterm\u0026#34; (buffer-name (current-buffer))) my:get-vterm--backup) (my:vterm--restore) (call-interactively #\u0026#39;my:vterm--select))))) (defun my:vterm--restore () (switch-to-buffer my:get-vterm--backup) (setq my:get-vterm--backup nil)) (defun my:vterm--select (choice) \u0026#34;Argument CHOICE user\u0026#39;s selection.\u0026#34; (interactive (list (completing-read \u0026#34;Choose: \u0026#34; (cl-remove-if #\u0026#39;null (mapcar (lambda (x) (and (string-match-p \u0026#34;vterm\u0026#34; (buffer-name x)) (buffer-name x))) (buffer-list))) nil t))) (car (split-string choice \u0026#34; \u0026#34;)) (setq my:get-vterm--backup (current-buffer)) (switch-to-buffer choice)) (after! vterm ;; Following must be used with bash alias: ;; =\u0026gt; alias eo=\u0026#39;echo $1 \u0026gt; ~/.config/emacs/.local/cache/vterm-pipe\u0026#39; (let* ((pipe-file (expand-file-name \u0026#34;vterm-pipe\u0026#34; user-emacs-directory)) (func-open-file (lambda (event) (find-file (with-temp-buffer (insert-file-contents (expand-file-name \u0026#34;vterm-pipe\u0026#34; user-emacs-directory)) (string-trim (buffer-string))))))) (file-notify-add-watch pipe-file \u0026#39;(change) func-open-file)) (add-hook \u0026#39;vterm-mode-hook (lambda () (evil-emacs-state)))) File open from vterm - filenotify Since Emacs-28.1, Emacs supports the filenotify package, which makes it possible to watch any change from the file. It means that whenever I write any to the file, Emacs can get the triggered event from the write. Let\u0026#39;s register a callback function for the vterm-pipe in user-emacs-directory.\n(file-notify-add-watch pipe-file \u0026#39;(change) callback-func) Add the following code to $HOME/.bashrc to use the alias eo command. Now, using the eo alias will trigger the event and invoke the callback function. It\u0026#39;s done.\nalias eo=\u0026#39;realpath $1 \u0026gt; ~/.config/emacs/.local/cache/vterm-pipe\u0026#39; Vterm session management Unfortunately, vterm does not support any functions to manage its session. And a function to toggle it is not perfect. Let\u0026#39;s improve it by using an interactive menu. You can toggle the vterm session with my:vterm-toggle. In the code, there are many to refactor but it is sufficient to resolve the lack of session management and inefficient UI toggle.\nWrap up Since I started to learn how to write code in elisp, I have been able to use Emacs efficiently. Beyond the simple editor, now I can see why Emacs has been loved by lots of developers. I know, this should be the same for VI/M users :P.\n", 
        "url": "\/\/localhost:1313\/posts\/vterm-in-emacs\/"
    },
    
    "\/\/localhost:1313\/tags\/nfs\/": {
        "title": "Nfs",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/nfs\/"
    },
    
    "\/\/localhost:1313\/tags\/rpi\/": {
        "title": "Rpi",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/rpi\/"
    },
    
    "\/\/localhost:1313\/tags\/tftp\/": {
        "title": "Tftp",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/tftp\/"
    },
    
    "\/\/localhost:1313\/posts\/%EB%9D%BC%EC%A6%88%EB%B2%A0%EB%A6%AC%ED%8C%8C%EC%9D%B4-%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC-%EB%B6%80%ED%8C%85-%EC%84%A4%EC%A0%95\/": {
        "title": "라즈베리파이 네트워크 부팅 설정",
        "tags": ["rpi","nfs","tftp",],
        "content": "네트워크 부팅 환경 설정 기본적으로 라즈베리파이의 경우 SD 카드에 부트에 필요한 파티션들을 저장한다. 이 때문에 커널이나 루트파일시스템의 수정사항이 있는 경우 매번 호스트에서 SD 카드에 관련된 파일을 넣어줘야 하는데 이것만큼 정말 번거로운 것이 없다.\n필자는 라즈베리파이4를 기준으로 환경을 구성하였다. 본 페이지에서는 TFTP와 라즈베리파이의 PXE Boot을 지원하는 기본 부트로더를 이용해 커널 이미지를 로드하는 방법과 NFS를 이용하여 루트파일시스템을 로드하는 방법을 함께 기술한다.\n네트워크 구성 일반적으로 부트로더에서 TFTP 클라이언트를 함께 제공한다. U-Boot 부트로더와 마찬가지로 라즈베리파이의 기본 부트로더 또한 내부적으로 TFTP 클라이언트를 포함하고 있어 부트로더에 관련된 설정만 추가해주면 된다. NFS의 경우 mount 명령어를 통해 하지만 그 전에 네트워크 구성에 대해 한 번 생각해봐야 한다.\nTFTP 서버는 Host PC (Windows) 의 tftpd를 이용한다. WSL2에서 tftp 서버를 구성해도 되지만 필자는 윈도우즈 호스트 PC에서 tftpd64 라는 프로그램을 이용하기로 했다. NFS 서버는 WSL2에 구성하고 라즈베리파이에서 WSL2에 접근할 수 있도록 NFS에서 사용하는 포트에 한하여 포트포워딩을 한다. TFTP 이용하여 커널 이미지 로드 TFTP를 이용해 커널 이미지를 로드하기 위해서는 아래의 작업 순서가 필요하다. 1번 내용은 Raspberry Pi Imager 라는 공식 유틸리티가 있고 굳이 설명을 하지 않아도 되기 때문에 따로 설명하지는 않겠다.\nSD 카드 이용해 기본 부트 라즈베리파이의 부트로더 설정 변경 SD 카드의 부트 파티션 복사 tftp64 이용해 부트 2. 부트로더 설정 변경 아래에 설명하는 부트로더 설정 변경 방법은 https://metebalci.com/blog/cardless-rpi4/ 에 따른 것이다. 찾아본 포스팅 중에 가장 정리가 잘 되어 있다. 포스팅 내 주요 내용을 정리하면, 아래의 순서로 부트로더 설정을 업데이트 한다.\n현재 부트로더 버전 확인 pieeprom.bin 바이너리로부터 설정 추출 설정 파일 내 TFTP 정보 추가 변경한 설정 파일을 포함한 pieeprom.bin 재생성 재생성한 pieeprom.bin으로 부트로더 업데이트 먼저 부트로더 버전을 확인해보자. vcgencmd bootloader_version 명령어를 이용하면 아래와 같이 현재 부트한 환경 기준으로 사용 중인 부트로더 버전을 확인할 수 있다.\npi@raspberrypi:~$ vcgencmd bootloader_version 2023/01/11 17:40:52 version 8ba17717fbcedd4c3b6d4bce7e50c7af4155cba9 (release) timestamp 1673458852 update-time 1681394003 capabilities 0x0000007f 버전을 확인했으니 해당 날짜에 맞는 pieeprom을 홈 디렉토리로 복사하고 바이너리 파일로부터 설정을 추출한다. 만약 rpi-eeprom-config가 없다면 rpi-eeprom 패키지를 설치해줘야 한다.\n$ pwd /home/pi $ cp /lib/firmware/raspberrypi/bootloader/default/pieeprom-2023-01-11.bin pieeprom.bin $ rpi-eeprom-config pieeprom.bin \u0026gt; config.txt 출처에서는 BOOT_ORDER 를 강조하고 있는데, TFTP 환경으로 부팅해야 하므로 앞서 네트워크 구성에서 고려했던 것과 같이 config.txt 파일을 변경해준다.\n[all] BOOT_UART=1 WAKE_ON_GPIO=1 POWER_OFF_ON_HALT=0 TFTP_IP=192.168.0.5 CLIENT_IP=192.168.0.4 SUBNET=255.255.255.0 GATEWAY= TFTP_PREFIX=0 BOOT_ORDER=0x21 TFTP_FILE_TIMEOUT=30000 설정이 끝났다. 이제 설정파일이 담긴 바이너리 파일을 생성하고 해당 파일로 업데이트 해주자.\n$ rpi-eeprom-config --out pieeprom-out.bin --config config.txt pieeprom.bin $ sudo rpi-eeprom-update -d -f ./pieeprom-out.bin $ sudo reboot 3. SD 카드 부트 파티션 복사 이제 부트로더 설정은 끝났으니 SD 카드의 boot partition을 Host PC에 저장한다. 필자는 그냥 귀찮아서 tftpd64 디렉토리(C:\\Program Files\\Tftpd64\\rpi_boot) 안에 넣어놓았다.\n4. tftp64 이용해 tftpboot 이제 tftp64 프로그램에서 디렉토리를 설정해주고 라즈베리파이를 부팅해주면 커널 로드까지는 정상적으로 되는 것을 확인할 수 있다. tftp64 프로그램은 아래 링크에서 다운로드 받을 수 있다.\nhttps://bitbucket.org/phjounin/tftpd64/downloads/ 이제 tftp를 이용한 커널 이미지 로드 준비는 끝이 났다. SD 카드를 빼고 전원을 인가하면 커널 로드까지는 성공적으로 되는 것을 확인할 수 있다.\nNFS 이용하여 루트파일시스템 로드 커널 이미지를 성공적으로 로드한다고 해도 루트파일시스템 로드가 되지 않으니 부팅이 될 리가 없다. 필자는 WSL2(Ubuntu)에 NFS 서버를 구성해서 rootfs 마운트 시점에 NFS를 마운트하도록 구성하였다.\n작업 순서는 아래와 같으며, 데비안 계열의 우분투 WSL 기준으로 설명하겠다.\n라즈베리파이 이미지 파일 (*.img) 마운트 및 복사 WSL2 내 nfs-server 설치 및 설정 nfs-server 서비스 실행 Windows 내 포트 포워딩 및 방화벽 설정 라즈베리파이 이미지 내 rootfs 파일 복사 먼저, 라즈베리파이 공식 사이트에서 os 이미지 파일을 다운로드 받는다. 본인은 현재 기준으로 lite version인 2023-02-21-raspios-bullseye-arm64-lite.img 파일을 다운로드 받았다. 그 후 아래와 같이 마운트를 하고 로컬 디렉토리에 내용을 복사한다.\n$ sudo mount -o loop,offset=272629760 ./2023-02-21-raspios-bullseye-arm 64-lite.img /mnt/ $ mkdir /rpi $ cp -ra /mnt/* /rpi NFS-Server 설치 및 설정 이제 rootfs는 준비되었으니 nfs-server를 설치할 차례이다. 필자는 라즈베리파이를 제외한 나머지 디렉토리는 필요하지 않기 때문에 rootdir을 /rpi로 설정하였다. 사용 환경에 따라 적절하게 설정해준다.\n$ sudo apt install nfs-kernel-server $ sudo vi /etc/nfs.conf # 아래와 같이 [exports] 설정 [exports] rootdir=/rpi # ... 이제 /etc/exports 파일을 아래와 같이 설정해준다. insecure 옵션을 넣어주었는데 본인의 경우 이 설정을 제외하면 파일 퍼미션이 보이지 않아 함께 넣어주었다.\n$ sudo vi /etc/exports # ... / *(rw,sync,no_root_squash,insecure) nfs-server 서비스 실행 이제 서비스를 실행하고 exportfs 를 업데이트 해준다.\n$ sudo service nfs-server restart $ sudo export -arv 한 가지 발견한 문제는 윈도우즈에서 2049 포트가 포트포워딩이 되어 있을 경우 port binding 에러로 인해 서비스 실행 에러가 생긴다는 점이다. 반드시 윈도우즈에서 포트포워딩 되어 있는 포트 중에 nfs 관련 포트가 없는지 확인하고 서비스를 실행한다.\nPS C:\\Users\\chaox\u0026gt; netsh interface portproxy show all Listen on ipv4: Connect to ipv4: Address Port Address Port --------------- ---------- --------------- ---------- 0.0.0.0 3000 172.29.124.79 3000 0.0.0.0 2049 172.29.124.79 2049 \u0026lt;- 만약에 이 부분이 있다면 에러가 발생한다. 포트 포워딩 제거시에는 아래 명령어를 사용한다.\n\u0026gt; netsh interface portproxy del v4tov4 listenport=2049 listenaddress=0.0.0.0 포트 포워딩 및 방화벽 설정 WSL2에서 서비스까지 정상적으로 실행되었다면 윈도우즈에서 아래와 같이 포트포워딩을 해준다. 그리고 정상적으로 2049 포트로 포트포워딩이 되어 있는지 확인한다. connectaddress는 wsl hostname -I 로 확인한 아이피 주소를 넣어준다.\n\u0026gt; netsh interface portproxy add v4tov4 listenport=2049 listenaddress=0.0.0.0 connectport=2049 connectaddress=172.29.124.79 \u0026gt; netsh interface portproxy show all Listen on ipv4: Connect to ipv4: Address Port Address Port --------------- ---------- --------------- ---------- 0.0.0.0 3000 172.29.124.79 3000 0.0.0.0 2049 172.29.124.79 2049 이제 윈도우즈의 Windows Defender Firewall with Advanced Security 를 열어서 Inbound Rules와 Outbound Rules 각각 포트 2049에 대해 허용하도록 설정한다.\n마무리 이제 마지막으로 앞서 복사해둔 경로 내 etc/fstab 을 아래와 같이 수정해준다.\n$ sudo vi /rpi/etc/fstab # ... proc /proc proc defaults 0 0 192.168.0.5:/ / nfs defaults,_netdav 0 1 마지막으로 부트 파티션으로 복사한 cmdline.txt 에 NFS를 로드할 수 있도록 변경해주자.\nconsole=serial0,115200 console=tty1 root=/dev/nfs nfsroot=192.168.0.5:/,nfsvers=4 ip=192.168.0.4 rw elevator=deadline fsck.repair=yes rootwait rootfstype=nfs 출처 https://metebalci.com/blog/cardless-rpi4/ ", 
        "url": "\/\/localhost:1313\/posts\/%EB%9D%BC%EC%A6%88%EB%B2%A0%EB%A6%AC%ED%8C%8C%EC%9D%B4-%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC-%EB%B6%80%ED%8C%85-%EC%84%A4%EC%A0%95\/"
    },
    
    "\/\/localhost:1313\/tags\/device-tree\/": {
        "title": "Device-Tree",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/device-tree\/"
    },
    
    "\/\/localhost:1313\/tags\/fwnode\/": {
        "title": "Fwnode",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/fwnode\/"
    },
    
    "\/\/localhost:1313\/posts\/fwnode\/": {
        "title": "Fwnode",
        "tags": ["device-tree","fwnode",],
        "content": "fwnode에 대해서는 자료가 거의 없는 것 같다. 너무 쉬운 개념이라 없는 건지 관심이 없어서 그런 것인지는 모르겠으나 개인적으로는 단번에 이해되지가 않았고 참고할 수 있는 자료가 너무 없어서 아쉬웠다.\n커밋 찾아보기 ChatGPT에게 fwnode를 설명해보았지만 제대로 대답해주지 않는다. ARM 공식 문서 내의 내용은 잘 설명해주는데 커널에 관련된 내용은 틀린 부분이 많다. 역시 이해하기 위해서는 최초 적용된 커밋을 보는게 제일 좋은 방법이다. 아래의 커밋을 살펴보면 어떤 동기를 가지고 코드를 작성했는지 이해할 수 있다.\nce793486e23e0162a732c605189c8028e0910e86 8a0662d9ed2968e1186208336a8e1fab3fdfea63 최초 커밋에서 author은 아래와 같이 설명하고 있다.\nThere are two benefits from that. First, the somewhat ugly and hackish struct acpi_dev_node can be dropped and, second, the same struct fwnode_handle pointer can be used in the future to point to other (non-ACPI) firmware device node types.\n디바이스 구성을 기술(표현)하는 방법: Device Tree 리눅스 커널에서는 디바이스 구성을 표현하기 위해 OF(Open Firmware)의 Device Tree를 사용한다. 하지만 디바이스를 표현하는 방법에는 Device Tree 외에도 윈도우즈에서 사용되는 ACPI를 이용하는 방법이 있다. 이러한 방식에 대한 호환성을 제공하기 위해 리눅스 커널에서는 해당 부분을 추상화시킬 필요가 생겼고 이 때문에 도입된 것이 바로 fwnode이다.\nACPI가 아닌 경우에도 원하는 디바이스 표현 형태가 있다면, 해당 디바이스 노드를 올바르게 읽어올 수 있도록 fwnode 인터페이스만 구현해주면 된다. 리눅스 커널에서 디바이스의 property를 얻어오는 함수는 아래와 같이 fwnode를 이용하도록 구현되어 있다. 해당 디바이스가 어떤 device description method로 표현되어 있는지 상관없이 디바이스에 설정된 방식에 따라 원하는 프로퍼티를 가져올 수 있다.\nint device_property_read_string_array(struct device *dev, const char *propname, const char **val, size_t nval) { return fwnode_property_read_string_array(dev_fwnode(dev), propname, val, nval); } fwnode_ops로 구분하는 디바이스 표현 방법 Open Firmware인 경우 리눅스 커널에서 특이한 경우가 아니라면 기본적으로 of_node_init을 이용해 디바이스를 초기화하며 이 때 of_fwnode_ops를 사용하도록 설정된다.\nstatic inline void of_node_init(struct device_node *node) { #if defined(CONFIG_OF_KOBJ) kobject_init(\u0026amp;node-\u0026gt;kobj, \u0026amp;of_node_ktype); #endif fwnode_init(\u0026amp;node-\u0026gt;fwnode, \u0026amp;of_fwnode_ops); } 하지만 표현되어 있는 디바이스 노드를 of_node가 아닌 acpi로서 아래와 같이 fwnode를 초기화할 수로 있고, swnode로서 초기화 할 수도 있다. 각 함수에서 호출되는 fwnode_init() 함수에 주목하자.\nSoftware Node (swnode) swnode_register(const struct software_node *node, struct swnode *parent, unsigned int allocated) { struct swnode *swnode; int ret; /* ... */ swnode-\u0026gt;id = ret; swnode-\u0026gt;node = node; swnode-\u0026gt;parent = parent; swnode-\u0026gt;kobj.kset = swnode_kset; fwnode_init(\u0026amp;swnode-\u0026gt;fwnode, \u0026amp;software_node_ops); /* ... */ } ACPI void acpi_init_device_object(struct acpi_device *device, acpi_handle handle, int type) { INIT_LIST_HEAD(\u0026amp;device-\u0026gt;pnp.ids); device-\u0026gt;device_type = type; device-\u0026gt;handle = handle; device-\u0026gt;parent = acpi_bus_get_parent(handle); fwnode_init(\u0026amp;device-\u0026gt;fwnode, \u0026amp;acpi_device_fwnode_ops); acpi_set_device_status(device, ACPI_STA_DEFAULT); acpi_device_get_busid(device); acpi_set_pnp_ids(handle, \u0026amp;device-\u0026gt;pnp, type); acpi_init_properties(device); acpi_bus_get_flags(device); device-\u0026gt;flags.match_driver = false; device-\u0026gt;flags.initialized = true; device-\u0026gt;flags.enumeration_by_parent = acpi_device_enumeration_by_parent(device); acpi_device_clear_enumerated(device); device_initialize(\u0026amp;device-\u0026gt;dev); dev_set_uevent_suppress(\u0026amp;device-\u0026gt;dev, true); acpi_init_coherency(device); } V4L2 media framework와 fwnode의 관계 그렇다면, fwnode와 V4L2 media framework은 무슨 상관인가? V4L2 media framework의 v4l2-fwnode.h 파일을 보면 operation은 별도로 구현하지 않고 있다. 그 말은 V4L2 media framework에서 디바이스를 표현하는 방법을 새롭게 정의한 것이 아니라 단순하게 fwnode가 가지고 있는 기능을 이용한다는 것을 짐작할 수 있다.\nv4l2-fwnode.h 파일에서 fwnode와 관련된 함수들을 살펴보면 아래와 같이 endpoint 개념이 나오는 것을 볼 수 있다.\nv4l2_async_nf_parse_fwnode_endpoints v4l2_fwnode_connector_add_link v4l2_fwnode_connector_free v4l2_fwnode_connector_parse v4l2_fwnode_device_parse v4l2_fwnode_endpoint_alloc_parse v4l2_fwnode_endpoint_free v4l2_fwnode_endpoint_parse v4l2_fwnode_parse_link v4l2_fwnode_put_link fwnode_handle v4l2_async_notifier v4l2_async_subdev fwnode_endpoint를 아래와 같이 상속하여 v4l2_fwnode_endpoint로 정의해 사용하고 있는 것을 알 수 있다. 즉, fwnode의 graph 구성 기능을 이용하여 v4l2_fwnode_endpoint로서 미디어 파이프라인을 표현하기에 필요한 몇 가지 정보를 더 추가하고 있는 것일 뿐이다.\nstruct v4l2_fwnode_endpoint { struct fwnode_endpoint base; /* * Fields below this line will be zeroed by * v4l2_fwnode_endpoint_parse() */ enum v4l2_mbus_type bus_type; struct { struct v4l2_fwnode_bus_parallel parallel; struct v4l2_fwnode_bus_mipi_csi1 mipi_csi1; struct v4l2_fwnode_bus_mipi_csi2 mipi_csi2; } bus; u64 *link_frequencies; unsigned int nr_of_link_frequencies; }; 마치며 개인적으로 해석했을 때 fwnode는 device description methods 들의 abstraction data structure이다. 끝.\n참고 https://events.static.linuxfound.org/sites/events/files/slides/unified_properties_API_0.pdf ", 
        "url": "\/\/localhost:1313\/posts\/fwnode\/"
    },
    
    "\/\/localhost:1313\/categories\/kernel\/": {
        "title": "Kernel",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/categories\/kernel\/"
    },
    
    "\/\/localhost:1313\/tags\/chatgpt\/": {
        "title": "Chatgpt",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/chatgpt\/"
    },
    
    "\/\/localhost:1313\/categories\/etc\/": {
        "title": "Etc",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/categories\/etc\/"
    },
    
    "\/\/localhost:1313\/tags\/openai\/": {
        "title": "Openai",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/openai\/"
    },
    
    "\/\/localhost:1313\/posts\/openai-chatgpt\/": {
        "title": "OpenAI ChatGPT",
        "tags": ["openai","chatgpt",],
        "content": "며칠 전 같은 팀 부장님께서 OpenAI의 ChatGPT를 알려주셨다. 처음에는 단순한 챗봇일 거라 생각했는데 성능이 예상했던 것보다 훨~씬 좋았다. MISRA-C나 CERT-C 에 문제가 없는지 확인하는 코드 리뷰부터 영어 문장이 문법적으로 옳은지, 또 ARM Architecture 에서 궁금했던 용어나 설명 자체가 이해되지 않을 때 이게 무슨 의미인지 물어보았는데, 소름돋을 정도로 대답을 잘 해주었다.\n아래는 ARM architecture manual 에서 Normal memory 의 속성 중 한 가지에 대한 설명을 물어보았을 때 받은 답변이다.\nwhat does it mean \u0026#34;Read accesses can be repeated with no side-effects\u0026#34;? \u0026#34;Read accesses can be repeated with no side-effects\u0026#34; means that it is safe to read a particular resource or piece of data multiple times without causing any unintended consequences. In other words, reading the resource multiple times will not modify the resource in any way, nor will it have any other effects on the system beyond the act of reading the resource. This is in contrast to write accesses, which can modify the resource and may have other side-effects on the system. This property is often desirable because it allows multiple clients or processes to read the resource concurrently without worrying about interference or race conditions. It can also allow for better performance and scalability in systems where the resource is frequently accessed, as it reduces the need for synchronization and locking mechanisms. 놀랍다. 단순하게 구글링을 해서 얻는 정보와 달리 상당히 정제되어 있고 피곤하게 찾을 필요도 없으며 기술 전문가와 1:1로 대화하는 것 같아 너무나 좋았다. 이런 식으로 공부하면서 모르는 것이 있을 때 ChatGPT를 이용하면 너무나 좋을 것 같다.\n또한 코드 리뷰도 잘 되었다. 일부러 커널 코드 일부에서 동기화 문제가 발생하도록 하거나 MISRA-C 규칙을 따르지 못하게 수정한 뒤에 아래와 같이 코드리뷰를 요청해보았는데 너무나도 잘 리뷰해주었다.\n아래는 MISRA-C 코딩 룰을 어떻게 가이드 해주는지 테스트해본 것이다.\nCan you fix the following code with MISRA-C? int a = 0; int b = 0; int c = a + b * 2; To fix this code in accordance with the MISRA-C guidelines, you could do the following: Add parentheses around the multiplication operation to ensure that it is performed before the addition:Copy codeint c = a + (b * 2); Add a space between the plus sign and the variables to improve readability:Copy codeint c = a + (b * 2) Declare the variables a, b, and c with the const keyword to indicate that their values will not be changed:Copy codeconst int a = 0; const int b = 0; const int c = a + (b * 2); These are just a few suggestions for how you could modify the code to comply with the MISRA-C guidelines. There may be other issues with the code that would need to be addressed as well. Is there anything else I can help you with? 괄호 뿐만 아니라 const 까지 가이드 해준다. 이번엔 커널의 mutex_unlock 부분을 제거해보고 코드리뷰를 맡겨보자.\nPlease check if the problem exists: static void timer_update_keys(struct work_struct *work) { mutex_lock(\u0026amp;timer_keys_mutex); timers_update_migration(); static_branch_enable(\u0026amp;timers_nohz_active); } Based on the code you have provided, it appears that there is a problem with the function as written. Specifically, the mutex_unlock function is missing from the end of the function, which means that the timer_keys_mutex mutex will remain locked after the function returns. This could lead to a deadlock if other code attempts to acquire the same mutex while it is still locked. To fix this issue, you should add a call to mutex_unlock at the end of the function, like this: Copy code static void timer_update_keys(struct work_struct *work) { mutex_lock(\u0026amp;timer_keys_mutex); timers_update_migration(); static_branch_enable(\u0026amp;timers_nohz_active); mutex_unlock(\u0026amp;timer_keys_mutex); } This will ensure that the mutex is properly unlocked when the function completes, avoiding the potential for a deadlock. Is there anything else I can help you with? 커널을 공부하면서 힘들었던 이유는 자료를 찾기도 힘들 뿐더러 내가 이해하는 게 과연 맞는 것인지, 문서에 기재된 내용이 어떤 문맥으로 작성된 것인지 알기 힘들었기 때문이다. 그런데 이러한 걱정이 아예 없어지니 내 옆에 든든한 선생님이 생긴 느낌이다.\nChatGPT 를 사용해보고 느낀 건 단순한 코드 몽키는 확실하게 없어질 것 같다. 하지만 시나리오와 코드를 최종적으로 검증하고 제품으로 응용하기 위해 관리하는 사람은 앞으로도 계속 필요할 것이다. 그리고 학원가의 모습도 많이 바뀔 것 같다. 이 정도의 성능이라면 나중에는 컴퓨터 앞에서 선생 없이 자율 학습 만으로도 공부할 수 있는 세상이 오지 않을까?\n결론은, 커널 스터디하기 훨씬 더 수월해져서 너무 좋다.\n참고 https://chat.openai.com/chat ", 
        "url": "\/\/localhost:1313\/posts\/openai-chatgpt\/"
    },
    
    "\/\/localhost:1313\/tags\/tasklet\/": {
        "title": "Tasklet",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/tasklet\/"
    },
    
    "\/\/localhost:1313\/posts\/tasklet\/": {
        "title": "Tasklet",
        "tags": ["tasklet",],
        "content": "개요 사내 커널 스터디에서 교재로서 사용하고 있는 디버깅을 통해 배우는 리눅스 커널의 구조와 원리 책에서는 태스크릿 (Tasklet)을 SoftIRQ와 함께 아래와 같이 설명한다.\nSoftIRQ 서비스 중 하나로 동적으로 Soft IRQ 서비스를 쓸 수 있는 인터페이스이다. 그리고 이와 함께 Tasklet의 실행 단계에 대한 내용이 자세하게 기술되어 있다.\n인터럽트 핸들러에서 task_schedule() 함수를 호출해 태스크릿 스케쥴링을 실행한다. task_schedule() 함수는 raise_softirq_irqoff() 함수를 호출해 TASKLET Soft IRQ 서비스를 요청한다. Soft IRQ 서비스 핸들러를 호출하는 __do_softirq() 함수에서 태스크릿 서비스 핸들러인 tasklet_action() 함수를 호출한다. Soft IRQ 서비스 핸들러를 실행한 후 실행 시간을 체크해 ksoftirqd 를 깨우고 실행을 마무리한다. ksoftirqd 스레드에서 태스크릿 서비스 핸들러인 tasklet_action() 함수를 호출한다. tasklet_action() 함수에서 태스크릿 핸들러 함수를 호출한다. 책에서는 SoftIRQ를 인터럽트 후반부를 빨리 처리해야 할 때 사용하는 기법으로 설명하고 있고 디바이스 드라이버에서 SoftIRQ를 사용하는 방법으로서 Tasklet 서비스를 설명하고 있다. 그런데 예전 소스 코드 기반이라 그런지 찾아본 내용과 다른 부분이 있다. 최신 코드를 기준으로 찾아보면 tasklet에 대해 아래와 같이 설명한다.\n/* Tasklets \u0026mdash; multithreaded analogue of BHs.\nThis API is deprecated. Please consider using threaded IRQs instead: https://lore.kernel.org/lkml/20200716081538.2sivhkj4hcyrusem@linutronix.de Main feature differing them of generic softirqs: tasklet is running only on one CPU simultaneously.\nMain feature differing them of BHs: different tasklets may be run simultaneously on different CPUs.\nProperties:\nIf tasklet_schedule() is called, then tasklet is guaranteed to be executed on some cpu at least once after this. If the tasklet is already scheduled, but its execution is still not started, it will be executed only once. If this tasklet is already running on another CPU (or schedule is called from tasklet itself), it is rescheduled for later. Tasklet is strictly serialized wrt itself, but not wrt another tasklets. If client needs some intertask synchronization, he makes it with spinlocks. */ $linux/include/linux/interrupt.h\n코드에서는 태스크릿 대신 Threaded IRQ를 사용할 것을 권한다. 코멘트의 링크를 따라가보면 태스크릿 매크로를 수정한 건에 대한 패치에서 태스크릿을 별도의 작업(refactor all tasklet users into other APIs · Issue #94 · KSPP/linux)으로 제거하자는 얘기가 나온다. 논의 내용을 잘 정리한 LWN 기사(Modernizing the tasklet API [LWN.net]) 도 찾을 수 있었는데 간단하게 정리해보면 아래와 같다.\nThreaded IRQ 도 atomic context에서 실행되므로 tasklet을 대체될 수 있다. 코어 개발자들도 태스크릿의 API를 개선하는 것보다 제거하는 게 더 큰 작업이지만 next step 으로 제거하는 데에는 동의하였다. LWN 에서 이전에 다룬 내용이지만 태스크릿은 software interrupt mode에서 실행되기 때문에 다른 highest-priority 태스크들을 블록할 수 있는 latency 제한 문제가 있다. Tasklet은 workqueues, timers, threaded interrupts로 대체 가능하다. Threaded irq를 사용하면 인터럽트 핸들러 자체에서 실행될 수 있다. 그리고 이러한 신규 매커니즘들은 태스크릿의 단점들이 없기 때문에 굳이(?) 개발자들이 tasklet 을 쓸 이유가 없다. 결론적으로 태스크릿을 꼭 써야한다면 어쩔 수 없겠지만 이후에 새로 나온 방법들로도 충분히 BH 를 다룰 수 있으며 이러한 방법을 제외하고 굳이 써야하는 이유는 없다는 것이다. 태스크릿을 반드시 써야 하는 경우를 판단해야 한다면 아래 링크들과 교재 내용을 함께 참고해보자.\n참고 Modernizing the tasklet API [LWN.net] Re: [PATCH 0/3] Modernize tasklet callback API - Allen (kernel.org) ", 
        "url": "\/\/localhost:1313\/posts\/tasklet\/"
    },
    
    "\/\/localhost:1313\/tags\/obsidian\/": {
        "title": "Obsidian",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/obsidian\/"
    },
    
    "\/\/localhost:1313\/posts\/obsidian\/": {
        "title": "Obsidian",
        "tags": ["obsidian",],
        "content": "Notion에서 Obsidian으로 갈아타기\n요즘에 많은 사람들이 노션을 사용한다. 이력서나 개인 위키, 심지어 업무 협업툴로써 사용한다. 필자는 개인 위키 용도로 노션을 1년 정도 넘게 사용했다. 그 전에 사용해오던 이맥스의 org나 dokuwiki와 달리 별도의 서버 관리도 필요하지 않고 인터페이스도 미려했다. 애플리케이션까지 제공돼 매우 편리했다. 또한 기존 도구들이 가지고 있던 고질적인 문제였던 기기 간의 동기화 문제를 해결해 주었다.\n하지만 노션에는 몇 가지 단점이 있다.\n첫 번째는 확장성이 떨어진다. 여러 기능들이 추가되는 것 같기는 하지만 유저들이 직접 개발해서 사용할 수 없다. 그리고 그러한 인터페이스도 없다. 최근에 Jira 통합 기능이 추가된 것을 보고 Jira cloud 가 아닌 호스팅인 경우도 가능한지 문의해봤는데 지원 계획에 없다고 한다. 지원하지 않으면 쓰지 마라는 건데 개발자 친화적이지 않다.\n두 번째는 내보내기 기능이 별로다. 분명 export 되는 마크다운 파일들은 타이틀을 따라갈 줄 알았는데 대부분 해시값으로 되어 있다. 이 부분은 욕이 절로 나온다.\n세 번째는 무료가 아니다. 무료 인데 무료로 할 수 있는 게 제한적이다. 굳이 개인용으로 사용하겠다면 상관없지만 작성한 문서를 남들과 공유하려 할 때는 문제가 된다.\n최근, 레딧에서 노션의 대체 솔루션으로 Obsidian 이 언급되고 있다. 사용한지 한 달 정도 되었는데 매우 만족스럽다. 겉보기에는 동기화 기능이나 Publish 등 유료로 보이지만 vscode와 같이 확장 플러그인에 대한 커뮤니티가 상당히 발달돼 있어 커뮤니티 플러그인으로도 충분하다.\n또한, 개발자 친화적이고 인터페이스가 vscode를 많이 닮아있다. 또한 마크다운 친화적이라 문서 작성하기에도 간편하다. 협업 기능이 필요하지 않고 단순하게 위키로 정리하면서 스마트폰과 pc에서 간단하게 동기화될 수 있다. 현재는 Dropbox를 이용하여 파일들을 동기화하고 스마트폰에서 동일하게 구성하여 사용중이다. draw.io (현재는 Diagrams.net)와 excalidraw, mermaid 를 포함하여 jira 티켓의 상태(jira filter)도 플러그인을 통해 간단하게 동기화된다.\n달러도 비싸졌는데 노션 값이라도 아끼고자 하는 분들에게 추천드린다.\n", 
        "url": "\/\/localhost:1313\/posts\/obsidian\/"
    },
    
    "\/\/localhost:1313\/posts\/%EA%B8%89%EC%84%B1-%EC%A0%80%EC%9D%8C%EC%84%B1-%EB%82%9C%EC%B2%AD\/": {
        "title": "급성 저음성 난청",
        "tags": ["난청",],
        "content": "지난 월요일 아침 눈을 뜨고 평소처럼 회사 출근 준비를 하려는데 한쪽 귀가 멍하더니 소리가 잘 들리지 않았다. 예전에 백신을 맞고 돌발성 난청이 왔었는데 그 때와는 약간 다른 느낌이라 회사에 도착하자마자 근처 이비인후과를 방문했다. 약을 처방받고 일을 하려는데 시끄러운 기내 소음같이 사람들이 얘기할 때마다 현기증이 날 정도로 소음이 너무 심해 결국 조퇴를 했다.\n결국 다음날 인천에 좀 더 큰 병원을 가서 스트레스로 인한 난청이라는 진단을 받고 다량의 스테로이드를 처방받았다. 예전 돌발성 난청 때는 하루 이틀만에 약을 먹고 바로 나았었는데 바람소리, 기내 소음같은 저음성 이명이 일주일 째 없어지질 않는다.\n그 동안 매일 새벽 두시 넘어서 자고 인천에서 잠실까지 계속해서 출퇴근하며 무리했던 게 몸에 쌓였었던 것 같다. 친구들과 기분좋게 주말에 춘천에 다녀오니 긴장이 풀려서 그런지 안나타나던 병세가 곧바로 운좋게(?) 나타나주었다. 그동안 계속해서 몸 생각 안하고 무리만 했었는데 아프고 나니 뒤늦게 내 몸과 주변 사람들에 미안함을 느낀다.\n생각보다 이명이 꽤 고통스럽다. 노이즈캔슬링 헤드셋을 끼면 괜찮은데 조그마한 소음이라도 들리면 걷잡을 수 없이 이명 소리가 커진다. 소리가 안들리는 것보다도 하루빨리 이명이 없어졌으면 좋겠다.\n개발자로서 몸 관리하는 게 정말 어렵다. 공부 안하면 뒤처지는 것 같고 일과 병행하며 시간 내는 것도 쉽지가 않다. 그래도 몸이 최우선이니 수면 시간부터 조정해보고 공부하는 건 자투리 시간을 이용하는 것으로 해야겠다.\n", 
        "url": "\/\/localhost:1313\/posts\/%EA%B8%89%EC%84%B1-%EC%A0%80%EC%9D%8C%EC%84%B1-%EB%82%9C%EC%B2%AD\/"
    },
    
    "\/\/localhost:1313\/tags\/%EB%82%9C%EC%B2%AD\/": {
        "title": "난청",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/%EB%82%9C%EC%B2%AD\/"
    },
    
    "\/\/localhost:1313\/posts\/strcpy\/": {
        "title": "strcpy",
        "tags": ["strcpy","strlcpy","strscpy",],
        "content": "개요 한달 전 회사에서 리눅스 디바이스 드라이버 코드에 MISRA-C, CERT-C 룰셋들을 이용하여 정적분석을 하는 도중, strcpy 에 대한 warning 을 어떻게 처리할까 고민하다가 LWN 에서 Ushering out strlcpy() 라는 기사문을 읽게 되었다. string copy에 대한 글을 읽고 블로그에 정리하자고 했는데 이제서야 겨우 정리할 수 있게 되었다.\n리눅스 커널에서 문자열 복사를 위해 만들어진 매크로들은 다양하다. 몇 개의 시리즈(?)가 있는데 정리해보면 다음과 같다.\nstrcpy strncpy strlcpy strscpy strcpy strcpy를 나타내면 아래와 같이 간단하다.\nstrcpy(s, t) char *s, *t; { while (*s++ = *t++) ; } } 하지만 이 경우 발생가능한 문제는 destination 크기가 source보다 작을 경우 overrun이 발생한다는 점이다. 이를 개선하고자 만들어진 것이 strncpy 이다.\nstrncpy strncpy는 아래와 같은 프로토타입을 갖는다.\nchar *strncpy(char *dest, char *src, size_t n); 명시적으로 복사하고자 하는 크기를 인자로 넘겨주기 때문에 앞서 strcpy처럼 overrun이 발생할 일이 거의 없다. 하지만 이처럼 문제가 없어보이는 데에도 잠재적인 문제가 있다. 아래의 두 가지 경우를 살펴보자.\n인자 n 보다 source 가 짧은 경우 인자 n 보다 source 가 길 경우 첫 번째 경우에는 source 가 인자 n보다 작은데도 불구하고 전체 array를 복사하게 되는 불필요한 연산이 발생할 수 있다.\n두 번째 경우에는 source 가 인자 n보다 큰 경우이다. 이 경우 destination 은 NULL 로 끝나지 않게 돼 문자열로써 사용할 수 없다. 이러한 문제를 해결하기 위해 사용 버전이 strlcpy이다.\nstrlcpy BSD 계열의 커널에서는 strncpy를 해결하기 위해 strlcpy를 구현하였다.\nsize_t strlcpy(char *dest, const char *src, size_t n); 프로토타입은 strncpy와 비슷하다. 하지만 strncpy와의 한 가지 차이점은 strlcpy는 항상 destination 문자열이 NUL-terminated 라는 것을 보장한다는 점이다. 그리고 반환값으로 src의 길이를 반환하기 때문에 *dest로 반환된 문자열과 비교함으로써 정상적으로 문자열 복사가 이뤄졌는지 비교할 수 있다. 하지만 당시에 비효율적이라는 이유로 glibc 메인테이너와 커널 개발자들에게도 strlcpy는 환영받지 못했다.\nThis is horribly inefficient BSD crap. Using these function only leads to other errors. Correct string handling means that you always know how long your strings are and therefore you can you memcpy (instead of strcpy). Beside, those who are using strcat or variants deserved to be punished.\n맞는 말이긴 하다. source 문자열의 길이가 얼마인지 알고 있기 때문에 명시적으로 하자면 memcpy를 이용하면 되지 굳이 strlcpy를 이용해가면서 반환값을 재차 *dest와 비교하는 코드를 짤 필요는 없다. 하지만 이것보다 더 중요한 몇 가지 결함이 있다.\n실제 데이터가 복사될 수 없는 경우에도 source 문자열을 읽어야 한다. source 문자열을 신뢰할 수 없는 경우(non-NUL terminated)를 처리하지 못한다. race condition 이 존재한다. strlen 을 이용해 전체 소스 문자열의 길이를 확인하기 위해 읽어야 하는 문제점이 존재하고, 아래와 같이 구현되어 있는 strlcpy 는 만약 source 문자열이 NUL로 끝나지 않는 상태일 경우 문제가 발생할 수 있다. 실제 아래의 코드를 보면 그러한 경우가 발생했을 때 클라이언트 쪽에서 알 수 있는 방법이 없다.\nsize_t strlcpy(char *dest, const char *src, size_t size) { size_t ret = strlen(src); if (size) { size_t len = (ret \u0026gt;= size) ? size - 1 : ret; memcpy(dest, src, len); dest[len] = \u0026#39;\\0\u0026#39;; } return ret; } 또한, race condition이 발생할 수 있다. 이 부분은 언뜻 생각하지 못한 부분인데, src 의 길이를 가져오고 난 뒤 중간에서 src가 바뀌는 경우에는 이를 처리하지 못한다.\nstrscpy ssize_t strscpy(char *dest, const char *src, size_t count); 이러한 결점들을 해결한 함수가 바로 strscpy이다. 프로토타입만 보면 다른 점이 없다. 차이점은 반환값에 있다. strlcpy와 달리 strscpy는 복사된 문자들의 개수를 반환한다는 특징이 있고 실제 구현(https://elixir.bootlin.com/linux/v5.19.3/source/lib/string.cL151 )을 살펴보았을 때도 위의 간단한 문자열 복사방법과는 사뭇 다르다.\n마치며 현재 가장 최신 버전의 strscpy 함수에서는 kasan도 함께 공부해야 완전하게 함수를 이해할 수 있을 것 같다. 이젠 하다하다 문자열 하나 복사하는 함수조차 쉽게 이해하기 힘들어질 지경까지 이르렀다. 배워도 까먹어버리니 언젠간 다시 이 글도 다시 뒤적거릴 때가 올 것이다.\n출처 https://lwn.net/Articles/905777/ https://lwn.net/Articles/612244/ https://github.com/torvalds/linux/commit/30035e45753b708e7d47a98398500ca005e02b86 ", 
        "url": "\/\/localhost:1313\/posts\/strcpy\/"
    },
    
    "\/\/localhost:1313\/tags\/strcpy\/": {
        "title": "Strcpy",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/strcpy\/"
    },
    
    "\/\/localhost:1313\/tags\/strlcpy\/": {
        "title": "Strlcpy",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/strlcpy\/"
    },
    
    "\/\/localhost:1313\/tags\/strscpy\/": {
        "title": "Strscpy",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/strscpy\/"
    },
    
    "\/\/localhost:1313\/tags\/file-descriptor\/": {
        "title": "File Descriptor",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/file-descriptor\/"
    },
    
    "\/\/localhost:1313\/posts\/%ED%94%84%EB%A1%9C%EC%84%B8%EC%8A%A4-%EC%A2%85%EB%A3%8C%EC%99%80-%ED%8C%8C%EC%9D%BC-%EB%94%94%EC%8A%A4%ED%81%AC%EB%A6%BD%ED%84%B0\/": {
        "title": "프로세스 종료와 파일 디스크립터",
        "tags": ["file descriptor",],
        "content": "파일을 open 했는데 close를 안하면? stdin(0), stdout(2), stderr(3) 이라는 정해진 공식과 함께 리다이렉션과 파이프의 개념만으로도 흥분하던 대학교 시절에 내가 알던 파일 디스크립터의 정의는 태스크가 파일을 열면 얻게 되는 고유 id값 이었다. 그리고 이러한 파일디스크립터는 항상 open 을 해주면 close 를 해줘야 한다고 배웠다. 그런데 막상 단순한 텍스트 파일을 open 한 뒤 프로세스 종료 전 close를 명시적으로 하지 않아도 이로 인한 오류는 발생하지 않는다. 그 전에는 단순하게 프로세스 종료 시에 파일 디스크립터도 함께 정리해주겠거니 하고 넘어갔던 내용이지만 본 글에서는 이 부분에 대해서 간단히(?) 살펴보고자 한다.\nstrace 로 삽질 포인트 찾기 업무와는 관련이 없지만 간혹 코드에 open만 있는데도 불구하고 파일 디스크립터에 대한 에러가 발생하지 않는 불편한 코드들을 보면서 평소에 궁금했던 부분이라 잠깐 이 부분을 찾아보기로 했다. 먼저 아래와 같이 간단한 코드를 하나 작성한 뒤 컴파일해준다.\n// cat sysclose_test.c #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;syscall.h\u0026gt; #include \u0026lt;fcntl.h\u0026gt; int main(void) { int fd; fd = open(\u0026#34;test\u0026#34;, O_RDWR); exit(1); } 마지막 라인의 exit(1) 부분은 추가하지 않아도 인자만 다른 채로 동일한 시스템 콜이 호출된다. 이제 strace 를 통해 시스템콜이 어떻게 호출되는지 살펴보자.\n$ strace ./sysclose_test execve(\u0026#34;./sysclose_test\u0026#34;, [\u0026#34;./sysclose_test\u0026#34;], 0x7fff0fb6ea80 /* 40 vars */) = 0 mmap(0x7fcdffece000, 360448, PROT_READ, MAP_PRIVATE|MAP_FIXED|MAP_DENYWRITE, 3, 0x1bc000) = 0x7fcdffece000 mmap(0x7fcdfff27000, 24576, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_DENYWRITE, 3, 0x214000) = 0x7fcdfff27000 mmap(0x7fcdfff2d000, 52816, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_ANONYMOUS, -1, 0) = 0x7fcdfff2d000 close(3) = 0 mmap(NULL, 8192, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7fcdffd10000 arch_prctl(ARCH_SET_FS, 0x7fcdfff3b5c0) = 0 set_tid_address(0x7fcdfff3b890) = 375046 set_robust_list(0x7fcdfff3b8a0, 24) = 0 mprotect(0x7fcdfff27000, 16384, PROT_READ) = 0 mprotect(0x55c5fce58000, 4096, PROT_READ) = 0 mprotect(0x7fcdfff81000, 8192, PROT_READ) = 0 prlimit64(0, RLIMIT_STACK, NULL, {rlim_cur=8192*1024, rlim_max=RLIM64_INFINITY}) = 0 munmap(0x7fcdfff3c000, 71131) = 0 openat(AT_FDCWD, \u0026#34;test\u0026#34;, O_RDWR) = 3 exit_group(0) = ? +++ exited with 0 +++ 마지막으로 출력된 라인으로부터 exit_group(0) 시스템 콜이 호출된 것을 알 수 있다. 삽질 포인트를 찾았으니 이제 본격적으로 삽질할 준비는 되었다.\ndo_exit -\u0026gt; exit_files exit_group(0) 을 시작으로 함수콜을 따라가다보면 다음과 같은 호출 경로를 찾아낼 수 있다.\n-\u0026gt; exit_group() +-\u0026gt; do_exit() +-\u0026gt; exit_files() +-\u0026gt; put_files_struct() +-\u0026gt; close_files() 핵심은 종료될 태스크의 파일 리스트를 얻은 뒤 NULL로 바꾸고, put_files_struct 에 리스트를 인자로 넘기면서 파일들을 close 하는 부분이다. 이 부분을 통해서 왜 굳이 명시적으로 close 를 하지 않아도 태스크 종료 시에 파일 디스크립터들이 정리되는지 알 수 있다.\nvoid exit_files(struct task_struct *tsk) { struct files_struct * files = tsk-\u0026gt;files; if (files) { io_uring_files_cancel(files); task_lock(tsk); tsk-\u0026gt;files = NULL; task_unlock(tsk); put_files_struct(files); } } void put_files_struct(struct files_struct *files) { if (atomic_dec_and_test(\u0026amp;files-\u0026gt;count)) { struct fdtable *fdt = close_files(files); /* free the arrays if they are not embedded */ if (fdt != \u0026amp;files-\u0026gt;fdtab) __free_fdtable(fdt); kmem_cache_free(files_cachep, files); } } static struct fdtable *close_files(struct files_struct * files) { /* * It is safe to dereference the fd table without RCU or * -\u0026gt;file_lock because this is the last reference to the * files structure. */ struct fdtable *fdt = rcu_dereference_raw(files-\u0026gt;fdt); unsigned int i, j = 0; for (;;) { unsigned long set; i = j * BITS_PER_LONG; if (i \u0026gt;= fdt-\u0026gt;max_fds) break; set = fdt-\u0026gt;open_fds[j++]; while (set) { if (set \u0026amp; 1) { struct file * file = xchg(\u0026amp;fdt-\u0026gt;fd[i], NULL); if (file) { filp_close(file, files); cond_resched(); } } i++; set \u0026gt;\u0026gt;= 1; } } return fdt; } 마치며 파일이 닫히는 것까지 커널 코드를 읽고 분석하는데 시간이 꽤 걸릴 것이라 생각했지만 생각보다 단계가 단순하여 금방 이해할 수 있었다. 한가지 중요한 것은, 본 글에서 분석한 내용이 코드 상에서 close()를 하지 않아도 된다는 의미는 아니라는 점이다. 리눅스에서는 디바이스 노드로써 디바이스를 컨트롤한다. 이 경우 open()과 close()를 명시적으로 사용하지 않으면 디바이스 초기화 시점을 코드로써 기술할 수 없게 되므로 문제가 발생할 가능성이 높아지기 때문에 주의해야 한다.\n", 
        "url": "\/\/localhost:1313\/posts\/%ED%94%84%EB%A1%9C%EC%84%B8%EC%8A%A4-%EC%A2%85%EB%A3%8C%EC%99%80-%ED%8C%8C%EC%9D%BC-%EB%94%94%EC%8A%A4%ED%81%AC%EB%A6%BD%ED%84%B0\/"
    },
    
    "\/\/localhost:1313\/categories\/computer-science\/": {
        "title": "Computer Science",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/categories\/computer-science\/"
    },
    
    "\/\/localhost:1313\/posts\/device-tree-overlay\/": {
        "title": "Device Tree Overlay",
        "tags": ["device tree","overlay","kernel",],
        "content": "Ramoops 덕분에 알게된 오버레이 며칠전 리눅스에서의 Tracing 방법에 대해 공부하다가 찾아낸 세미나 영상에서 ramoops 라는 것을 알게 되었다. ramoops는 커널이 oops/panic 이 발생하면서 warm reset 되었을 경우 재부팅 이후에 pstore (persistent store)을 이용하여 이전에 기록된 dmesg 나 user 콘솔의 기록을 확인할 수 있도록 하는 logger 이다. ramoops 는 cold reset 이 되면 기록이 남아있지 않는다는 단점이 있어 최근에는 ramoops 대신 blk oops/panic logger 를 사용하기도 한다.\n이러한 로거를 현업에서 사용하기 위해 사내 평가보드에서 먼저 확인해보았다. 평가보드에서는 간단하게 memblock의 reserved memory 영역에 ramoops 영역을 추가함으로써 정상 동작하는 것을 금방 확인할 수 있었다. 하지만, 개인적으로 갖고 있던 라즈베리파이 보드에서는 이같은 방법이 제대로 동작하지 않았다. 이에 구글링을 하던 도중 디바이스 트리 오버레이로 ramoops 에 대한 디바이스 트리를 수정하는 방법을 접하면서, 오버레이가 특정 벤더의 BSP에서만 사용 가능한 것이 아닌 OF의 API로서 커널 내에 구현되어 있다는 사실 또한 함께 알게 되었다. (부끄럽게도 이제서야 알게 되었다.)\n오버레이 작성 오버레이는 런타임에 FDT (Flattened Device Tree) 를 수정할 수 있는 방법이다. 여기서 FDT란, 메모리에 로드된 디바이스 트리를 말한다. DTC (Device Tree Compiler) 버전에 따라 syntax 가 조금씩 달라지지만, 이전 방법으로는 아래와 같이 작성할 수 있다.\n/dts-v1/; /plugin/; / { compatible = \u0026#34;brcm,bcm2835\u0026#34;; fragment@0 { target = \u0026lt;\u0026amp;rmem\u0026gt;; __overlay__ { #address-cells= \u0026lt;1\u0026gt;; #size-cells = \u0026lt;1\u0026gt;; ranges; ramoops: ramoops@39000000{ compatible = \u0026#34;ramoops\u0026#34;; reg = \u0026lt;0x39000000 0x100000\u0026gt;; ecc-size = \u0026lt;16\u0026gt;; record-size = \u0026lt;0x20000\u0026gt;; console-size = \u0026lt;0x20000\u0026gt;; pmsg-size = \u0026lt;0x20000\u0026gt;; ftrace-size = \u0026lt;0\u0026gt;; }; }; }; }; 그리고 이를 아래와 같이 컴파일한다.\n$ dtc -@ -O dtb -o ramoops.dtbo ramoops-overlay.dts configfs 앞서 컴파일한 오버레이를 사용하기 위해서는 디바이스 트리를 사용했던 것과 마찬가지로 특정 메모리 영역에 dtbo 파일을 두고 오버레이 인터페이스를 통해 접근해야 한다. 하지만 overlay에 관련된 API 를 직접 호출할 필요 없이도 configfs 를 통해 쉽게 오버레이를 추가하거나 제거할 수 있다. 커널에서는 아래와 같이 DT overlay interface 로서 CONFIGFS 를 제공한다.\nCONFIG_OF_CONFIGFS: Enable a simple user-space driven DT overlay interface. Symbol: OF_CONFIGFS [=y] Type : bool Defined at drivers/of/Kconfig:97 Prompt: Device Tree Overlay ConfigFS interface Depends on: OF [=y] Location: -\u0026gt; Device Drivers -\u0026gt; Device Tree and Open Firmware support (OF [=y]) Selects: CONFIGFS_FS [=y] \u0026amp;\u0026amp; OF_OVERLAY [=y] 만약 이 커널 옵션이 정상적으로 활성화되어 빌드되었다면 아래와 같이 /sys/kernel/configs 경로에 configfs 파일시스템이 마운트 되어 있는 것을 확인할 수 있다.\n$ mount | grep -i config configfs on /sys/kernel/config type configfs (rw,nosuid,nodev,noexec,relatime) 먼저, 파일시스템을 탐색해보면 아무것도 없다. /sys/kernel/config/device-tree/overlay/ 까지의 디렉토리만 생성되어 있을 뿐 아무런 파일도 존재하지 않는다. 이 때, 임시로 디렉토리 하나를 만들어주면 아래와 같이 파일 여러개가 생성되어 있는 것을 알 수 있다. 그리고 해당 파일들의 내용을 보면 비어 있다.\n$ mkdir /sys/kernel/config/device-tree/overlays/test $ ls /sys/kernel/config/device-tree/overlays/test dtbo path status $ grep \u0026#34;\u0026#34; /sys/kernel/config/device-tree/overlays/test/* /sys/kernel/config/device-tree/overlays/test/path: /sys/kernel/config/device-tree/overlays/test/status:unapplied 이제, 앞서 컴파일 해놓은 dtbo 파일을 해당 파일시스템을 통해 로드해보자. 앞서 생성한 ramoops 노드가 동적으로 런타임에 추가된 것을 볼 수 있다(!!)\n$ cat ramoops.dtbo \u0026gt; /sys/kernel/config/device-tree/overlays/test/dtbo $ ls /proc/device-tree/reserved-memory/ -al total 0 drwxr-xr-x 5 root root 0 Apr 26 04:39 . drwxr-xr-x 25 root root 0 Jul 21 2021 .. -r--r--r-- 1 root root 4 Apr 26 04:51 \u0026#39;#address-cells\u0026#39; drwxr-xr-x 2 root root 0 Apr 26 04:51 linux,cma -r--r--r-- 1 root root 16 Apr 26 04:51 name -r--r--r-- 1 root root 4 Apr 26 04:51 phandle drwxr-xr-x 2 root root 0 Apr 26 10:46 ramoops@39000000 -r--r--r-- 1 root root 0 Apr 26 10:46 ranges -r--r--r-- 1 root root 4 Apr 26 04:51 \u0026#39;#size-cells\u0026#39; $ grep \u0026#34;\u0026#34; /sys/kernel/config/device-tree/overlays/test/* Binary file /sys/kernel/config/device-tree/overlays/test/dtbo matches /sys/kernel/config/device-tree/overlays/test/path: /sys/kernel/config/device-tree/overlays/test/status:applied 이렇게 오버레이를 통해 노드를 로드한 뒤에는 드라이버도 함께 신경써줘야 한다. 만약 관련된 드라이버가 built-in 되어 컴파일된 경우라면 자동으로 로드되지만 모듈로 빌드된 경우에는 반드시 modprobe 명령어로 로드해줘야 한다. 더이상 필요하지 않은 경우에는 아래와 같이 단순하게 디렉토리를 삭제해주면 된다.\n$ rmdir /sys/kernel/config/device-tree/overlays/test/ 언제 사용할까? 오버레이를 통해 런타임에 동적으로 FDT 의 내용을 변경할 수 있다는 점은 충분히 매력적이다. 어째서 라즈베리파이에서 상당 부분의 모듈들을 오버레이를 통해 제공하고 있는지도 함께 이해할 수 있었다. 현재 현업에서는 오버레이는 적용되지 않은채 디바이스 트리의 상속을 통해서 구조화 시킨채 FDT는 고정적으로 사용하고 있다. 만약 시나리오에 따라 디바이스 트리가 변경되도록 BSP를 개발해야 하는 때가 온거나 현재 커널 내에 구현된 유닛테스트와 같이 고정된 테스트 코드에 동적인 설정값들을 사용해야 한다면 오버레이가 그 해답이 될 수 있을 것이라 생각한다.\n참고 자료 Android Device Tree Overlay dtbocfg Dynamically Loading Device Tree Overlay ", 
        "url": "\/\/localhost:1313\/posts\/device-tree-overlay\/"
    },
    
    "\/\/localhost:1313\/tags\/kernel\/": {
        "title": "Kernel",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/kernel\/"
    },
    
    "\/\/localhost:1313\/tags\/overlay\/": {
        "title": "Overlay",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/overlay\/"
    },
    
    "\/\/localhost:1313\/posts\/525.-contiguous-array\/": {
        "title": "525. Contiguous Array",
        "tags": ["525번","HashMap",],
        "content": "HashMap을 이용한 문제 문제를 읽고 HashMap을 이용한 방법이 한번에 와닿지 않아 이를 다시 한번 더 정리하고자 한다. 이번 \u0026ldquo;Contiguous Array\u0026rdquo; 문제는 배열 내의 0, 1 개수가 동일한 최대 길이를 구하는 문제이다. 즉, [0,1,0], [0,1,0,1,1] 등은 각각 2, 4가 된다. 이를 풀기 위해서는 {count: index} 로 구성된 HashMap 을 이용한다. 이 때 count 값은 0부터 시작하여 0일 경우 -1, 1일 경우 +1을 더하여 구하면 주어진 배열에 대해 아래와 같이 1차원 그래프가 나온다.\n(예) [0, 1, 0, 1, 1, 1, 0]\n여기서 중요한 점은 동일한 count 값을 가지는 포인트가 최대 길이가 된다는 점이다. HashMap의 초기값을 {0: -1} 이라고 했을 때, 예제 배열에서 [0, 1] 이 지난 이후의 해시맵은 아래와 같다.\n{ # count: index 0: -1, -1: 0 } 인덱스 1에서 count 값은 0이므로, 기존에 저장되어 있던 map[0] 과의 차이를 구해주면 최대길이 2를 얻을 수 있다. 배열의 바이너리 값을 count 값을 이용한 또 다른 값의 형태로 변환하고 이를 이용하여 해답을 찾는 방식이 매우 새로웠던 문제이다.\n", 
        "url": "\/\/localhost:1313\/posts\/525.-contiguous-array\/"
    },
    
    "\/\/localhost:1313\/tags\/525%EB%B2%88\/": {
        "title": "525번",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/525%EB%B2%88\/"
    },
    
    "\/\/localhost:1313\/tags\/hashmap\/": {
        "title": "HashMap",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/hashmap\/"
    },
    
    "\/\/localhost:1313\/categories\/leetcode\/": {
        "title": "Leetcode",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/categories\/leetcode\/"
    },
    
    "\/\/localhost:1313\/tags\/canary\/": {
        "title": "Canary",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/canary\/"
    },
    
    "\/\/localhost:1313\/tags\/ssp\/": {
        "title": "Ssp",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/ssp\/"
    },
    
    "\/\/localhost:1313\/tags\/stack\/": {
        "title": "Stack",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/stack\/"
    },
    
    "\/\/localhost:1313\/posts\/stack-protector%EC%99%80-%EB%94%94%EB%B2%84%EA%B9%85-%EC%9D%B4%EC%95%BC%EA%B8%B0\/": {
        "title": "Stack Protector와 디버깅 이야기",
        "tags": ["ssp","stack","canary",],
        "content": "Stack Protector, 넌 뭐하는 놈이냐? 현업에서 커널의 CONFIG_STACK_PROTECTOR 를 활성화하면 커널 부트가 안된다는 이슈가 보고되었다. Trace32 로 callstack을 살펴보니 내 파트에서 맡고 있는 디바이스 드라이버 코드 때문에 Stack Overflow가 발생하여 부트가 안되고 있었다. 문제의 지점은 사수가 발견하고 파트장의 수정으로 마무리되었다.\n하지만 이슈가 마무리 되고 Stack Protector 가 어떤 원리로 동작하는지 궁금했고 설 연휴를 맞아 자세하게 정리할 수 있었다. 그 과정에서 우분투에 잘못된 버그 리포트 티켓을 만들어내긴 했지만 말이다.\nStack Protection은 GCC의 -fstack-protector, -fstack-protector-all, -fstack-protector-strong 옵션을 통해 활성화할 수 있고 -fno-stack-protector 옵션으로 비활성화 할 수 있다.\n스택 레이아웃 살펴보기 비활성화 시의 레이아웃 먼저, Stack Protector를 비활성화/활성화 되었을 때의 각각 콜스택이 어떻게 되는지 살펴보자.\n먼저, 아래와 같이 간단한 코드를 준비하였다.\nvoid mul(int a) { a = a * 2; } void add(int a, int b, int c, int d) { int j = a + 1; int e = b + 2; int f = c + 3; int g = d + 4; mul(j + e + f + g); } int main() { int a = 0; int b = 1; int c = 2; int d = 3; add(a, b, c, d); return a; } 호출 전/후로 ARM64 아키텍처에서의 Calling Convention을 확인하기 편하도록 최대한 코드를 할당 중심으로 작성하였다. 위 코드를 아래의 명령어로 컴파일한 후 다시 어셈블리로 바꿔주자.\n$ aarch64-linux-gnu-gcc callstack.c \\ -fno-stack-protector \\ -fno-asynchronous-unwind-tables \\ -fno-exceptions \\ -fno-rtti -fverbose-asm \\ -o callstack.o $ aarch64-linux-gnu-objdump -dS callstack.o \u0026gt; callstack.disassemble 위와 같이 오브젝트 파일을 만들었다가 다시 dump 하는 이유는 불필요한 어셈블리 레이블을 없애기 위해서다. 실제로 gcc의 -S 옵션을 사용하여 단순하게 어셈블리 코드를 만들어내면 원하는 어셈 코드를 얻기 힘들다. 이제 얻어낸 어셈블리 코드를 살펴보자.\n0000000000000714 \u0026lt;mul\u0026gt;: 714:\td10043ff sub\tsp, sp, #0x10 718:\tb9000fe0 str\tw0, [sp, #12] 71c:\tb9400fe0 ldr\tw0, [sp, #12] 720:\t531f7800 lsl\tw0, w0, #1 724:\tb9000fe0 str\tw0, [sp, #12] 728:\td503201f nop 72c:\t910043ff add\tsp, sp, #0x10 730:\td65f03c0 ret 0000000000000734 \u0026lt;add\u0026gt;: 734:\ta9bd7bfd stp\tx29, x30, [sp, #-48]! 738:\t910003fd mov\tx29, sp 73c:\tb9001fe0 str\tw0, [sp, #28] 740:\tb9001be1 str\tw1, [sp, #24] 744:\tb90017e2 str\tw2, [sp, #20] 748:\tb90013e3 str\tw3, [sp, #16] 74c:\tb9401fe0 ldr\tw0, [sp, #28] 750:\t11000400 add\tw0, w0, #0x1 754:\tb9002fe0 str\tw0, [sp, #44] 758:\tb9401be0 ldr\tw0, [sp, #24] 75c:\t11000800 add\tw0, w0, #0x2 760:\tb9002be0 str\tw0, [sp, #40] 764:\tb94017e0 ldr\tw0, [sp, #20] 768:\t11000c00 add\tw0, w0, #0x3 76c:\tb90027e0 str\tw0, [sp, #36] 770:\tb94013e0 ldr\tw0, [sp, #16] 774:\t11001000 add\tw0, w0, #0x4 778:\tb90023e0 str\tw0, [sp, #32] 77c:\tb9402fe1 ldr\tw1, [sp, #44] 780:\tb9402be0 ldr\tw0, [sp, #40] 784:\t0b000021 add\tw1, w1, w0 788:\tb94027e0 ldr\tw0, [sp, #36] 78c:\t0b000021 add\tw1, w1, w0 790:\tb94023e0 ldr\tw0, [sp, #32] 794:\t0b000020 add\tw0, w1, w0 798:\t97ffffdf bl\t714 \u0026lt;mul\u0026gt; 79c:\td503201f nop 7a0:\ta8c37bfd ldp\tx29, x30, [sp], #48 7a4:\td65f03c0 ret 00000000000007a8 \u0026lt;main\u0026gt;: 7a8:\ta9be7bfd stp\tx29, x30, [sp, #-32]! 7ac:\t910003fd mov\tx29, sp 7b0:\tb9001fff str\twzr, [sp, #28] 7b4:\t52800020 mov\tw0, #0x1 // #1 7b8:\tb9001be0 str\tw0, [sp, #24] 7bc:\t52800040 mov\tw0, #0x2 // #2 7c0:\tb90017e0 str\tw0, [sp, #20] 7c4:\t52800060 mov\tw0, #0x3 // #3 7c8:\tb90013e0 str\tw0, [sp, #16] 7cc:\tb94013e3 ldr\tw3, [sp, #16] 7d0:\tb94017e2 ldr\tw2, [sp, #20] 7d4:\tb9401be1 ldr\tw1, [sp, #24] 7d8:\tb9401fe0 ldr\tw0, [sp, #28] 7dc:\t97ffffd6 bl\t734 \u0026lt;add\u0026gt; 7e0:\tb9401fe0 ldr\tw0, [sp, #28] 7e4:\ta8c27bfd ldp\tx29, x30, [sp], #32 7e8:\td65f03c0 ret 어셈블리 코드를 살펴보면 main, add, mul 함수가 호출될 때마다 첫번째 라인에서 x29/x30 값을 sp (스택 포인터)에 저장하고 Stack Frame을 확보하는 것을 알 수 있다. x29는 Frame Pointer, x30은 Link Register로서 사용되며 각각 스택 프레임의 base, Return Address를 갖고 있다고 생각하면 된다. add 함수가 mul 함수에서 각각 stack frame을 확보하는 어셈블리 명령어가 다르게 나와있다. add 함수를 먼저 살펴보면 아래와 같다.\nstp x29, x30, [sp, #-48]! 이는 x29, x30 값을 [sp]에 저장(sp가 갖고 있는 메모리 주소에)한 다음 sp를 -48 오프셋만큼 이동하라는 뜻이다. 이 때, 스택 할당은 메모리 반대 방향으로 확보되는 점에 주목하자. mul 함수는 별도의 백업 없이 곧바로 스택 프레임을 확보하는 것을 볼 수 있다.\n계속해서 add 함수를 살펴보면 스택 프레임을 확보한 후 int d, e, f에 해당하는 지역 변수들을 스택에 저장하는 것을 알 수 있다. x29는 stack frame pointer, x30은 return address를 저장하고 있다.\n활성화 시의 레이아웃 그렇다면 stack protector가 활성화된 메모리 레이아웃은 어떻게 될까? 이번에는 -fstack-protector-all 옵션을 이용하여 어셈블리 코드를 생성해주자.\n000000000000086c \u0026lt;add\u0026gt;: 86c:\ta9bc7bfd stp\tx29, x30, [sp, #-64]! 870:\t910003fd mov\tx29, sp 874:\tb9001fe0 str\tw0, [sp, #28] 878:\tb9001be1 str\tw1, [sp, #24] 87c:\tb90017e2 str\tw2, [sp, #20] 880:\tb90013e3 str\tw3, [sp, #16] 884:\t90000080 adrp\tx0, 10000 \u0026lt;__FRAME_END__+0xf520\u0026gt; 888:\tf947f400 ldr\tx0, [x0, #4072] 88c:\tf9400001 ldr\tx1, [x0] 890:\tf9001fe1 str\tx1, [sp, #56] 894:\td2800001 mov\tx1, #0x0 // #0 898:\tb9401fe0 ldr\tw0, [sp, #28] 89c:\t11000400 add\tw0, w0, #0x1 8a0:\tb9002be0 str\tw0, [sp, #40] 8a4:\tb9401be0 ldr\tw0, [sp, #24] 8a8:\t11000800 add\tw0, w0, #0x2 8ac:\tb9002fe0 str\tw0, [sp, #44] 8b0:\tb94017e0 ldr\tw0, [sp, #20] 8b4:\t11000c00 add\tw0, w0, #0x3 8b8:\tb90033e0 str\tw0, [sp, #48] 8bc:\tb94013e0 ldr\tw0, [sp, #16] 8c0:\t11001000 add\tw0, w0, #0x4 8c4:\tb90037e0 str\tw0, [sp, #52] 8c8:\tb9402be1 ldr\tw1, [sp, #40] 8cc:\tb9402fe0 ldr\tw0, [sp, #44] 8d0:\t0b000021 add\tw1, w1, w0 8d4:\tb94033e0 ldr\tw0, [sp, #48] 8d8:\t0b000021 add\tw1, w1, w0 8dc:\tb94037e0 ldr\tw0, [sp, #52] 8e0:\t0b000020 add\tw0, w1, w0 8e4:\t97ffffcc bl\t814 \u0026lt;mul\u0026gt; 8e8:\td503201f nop 8ec:\t90000080 adrp\tx0, 10000 \u0026lt;__FRAME_END__+0xf520\u0026gt; 8f0:\tf947f400 ldr\tx0, [x0, #4072] 8f4:\tf9401fe2 ldr\tx2, [sp, #56] 8f8:\tf9400001 ldr\tx1, [x0] 8fc:\teb010042 subs\tx2, x2, x1 900:\td2800001 mov\tx1, #0x0 // #0 904:\t54000040 b.eq\t90c \u0026lt;add+0xa0\u0026gt; // b.none 908:\t97ffff66 bl\t6a0 \u0026lt;__stack_chk_fail@plt\u0026gt; 90c:\ta8c47bfd ldp\tx29, x30, [sp], #64 910:\td65f03c0 ret 보기에도 이전에 살펴봤던 add 함수보다 훨씬 코드가 길어졌다. 여기서 중요한 건 함수 초기에 스택 포인터를 움직인 후 canary 영역을 스택에 저장하는 부분이다.\n884:\t90000080 adrp\tx0, 10000 \u0026lt;__FRAME_END__+0xf520\u0026gt; 888:\tf947f400 ldr\tx0, [x0, #4072] 88c:\tf9400001 ldr\tx1, [x0] 890:\tf9001fe1 str\tx1, [sp, #56] ... 908:\t97ffff66 bl\t6a0 \u0026lt;__stack_chk_fail@plt\u0026gt; Stack Frame Pointer와 Link Register 정보를 스택 하위에 두고 일반적으로는 곧바로 지역변수들이 위치하지만 Stack Protector 를 활성화하면 이 영역이 canary 영역으로 채워지는 것을 알 수 있다.\nStack Smashing 에러가 안난다? Canary Boundary ARM Reference 문서에 나와있는 예제 (https://developer.arm.com/documentation/101754/0616/armclang-Reference/armclang-Command-line-Options/-fstack-protector---fstack-protector-all---fstack-protector-strong---fno-stack-protector ) 로 직접 확인해보려 했지만 의도된대로 에러가 발생하지 않았다. 이에 직접 GDB 를 이용하여 디버깅을 해보니 아래와 같이 fs:0x28, 즉 canary value의 하위 8비트가 0으로 초기화되어 있었다.\nfs, gs 레지스터는 특별한 운영체제의 자료구조에 접근하기 위한 것이다. 특히, FS:0x28은 리눅스에서 stack-guard 값을 저장하고 stack-guard check 루틴에서 사용된다. (https://stackoverflow.com/questions/10325713/why-does-this-memory-address-fs0x28-fs0x28-have-a-random-value ) 그런데 fs:0x28 값이 처음부터 하위 1바이트가 초기화되어 있다는 것은 커널 쪽 코드에 의한 것이라고 생각하고 살펴보니, 커널 include/linux/random.h 파일에 아래의 코드가 있었다.\n/* * On 64-bit architectures, protect against non-terminated C string overflows * by zeroing out the first byte of the canary; this leaves 56 bits of entropy. */ #ifdef CONFIG_64BIT # ifdef __LITTLE_ENDIAN # define CANARY_MASK 0xffffffffffffff00UL # else /* big endian, 64 bits: */ # define CANARY_MASK 0x00ffffffffffffffUL # endif #else /* 32 bits: */ # define CANARY_MASK 0xffffffffUL #endif static inline unsigned long get_random_canary(void) { unsigned long val = get_random_long(); return val \u0026amp; CANARY_MASK; } 처음에는 굳이 이렇게 NULL을 처리해야 하나 싶었는데 블라인드를 통해 알게된 사실은 canary value를 바로 출력하지 못하도록 NULL 문자를 이용해 boundary를 생성하기 위한 용도라는 것을 알게됐다. 이로써 stack protector가 스택에서 어떻게 위치하는지, 그리고 왜 ARM 레퍼런스 문서에 있는 예제가 동작을 하지 않는지, canary boundary 값이 왜 NULL로 되어있는지 등을 알 수 있었다.\n참고 자료 FS/GS 레지스터 in Stackoverflow stp 레지스터 사용 예 ARM 어셈블리 강좌 자료 - Function Calls ARM64 스택 분석 자료 GCC 어셈블리 strip 방법 #1 GCC 어셈블리 strip 방법 #2 ", 
        "url": "\/\/localhost:1313\/posts\/stack-protector%EC%99%80-%EB%94%94%EB%B2%84%EA%B9%85-%EC%9D%B4%EC%95%BC%EA%B8%B0\/"
    },
    
    "\/\/localhost:1313\/tags\/preprocessor\/": {
        "title": "Preprocessor",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/preprocessor\/"
    },
    
    "\/\/localhost:1313\/posts\/%EC%BB%B4%ED%8C%8C%EC%9D%BC-%ED%83%80%EC%9E%84%EC%97%90-%EB%A7%A4%ED%81%AC%EB%A1%9C-%EB%B3%80%EC%88%98-%EA%B0%92-%ED%99%95%EC%9D%B8%ED%95%98%EA%B8%B0\/": {
        "title": "컴파일 타임에 매크로 변수 값 확인하기",
        "tags": ["preprocessor",],
        "content": "개요 현업에서 사용하는 코드 중 상당히 많은 부분들이 매크로 변수 형태로 정의되어 사용되고 있다. 이러한 변수들은 런타임 때 정의되지 않기 때문에 굳이 값을 확인하기 위해서 불필요하게 런타임에서까지 확인해볼 필요는 없지만, 통상적으로 이러한 변수들의 값이 어떻게 설정되어 있는지에 대한 로깅 코드들이 많이 있다.\n그렇다면, 컴파일 타임에서 매크로 변수의 값을 알 수 있는 방법은 없을까? #pragma 와 같은 전처리 키워드를 사용하면 가능하다. 예를 들어, 아래의 코드를 보자.\n#include \u0026lt;stdio.h\u0026gt; /* #define VAR_NAME_VALUE(var) #var \u0026#34;=\u0026#34; VALUE(var) */ #define DO_PRAGMA(x) _Pragma (#x) /* Some test definition here */ #define DEFINED_BUT_NO_VALUE #define DEFINED_INT 3 #define DEFINED_STR \u0026#34;ABC\u0026#34; /* definition to expand macro then apply to pragma message */ #define VALUE_TO_STRING(x) #x #define VALUE(x) VALUE_TO_STRING(x) #define VAR_NAME_VALUE(var) #var \u0026#34;=\u0026#34; VALUE_TO_STRING(var) /* Some example here */ #pragma message(VAR_NAME_VALUE(NOT_DEFINED)) #pragma message(VAR_NAME_VALUE(DEFINED_BUT_NO_VALUE)) #pragma message(VAR_NAME_VALUE(DEFINED_INT)) #pragma message(VAR_NAME_VALUE(DEFINED_STR)) #define PRINT_INT(x) DO_PRAGMA(message(VAR_NAME_VALUE(DEFINED_INT))) PRINT_INT(DEFINED_INT); #if DEFINED_INT \u0026gt; 2 #warning \u0026#34;ERROR\u0026#34; #endif int main(void) { printf(\u0026#34;This is sample application to make compiler to \\ show the value of macro variable\\n\u0026#34;); return 0; } 위와 같이 매크로를 정의하면, 컴파일 시에 PRINT_INT 매크로 뿐만 아니라 #pragma message(...) 를 이용하여 각각의 매크로 변수들의 값을 직접 출력할 수 있다.\n이제 커널의 fixmap 영역이 정의된 부분에서 확인해보자. (사실 fixmap 영역에서 사용하는 값들을 직접 확인해보기 위함이었는데 enum 으로 정의되어 있는 부분은 제대로 출력되지 않았다.)\n... }; #pragma message(VAR_NAME_VALUE(FIX_FDT_SIZE)) ./arch/arm64/include/asm/fixmap.h:102:9: note: #pragma message: FIX_FDT_SIZE=(0x00200000 + 0x00200000) 102 | #pragma message(VAR_NAME_VALUE(FIX_FDT_SIZE)) | ^~~~~~~ ", 
        "url": "\/\/localhost:1313\/posts\/%EC%BB%B4%ED%8C%8C%EC%9D%BC-%ED%83%80%EC%9E%84%EC%97%90-%EB%A7%A4%ED%81%AC%EB%A1%9C-%EB%B3%80%EC%88%98-%EA%B0%92-%ED%99%95%EC%9D%B8%ED%95%98%EA%B8%B0\/"
    },
    
    "\/\/localhost:1313\/tags\/gcc\/": {
        "title": "Gcc",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/gcc\/"
    },
    
    "\/\/localhost:1313\/tags\/kasan\/": {
        "title": "Kasan",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/kasan\/"
    },
    
    "\/\/localhost:1313\/posts\/%EC%BB%A4%EB%84%90%EC%9D%98-kasan-%EC%BD%94%EB%93%9C%EA%B0%80-%EC%82%BD%EC%9E%85%EB%90%98%EB%8A%94-%EB%B0%A9%EB%B2%95\/": {
        "title": "커널의 KASAN 코드가 삽입되는 방법",
        "tags": ["kasan","gcc",],
        "content": "커널의 KASAN 코드가 삽입되는 방법 Generic KASAN 의 경우, 위와 같이 __asan_load와 __asan_store 함수가 정의되어 있다. 단순하게 KASAN의 사용법만 보았을 때, 과연 커널에서 어떻게 모든 메모리에 접근할 때마다 특정 함수의 내용을 실행할까 라는 궁금증이 생겼다. 커널 문서에 따르면, 컴파일러에 의해 위 함수들이 인라인 형태로 모든 메모리 접근 전에 삽입되어 해당 메모리가 안전한지 확인한다고 기술하고 있다. 이에 처음에는 static inline 형태로 정의된 함수가 컴파일러에 의해 처리되는 것인가? 라고 생각했다. 하지만, 실제 코드를 보았을 때 함수와 EXPORT_SYMBOL 이 사용된 것 외에는 그 어디에도 inline 키워드는 사용도지 않았다. 컴파일러가 해당 코드를 삽입한다고 하는데 정확하게 어떻게 삽입하는지, 해당 함수들의 이름이 바뀌면 어떤 결과가 나올지 궁금했다. 또한 커널 소스를 다 뒤져봐도 ASAN에 관련된 호출 부분을 아무리 찾아도 어떤 방식으로 __asan_loadN, __asan_storeN 이 메모리 접근 전에 삽입되는지 찾을 수 없었다.\nKASAN 지원 여부가 컴파일러 버전에 따라 달라지는 것을 확인하고, 이에 컴파일러가 관련된 코드를 삽입하는 것을 직접 확인하기 위해 GCC 코드를 살펴보았다.\nGCC 코드 GCC 코드($gcc_root/gcc/sanitizer.def)에는 커널에서 _asan* 형태로 정의해놓은 심볼에 대해 DEF_SANITIZER_BUILTIN 이라는 매크로와 함께 아래와 같이 정의해놓았다.\nDEF_SANITIZER_BUILTIN(BUILT_IN_ASAN_LOAD1, \u0026#34;__asan_load1\u0026#34;, BT_FN_VOID_PTR, ATTR_TMPURE_NOTHROW_LEAF_LIST) DEF_SANITIZER_BUILTIN(BUILT_IN_ASAN_LOAD2, \u0026#34;__asan_load2\u0026#34;, BT_FN_VOID_PTR, ATTR_TMPURE_NOTHROW_LEAF_LIST) DEF_SANITIZER_BUILTIN(BUILT_IN_ASAN_LOAD4, \u0026#34;__asan_load4\u0026#34;, BT_FN_VOID_PTR, ATTR_TMPURE_NOTHROW_LEAF_LIST) DEF_SANITIZER_BUILTIN(BUILT_IN_ASAN_LOAD8, \u0026#34;__asan_load8\u0026#34;, BT_FN_VOID_PTR, ATTR_TMPURE_NOTHROW_LEAF_LIST) DEF_SANITIZER_BUILTIN(BUILT_IN_ASAN_LOAD16, \u0026#34;__asan_load16\u0026#34;, BT_FN_VOID_PTR, ATTR_TMPURE_NOTHROW_LEAF_LIST) DEF_SANITIZER_BUILTIN(BUILT_IN_ASAN_LOADN, \u0026#34;__asan_loadN\u0026#34;, BT_FN_VOID_PTR_PTRMODE, ATTR_TMPURE_NOTHROW_LEAF_LIST) DEF_SANITIZER_BUILTIN(BUILT_IN_ASAN_STORE1, \u0026#34;__asan_store1\u0026#34;, BT_FN_VOID_PTR, ATTR_TMPURE_NOTHROW_LEAF_LIST) DEF_SANITIZER_BUILTIN(BUILT_IN_ASAN_STORE2, \u0026#34;__asan_store2\u0026#34;, BT_FN_VOID_PTR, ATTR_TMPURE_NOTHROW_LEAF_LIST) DEF_SANITIZER_BUILTIN(BUILT_IN_ASAN_STORE4, \u0026#34;__asan_store4\u0026#34;, BT_FN_VOID_PTR, ATTR_TMPURE_NOTHROW_LEAF_LIST) DEF_SANITIZER_BUILTIN(BUILT_IN_ASAN_STORE8, \u0026#34;__asan_store8\u0026#34;, BT_FN_VOID_PTR, ATTR_TMPURE_NOTHROW_LEAF_LIST) DEF_SANITIZER_BUILTIN(BUILT_IN_ASAN_STORE16, \u0026#34;__asan_store16\u0026#34;, BT_FN_VOID_PTR, ATTR_TMPURE_NOTHROW_LEAF_LIST) DEF_SANITIZER_BUILTIN(BUILT_IN_ASAN_STOREN, \u0026#34;__asan_storeN\u0026#34;, 정의된 SANITIZER 중에서 BUILT_IN_ASAN_LOAD1 을 따라가보면, gcc_root/gcc/sanopt.c 경로에 pass_sanopt::execute 메서드로 아래와 같이 enum 형태로 정의되어 있다. 호출 스택은 pass_sanopt::execute → asan_expand_check_ifn → check_func 으로 구성된다.\nstatic tree check_func (bool is_store, bool recover_p, HOST_WIDE_INT size_in_bytes, int *nargs) { static enum built_in_function check[2][2][6] = { { { BUILT_IN_ASAN_LOAD1, BUILT_IN_ASAN_LOAD2, BUILT_IN_ASAN_LOAD4, BUILT_IN_ASAN_LOAD8, BUILT_IN_ASAN_LOAD16, BUILT_IN_ASAN_LOADN }, { BUILT_IN_ASAN_STORE1, BUILT_IN_ASAN_STORE2, BUILT_IN_ASAN_STORE4, BUILT_IN_ASAN_STORE8, BUILT_IN_ASAN_STORE16, BUILT_IN_ASAN_STOREN } }, { { BUILT_IN_ASAN_LOAD1_NOABORT, BUILT_IN_ASAN_LOAD2_NOABORT, BUILT_IN_ASAN_LOAD4_NOABORT, BUILT_IN_ASAN_LOAD8_NOABORT, BUILT_IN_ASAN_LOAD16_NOABORT, BUILT_IN_ASAN_LOADN_NOABORT }, { BUILT_IN_ASAN_STORE1_NOABORT, BUILT_IN_ASAN_STORE2_NOABORT, BUILT_IN_ASAN_STORE4_NOABORT, BUILT_IN_ASAN_STORE8_NOABORT, BUILT_IN_ASAN_STORE16_NOABORT, BUILT_IN_ASAN_STOREN_NOABORT } } }; if (size_in_bytes == -1) { *nargs = 2; return builtin_decl_implicit (check[recover_p][is_store][5]); } *nargs = 1; int size_log2 = exact_log2 (size_in_bytes); return builtin_decl_implicit (check[recover_p][is_store][size_log2]); } GCC 코드에서 Optimize and expand sanitizer functions 라고 기술되어 있는 위의 $gcc_root/gcc/sanopt.c 파일를 살펴보고 난 뒤, 커널 코드 내에서 별도의 호출 없이 어떻게 \u0026ldquo;모든\u0026rdquo; 메모리 접근에 대해 유효성 확인을 하는 코드를 삽입할 수 있는지, Generic KASAN에 관련된 함수들이 실제로 메모리 접근 전 어떻게 inline 형태로 추가되는지 대략적으로 이해할 수 있었다.\n결론은 Memory Sanitizer 연관 함수들은 커널에서 정의하였지만 해당 함수들이 실제로 메모리 접근 전에 인라인 또는 아웃라인으로 삽입/호출되는 부분은 컴파일러가 그 역할을 담당한다.\n", 
        "url": "\/\/localhost:1313\/posts\/%EC%BB%A4%EB%84%90%EC%9D%98-kasan-%EC%BD%94%EB%93%9C%EA%B0%80-%EC%82%BD%EC%9E%85%EB%90%98%EB%8A%94-%EB%B0%A9%EB%B2%95\/"
    },
    
    "\/\/localhost:1313\/tags\/dmabuf\/": {
        "title": "Dmabuf",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/dmabuf\/"
    },
    
    "\/\/localhost:1313\/tags\/mmap\/": {
        "title": "Mmap",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/mmap\/"
    },
    
    "\/\/localhost:1313\/tags\/v4l2\/": {
        "title": "V4l2",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/v4l2\/"
    },
    
    "\/\/localhost:1313\/posts\/v4l2-memory-type\/": {
        "title": "V4L2 Memory Type",
        "tags": ["v4l2","vb2","dmabuf","mmap",],
        "content": "개요 최근 Capture 디바이스 드라이버 코드의 V4L2 표준화 작업을 위해 한 가지 업무를 할당 받았다. 거의 일주일 동안 헤매었는데 다른 선임 개발자가 몇 시간 만에 코드를 수정하니 기대한 결과값이 나오는 것처럼 보였다. 어떻게 동작이 가능했을까 머리로 이해가 되지않아 토요일 하루종일 V4L2 프레임워크와 LWN 을 뒤져가며 프레임워크를 분석하고 나니 왜 그동안 이해가 안됐었는지, 그리고 현재 무엇이 잘못됐는지를 파악할 수 있었다.\n본론에 들어가기에 앞서, V4L2 (Video for Linux) 와 Video Buffer 에 대해 간단하게 설명하면, V4L2는 Video Streaming I/O 를 지원하기 위한 프레임워크이다. 스트리밍 API이므로 성능이 중요하고 userspace와 kernel 간의 메모리 교환에서 반드시 zero-copy가 이뤄져야 한다. 이 때문에 구현해야 하는 API들이 꽤 복잡하다. 복잡성을 조금이라도 줄이기 위해, 스트리밍에 사용하는 버퍼에 관련된 코드의 일관성을 유지하고자 나온 것이 현재의 Video Buffer 프레임워크이다(현재 버전은 2이다). 버퍼용 메모리 할당을 위해 기본적인 memory allocator가 메모리 버퍼 방식에 따라 아래와 같이 지원된다.\nvmalloc dma_contig dma_contig_sg vmalloc 의 경우 커널 가상 메모리 상에서는 연속적이지만 실제 물리적으로는 연속적이지 않지만 비교적 효율적이다. IOMMU가 지원되지 않는 경우 직접 DMA에 매핑하는데 한계가 있다. dma_contig와 같은 경우 물리/가상 주소에서 연속되는 메모리 영역을 할당받는다. 물리/가상 주소에서 동일하게 비연속적인 경우는 dma_contig_sg를 사용한다. 이 경우 하드웨어 적으로 scatter/gather DMA operation이 지원되어야 하는 제약사항이 있다.\n문제 IOMMU가 지원되지 않으면서 vmalloc을 사용한다는 것은 문제가 있기에 dma_contig를 이용하여 할당받아 사용해야 한다. 이에 dma_contig 버퍼 타입으로 버퍼 할당을 하면서 DMABUF 방식을 지원하도록 지시를 받았는데 여기서 한 가지 문제가 있었다. DMABUF 방식의 사용 목적을 고려하지 않았다는 점이다.\n할당자가 변경되어야 하는 배경은 이해했지만, DMABUF 방식은 이해할 수 없었다. \u0026lsquo;왜 이해가 안되는 것인지\u0026rsquo;가 머리로 이해가 안되는 답답함에 코드를 보고 있었는데, 각각의 메모리 방식을 사용하는 이유에 대해서 우선적으로 알아보지 않았던 것이 문제였다.\nV4L2 memory에는 아래와 같이 MMAP/USERPTR/OVERLAY/DMABUF 등이 제공된다.\nenum v4l2_memory { V4L2_MEMORY_MMAP = 1, V4L2_MEMORY_USERPTR = 2, V4L2_MEMORY_OVERLAY = 3, V4L2_MEMORY_DMABUF = 4, OVERLAY는 생략하고 나머지를 보면, 먼저 MMAP은 드라이버가 버퍼를 할당하여 userspace의 애플리케이션에서 매핑해서 사용하기 위한 목적의 메모리 타입이다. USERPTR은 반대로 userspace application에서 메모리를 할당하여 드라이버에서 사용하는 방식이다. 그리고 나머지 DMABUF는 다른 디바이스에서 이미 할당한 \u0026lsquo;shared buffer\u0026rsquo;를 사용하기 위한 메모리 타입이다. 즉, v4l2 ioctl을 이횽하여 아무리 애플리케이션에서 드라이버 쪽으로 request buffer를 해봤자 DMABUF 방식에서는 memory allocation이 안된다는 점이다. 이미 다른 디바이스에서 할당한 메모리를 사용하기 위한 목적이기 때문이다. 실제 코드를 보았을 때도, 문서를 참고했을 때에도 MMAP 방식을 제외한 나머지 메모리 타입에서는 request buffer를 했을 때 관련된 정보들만 설정할 뿐이지 버퍼 메모리를 할당하지 않는다.\n끝맺음 다음 주에는 V4L2_MEMORY_MMAP으로 할당한 뒤, 이를 expbuf 를 이용하여 DMABUF 방식으로 share buffer 형태로 정보를 가져온 뒤에 활용할 수 있도록 코드가 구현되어 있는지, 어떻게 하면 개선할 수 있는지 좀 더 찾아보려 한다. 이제서야 DMABUF의 이름이 디바이스에서 DMA를 위해 사용하는 버퍼들을 다른 디바이스에서 공유할 수 있도록 하기 위해 명명된 것이라 이해할 수 있게 되었다. DMABUF의 exporter \u0026amp; importer 기능이 잘못 이해되어 request buffer 시에 메모리 할당을 시도하는 코드가 있는지도 함께 살펴봐야겠다.\n", 
        "url": "\/\/localhost:1313\/posts\/v4l2-memory-type\/"
    },
    
    "\/\/localhost:1313\/tags\/vb2\/": {
        "title": "Vb2",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/vb2\/"
    },
    
    "\/\/localhost:1313\/posts\/kernel-debug-with-kgdb\/": {
        "title": "Kernel Debug With Kgdb",
        "tags": ["kgdb",],
        "content": "개요 필자에게는 디버깅이 현업에서 가장 많은 시간을 소모하는 일이다. 업무 효율을 위해서 디버깅 하는 방법을 찾던 도중 커널에서 제공하는 kdb와 kgdb를 이용하는 방법에 대한 세미나를 보게 되었다. 유투브에서 «Using Serial kdb / kgdb to Debug the Linux Kernel - Douglas Anderson, Google» 검색한 영상인데, KDB와 KGDB 활용하는 방법에 대해 상세하게 설명하고 있다. 본 포스팅에서는 링크의 영상을 테스트 하기 위해 필요한 디버깅 환경 구성 방법에 대해서만 간단하게 정리한다.\nKDB/KGDB 를 이용하는 방법은 Trace32 를 이용하여 디버깅할 수 없는 환경에서 매우 유용하다. 타겟 보드에 따라 JTAG 디버깅 포트가 나와있지 않은 경우도 꽤 있기 때문이다. 한 가지 단점으로는 디버깅 환경 구성이 생각보다 복잡하다.\n환경 구성을 위해 필요한 작업은 아래와 같다.\nDe-muxing Serial communication (kdmx) Kernel configuration Attaching GDB Demuxing Serial Communication using kdmx 필자는 라즈베리파이를 이용하여 디버깅 환경을 구성했다. 호스트가 리눅스 랩탑이었으면 좋았겠지만, 안타깝게도 맥 OS 환경을 사용하였다. 타겟 보드와 시리얼 통신을 한다는 가정 하에, GDB와 터미널 환경을 하나의 시리얼 포트로 연결하기 위해서는 가상 시리얼 포트를 생성하고 통신을 De-mux 해주는 프로그램이 필요하다. 그리고 이를 위한 간단한 도구가 kdmx이다. 본래 agent-proxy 라는 프로젝트 밑에 간단한 프로그램 형태로 들어가 있지만, 손쉽게 받아서 별도의 환경 변수 설정 없이 곧바로 빌드가 가능하다.\nioctl을 사용하지 않는 BSD 계열에서는 약간의 수정사항이 필요하지만 필자가 올려놓은 저장소 내의 코드(https://github.com/seokbeomKim/kdmx )를 이용하면 된다. 리눅스 계열이라면, git://git.kernel.org/pub/scm/utils/kernel/kgdb/agent-proxy.git 에서 다운받아서 사용하도록 하자.\nkdmx 를 빌드한 뒤에 아래와 같이 실행해주면, pseudo tty가 만들어진 것을 확인할 수 있다.\n┌─[sukbeom@Sukbeomui-MacBookPro] - [~/Workspaces/kdmx/kdmx] - [3061] └─[$] ./kdmx -p /dev/tty.usbserial-0001 -b 115200 [23:37:07] /dev/ttys000 is slave pty for terminal emulator /dev/ttys003 is slave pty for gdb Use \u0026lt;ctrl\u0026gt;C to terminate program 테스트를 위해 /dev/ttys000 를 열어 아래와 같이 확인해보자.\n$ minicom -D /dev/ttys000 -b 115200 Welcome to minicom 2.8 OPTIONS: Compiled on Jan 4 2021, 00:04:46. Port /dev/ttys000, 23:52:43 Press Meta-Z for help on special keys Kernel Configuration 아래의 커널 설정 플래그들을 확인한다. 커널 컴파일 하는 방법은 디버깅 환경 구성과 다른 내용이므로 이 포스팅에서 자세하게 설명하지 않겠다.\nCONFIG_KGDB_KDB=y CONFIG_KDB_DEFAULT_ENABLE=0x1 CONFIG_KDB_KEYBOARD=y CONFIG_KDB_CONTINUE_CATASTROPHIC=0 # CONFIG_WATCHDOG is not set # CONFIG_WQ_WATCHDOG is not set CONFIG_MAGIC_SYSRQ=y CONFIG_MAGIC_SYSRQ_DEFAULT_ENABLE=0x1 CONFIG_MAGIC_SYSRQ_SERIAL=y CONFIG_MAGIC_SYSRQ_SERIAL_SEQUENCE=\u0026#34;.\u0026#34; Attaching GDB 이제 KGDB를 직접 이용해보자. 필자는 컴파일용 리눅스 서버에서 커널을 빌드하고 생성된 vmlinux 파일을 Mac OS에 복사하여 심볼을 로드하는데 사용하였다. 맥용 gdb가 필요하다면 반드시 https://seokbeomkim.github.io/posts/kernel-hacking-on-mac/ 포스팅을 참고하도록 한다. (homebrew 를 이용하여 gdb 를 설치해봤자 정상적으로 동작하지 않으니 반드시 포스팅에 기술된대로 직접 GDB를 빌드해 사용해야 한다.) 우분투와 같은 데비안 계열이라면 gdb-multiarch를, 아치리눅스라면 AUR 내에 있는 컴파일러 패키지들을 이용하자.\n먼저, kdmx 를 이용하여 시리얼 통신이 제대로 demuxing 되고 있다는 가정 하에 진행한다. 단순하게 kgdb의 동작을 테스트할 목적이므로, sysrq 를 이용하여 kdb에 진입하여 kgdb를 붙인 뒤 고의로 커널 패닉을 발생시켜 gdb로 어떻게 분석 가능한지를 보일 것이다.\n먼저, kgdb 에서 사용할 시리얼을 아래와 같이 설정해준다.\nroot@raspberrypi:/home/pi# who | awk \u0026#39;{print $2}\u0026#39; \u0026gt; /sys/module/kgdboc/parameters/kgdboc root@raspberrypi:/home/pi# cat /sys/module/kgdboc/parameters/kgdboc ttyS0 이제 sysrq 를 이용하여 KDB로 진입한 뒤 kgdb 를 실행한다.\nroot@raspberrypi:/home/pi# echo g \u0026gt; /proc/sysrq-trigger [ 1141.184978] sysrq: DEBUG Entering kdb (current=0x836b8000, pid 552) on processor 0 due to Keyboard Entry [0]kdb\u0026gt; [0]kdb\u0026gt; [0]kdb\u0026gt; kgdb Entering please attach debugger or use + or 이제 호스트에서 GDB를 실행한 뒤 시리얼 통신으로 붙여준다. 아래와 같이 정상적으로 attach 가 된 것을 알 수 있다.\n$ ./arm-linux-gnueabihf-gdb ~/Workspaces/rpi/vmlinux (gdb) file ~/Workspace/rpi/vmlinux (gdb) cd /Volumes/Kernel/rpi_kernel (gdb) target remote /dev/ttys003 Remote debugging using /dev/ttys003 warning: multi-threaded target stopped without sending a thread-id, using first non-exited thread [Switching to Thread 4294967294] arch_kgdb_breakpoint () at ./arch/arm/include/asm/kgdb.h:46 warning: Source file is more recent than executable. 46\tasm(__inst_arm(0xe7ffdeff)); (gdb) 끝맺음 현업에서 다른 사람들의 디버깅 방법을 보면서 가장 답답한 부분은 디버깅 시에 툴을 사용하지 않는다는 점이다. 몇몇 스타 개발자의 경우 디버깅 툴을 싫어하고 로그 메시지만으로도 충분하다고 하는데 개인적으로는 이러한 의견에 반대한다. 로그 메시지를 이용하여 문제를 해결하는 방향을 세우고 분석하는 것도 중요하지만, 그러한 문제 해결에 도움을 주는 도구를 이용하여 불필요한 시간을 줄이는 것도 중요하다. 물론, 그들처럼 똑똑하지 않은 것도 중요한 이유다.\n", 
        "url": "\/\/localhost:1313\/posts\/kernel-debug-with-kgdb\/"
    },
    
    "\/\/localhost:1313\/tags\/kgdb\/": {
        "title": "Kgdb",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/kgdb\/"
    },
    
    "\/\/localhost:1313\/tags\/c\/": {
        "title": "C",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/c\/"
    },
    
    "\/\/localhost:1313\/posts\/kernel-macro-__is_constexpr\/": {
        "title": "Kernel macro __is_constexpr",
        "tags": ["c",],
        "content": "커널에서 한 가지 재미있는(?) 매크로를 발견했다. 깊이 살펴보고 나니, GCC로 컴파일 시에 삼항 연산자를 이러한 방식으로 사용할 수 있다는 점에 한 번 놀랐고 이러한 방식으로 매크로를 활용할 수 있다는 것에 다시 한번 놀랐다. 가히 변태적인 매크로다. 관련 패치를 보고 리누즈가 한 말에 완전 동의한다.\nThat is either genius, or a seriously diseased mind. 추가한 매크로는 ICE (Integer Constant Expression) 을 알아내기 위한 매크로이고 아래와 같이 정의한다.\n#define __is_constexpr(x)\t\\ (sizeof(int) == sizeof(*(8 ? ((void *)((long)(x) * 0l)) : (int *)8))) 이러한 매크로는 VLA (Variable Length Arrays)를 제거하기 위한 패치의 일부인데, GCC의 -Wvla 옵션으로는 아래와 같은 상황을 구분하지 못하고 경고를 출력한다.\n#define BTRFS_NAME_LEN 255 #define XATTR_NAME_MAX 255 char namebuf[max(BTRFS_NAME_LEN, XATTR_NAME_MAX)]; 단순하게 배열의 크기를 선언하는 데에 있어서 프로그래머라면 당연하게 컴파일 타임에 정의되는 것이므로 VLA 가 아니라고 생각하겠지만, GCC는 이를 VLA 로 처리해버린다. 이를 해결하기 위해 만든 매크로가 위의 매크로이다. 기존의 max 매크로 대신, is_constexpr 매크로를 사용한 max_t 등을 새로 구현하여 VLA에 대한 에러를 성공적으로 제거했다.\n그렇다면, 이 매크로는 어떻게 동작하는 걸까?\n복잡해 보이지만, 알고보면 간단하다. ICE 인 경우 내부 값은 ((void *) NULL) 이 된다. 이 때 리턴 값은 (void *) NULL 이 아니라, 삼항 연산자의 마지막 항 (int *)8에 의해 자동으로 (int *) NULL이 되어 sizeof(int) == sizeoof(*(int *)NULL) 을 만족하게 된다. 만약 ICE가 아닌 경우에는, 위 값은 (void *)(possible values) 가 되고 결국 sizeof(*(void *)value) == 1 이 된다.\n내부적으로 long으로 캐스팅하고 난 뒤에 0L으로 곱하는 이유는 아키텍처에 따라 64비트 변수에 대해 발생할 수 있는 컴파일러 에러를 없애기 위함이다.\n참고 링크 https://stackoverflow.com/questions/49481217/linux-kernels-is-constexpr-macro https://lkml.org/lkml/2018/3/20/845 ", 
        "url": "\/\/localhost:1313\/posts\/kernel-macro-__is_constexpr\/"
    },
    
    "\/\/localhost:1313\/tags\/state\/": {
        "title": "State",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/state\/"
    },
    
    "\/\/localhost:1313\/posts\/state-vs-status\/": {
        "title": "state vs status",
        "tags": ["state","status",],
        "content": "개요 코드를 보다보면 state와 status를 구분하지 않고 사용하는 경우가 많다. 두 용어의 차이점이 무엇인지 명확하게 하기 위해 직접 찾아보니 “state” or “status”? When should a variable name contain the word “state”, and when should a variable name instead contain the word “status”? 스택오버플로우에 이미 관련된 질문이 올라와 있었다.\n둘의 차이점은 간단 명료하게 아래와 같이 정리할 수 있다.\nstatus: 결과 (success/fail); \u0026ldquo;마지막 상태\u0026rdquo; state: 상태 (pending/dispatched) 앞으로는 둘을 잘 구분해서 사용해야겠다. 잘못된 코드만 보다보니 어떤 것이 맞는건지 기존에 알던 것이 잘못된 것처럼 인식이 되는데 정확히 알고 사용하도록 하자.\n", 
        "url": "\/\/localhost:1313\/posts\/state-vs-status\/"
    },
    
    "\/\/localhost:1313\/tags\/status\/": {
        "title": "Status",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/status\/"
    },
    
    "\/\/localhost:1313\/tags\/gdb\/": {
        "title": "Gdb",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/gdb\/"
    },
    
    "\/\/localhost:1313\/tags\/mac_os\/": {
        "title": "Mac_os",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/mac_os\/"
    },
    
    "\/\/localhost:1313\/posts\/mac%EC%97%90%EC%84%9C-gdb-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EC%BB%A4%EB%84%90-%ED%95%B4%ED%82%B9%ED%95%98%EA%B8%B0\/": {
        "title": "Mac에서 GDB 이용한 커널 해킹하기",
        "tags": ["gdb","mac_os",],
        "content": "개요 맥에서도 GDB 를 이용하여 커널 디버깅을 하려고 여러 방법을 시도해보았지만 쉽게 되지 않았다. 리눅스 커널 컴파일부터 qemu 실행, gdb attach 까지 단번에 되는게 하나도 없었다. 특히 homebrew 를 통해 설치하는 gdb가 말썽이었는데, aarch64 아키텍처로 빌드된 바이너리로부터 심볼 테이블을 읽지 못했다. 대체 Dave 는 누구인가?\n(gdb) file ~/Workspaces/kernel_dev/vmlinux Reading symbols from ~/Workspaces/kernel_dev/vmlinux... I\u0026#39;m sorry, Dave, I can\u0026#39;t do that. Symbol format `elf64-littleaarch64\u0026#39; unknown. 이에 해결을 위한 시나리오는 아래와 같이 구성했다.\n컴파일 서버 구성: 개인적으로 사용하고 있는 게이밍 노트북 Hyper-V 로 리눅스 Guest OS를 올려 컴파일 서버로 만든다.\nsftp를 통해 컴파일 서버에서 빌드한 lisa-qemu 부트 이미지와 vmlinux 파일을 받아 맥 os에서 qemu로 VM을 실행한다.\ngdb 클라이언트로 gdb server에 접속하여 디버깅을 한다.\nGuest OS 및 포트포워딩, ssh 서버 설정 Guest OS는 lisa-qemu 설치를 위해서 우분투를 사용하였다. 우분투 설치 후 아래와 갈이 스위치 설정을 해준다.\nNew-VMSwitch -SwitchName \u0026#34;KeyNATSwitch\u0026#34; -SwitchType Internal New-NetIPAddress -IPAddress 10.0.2.1 -PrefixLength 24-InterfaceAlias \u0026#34;vEthernet (KeyNATSwitch)\u0026#34; 그리고 hyper-v guest 설정 - 네트워크 어댑터 - 가상 스위치 설정에서 KeyNATSwitch를 선택해준다.\n이제 우분투로 돌아가 스위치에 물리기 위한 netplan을 아래와 같이 설정한다.\nnetwork: version: 2 renderer: NetworkManager ethernets: eth0: dhcp4: no addresses: - 10.0.2.4/8 gateway4: 10.0.2.1 nameservers: addresses: [8.8.8.8, 8.8.4.4] 설정 후에 sudo netplan apply 로 설정을 적용한다. 이제 마지막으로 포트포워딩을 설정해준다. External Port 는 아래와 같이 동일하게 해도 되나, 귀찮게 구는 중국으로부터의 트래픽을 피하고 싶다면 반드시 다른 포트로 설정해주자.\nNew-NetNAT -Name \u0026#34;NATNetwork\u0026#34; -InternalIPInterfaceAddressPrefix 10.0.2.0/24 Add-NetNatStaticMapping -ExternalIPAddress \u0026#34;0.0.0.0/24\u0026#34; -ExternalPort 22 -Protocol TCP -InternalIPAddress \u0026#34;10.0.2.4\u0026#34; -InternalPort 22 -NatName KeyNATNetwork 이제 컴파일러 서버가 준비되었으니 lisa-qemu를 설치하고 커널 부트 이미지를 준비한다. 이 부분은 이미 lisa-qemu 에 쉽게 가이드가 있으므로 생략한다.\naarch64 target 용 gdb 빌드 먼저, gdb-10.1(https://ftp.gnu.org/gnu/gdb/gdb-10.1.tar.xz )을 받은 뒤 압축을 풀고, 맥에서 빌드시 문제가 되는 부분을 아래와 같이 수정해주자.\n$ vi bfd/elf-bfd.h #define _LIBELF_H_ 1 +#include \u0026lt;string.h\u0026gt; \u0026lt;string.h\u0026gt; 헤더파일을 추가해준 뒤, target만 지정해주고 컴파일해준다. 그리고 마지막으로 빌드된 gdb 바이너리를 bin 디렉토리에 복사해준다.\n$ /configure --disable-debug --disable-dependency-tracking --without-python --target=aarch64-linux-gnu --prefix=$HOME/xtools $ make \u0026amp;\u0026amp; make install $ cp $gdb-10.1/gdb/gdb $HOME/xtools/bin 이제 모든게 준비가 되었다. lisa-qemu 를 디버깅 옵션으로 실행해주고 gdb 를 붙여주면 아래와 같이 맥에서도 디버깅이 가능해진다. 거기에 컴파일 서버까지 생긴건 덤이다.\n", 
        "url": "\/\/localhost:1313\/posts\/mac%EC%97%90%EC%84%9C-gdb-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EC%BB%A4%EB%84%90-%ED%95%B4%ED%82%B9%ED%95%98%EA%B8%B0\/"
    },
    
    "\/\/localhost:1313\/posts\/i3-window-manager\/": {
        "title": "i3 window manager",
        "tags": ["i3wm",],
        "content": "우분투 데스크탑 환경 삭제하기 개인적으로 우분투를 좋아하지 않지만, 맥북에 리눅스 환경을 구성하기 위해 필요한 서드파티 드라이버들이 우분투를 기반으로 배포되고 있어 이들을 손쉽게 설치 하기 위해 우분투를 설치하였다. (https://seokbeomkim.github.io/posts/linux-on-mbp/) . 우분투를 설치한 뒤에 필요한 커널 코드 컴파일와 GDB, QEMU 연동 환경을 구성하자 램 부족으로 가상머신이 죽어버렸다. 겨우 이맥스와 gdb, firefox, qemu 만을 돌렸을 뿐인데 이렇게 힘들어하니 이해할 수가 없었다.\n리소스 부족의 원인을 살펴보니 불필요한 서비스들이 너무 많이 돌고 있었다. 예를 들어, goa (gnome online accounts), snapd (snap package manager) 등 원하지도 않는 서비스들이 설치되어 리소스들을 좀먹고 있었다.\nDesktop Environment 로서 제공되는 Gnome, Kde, Xfce, Lxde 등 다양한 \u0026ldquo;데스크탑 환경\u0026quot;들이 제공되지만, 어짜피 데스크탑 환경이지 반드시 설치할 필요는 없다. 아치 리눅스나 젠투 리눅스를 사용했다면 이를 처음부터 설치하지 않을 수 있었겠지만, 우분투이므로 그러려니 했다. ubuntu-desktop 및 패키지 중에서 gnome 응용 프로그램에 관련된 모든 것은 지워버리고 본래 사용하던 i3wm 만을 설치하여 작업 환경을 구성하였다.\nLogin manager 또한 필요가 없기에 gdm3, lightdm, xdm 등의 프로그램들은 모두 삭제하고 단순하게 터미널로 접속했을 때 자동으로 X 서버에 접속하도록 shell init script를 수정하였다. 마지막으로 부팅 스플래시 로고를 위해 내 랩탑을 힘들게 하는 plymouth 도 함께 제거해주었다.\n# bash_profile 또는 zshrc에 아래 내용을 추가한다. # 이 때 ~/.xsession에는 exec i3가 반드시 작성되어 있어야 한다. if [[ ! ${DISPLAY} \u0026amp;\u0026amp; ${XDG_VTNR} == 1 ]]; then exec startx fi 윈도우 매니저 설치 후 리소스 사용량 그놈을 설치하여 부팅 후의 메모리 사용량은 대략 2GB 를 초과한다. 하지만 windows manager (개인적으로는 xmonad, i3를 선호한다)만을 사용하여 간결하게 구성하면 부트 후에 700 ~ 800 MB 정도의 메모리 사용으로 충분하게 구성할 수 있다. 이렇게 차이가 많이 나니, 8GB 메모리를 가지고 있는 지금의 랩탑에서 그놈 환경이라면, 인터넷 브라우저만 실행해도 가용 메모리가 절반 가량으로 줄어든다. 대략 lisa-qemu 의 메모리 최소 요구사항이 4 GB 인 것을 감안하면 커널 분석 환경 자체가 정말 간당간당하게 돌아가고 디버깅을 하다보면 어쩔 수 없이 가상머신이 램 부족으로 죽어버린다.\nshared input method 와 i3wm 성능 문제 환경을 구성하다 보니 한 가지 이슈가 있었는데, ibus 의 현재 설정된 input method 을 다른 window와 공유하지 않게 설정할 경우 창 관리자의 성능이 급격하게 저하하는 이슈가 있다. 널리 퍼져 있는 내용은 아니지만, 아래 링크에서 관련된 내용을 참고할 수 있다. (https://github.com/i3/i3/issues/3924 )\nRofi (alternative to dmenu) 몇 년 전까지만 해도 dmenu 만을 알고 사용했었는데, rofi 라는 패키지가 있다는 것을 새로 알게됐다. 인터페이스도 dmenu 보다 훨씬 더 미려할 뿐만 아니라 설정 가능한 부분이 많아 dmenu 보다 유용했다. rofi 를 이용해서 디스플레이나 마운트 등을 할 수 있도록 간단하게 스크립트를 짜놓으니 아래와 같이 다양하게 활용할 수 있었다. bash 스크립트로 정말 간단하게 스크립트만 짜면 아래와 같이 현재 플러그인 되어 있는 장치들을 리스트하여 마운트 또는 언마운트 할 수 있다. 관련 스크립트는 https://github.com/seokbeomKim/ShellScriptRepo/tree/master/i3_stuff 에 올려놓았다.\n", 
        "url": "\/\/localhost:1313\/posts\/i3-window-manager\/"
    },
    
    "\/\/localhost:1313\/tags\/i3wm\/": {
        "title": "I3wm",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/i3wm\/"
    },
    
    "\/\/localhost:1313\/posts\/emacs%EC%97%90%EC%84%9C-magit-%EC%B5%9C%EC%A0%81%ED%99%94%ED%95%98%EA%B8%B0\/": {
        "title": "Emacs에서 magit 최적화하기",
        "tags": ["emacs","magit",],
        "content": "개요 이맥스에서 magit 을 이용하여 수정 상태나 커밋 메시지를 확인할 때 오래 걸리는 문제가 있었다. magit 의 매뉴얼 페이지를 보아도 원하는 것은 없었는데, Speeding up magit - Jake McCray 블로그의 내용으로 문제를 해결할 수 있었다.\n링크된 블로그에서는 magit을 이용하면서 어디에서 속도가 느려지는지를 확인하기 위해 magit-refresh-verbose 을 이용하는 방법을 가이드해주고 있다. 커밋 메시지나 깃 상태를 확인할 때 실행되는 hook function 들에서 몇 초간 소모되는지를 나타내주는데, 이를 이용해서 태그와 브랜치 등의 헤더 정보, 코드 diff를 가져오는데 엄청난 시간이 걸린다는 것을 알게 되었다.\nRefreshing buffer ‘magit: example-repo’... magit-insert-error-header 1e-06 magit-insert-diff-filter-header 2.3e-05 magit-insert-head-branch-header 0.026227 magit-insert-upstream-branch-header 0.014285 magit-insert-push-branch-header 0.005662 magit-insert-tags-header 1.7119309999999999 magit-insert-status-headers 1.767466 magit-insert-merge-log 0.005947 magit-insert-rebase-sequence 0.000115 magit-insert-am-sequence 5.1e-05 magit-insert-sequencer-sequence 0.000105 magit-insert-bisect-output 5.3e-05 magit-insert-bisect-rest 1.1e-05 magit-insert-bisect-log 1e-05 magit-insert-untracked-files 0.259485 magit-insert-unstaged-changes 0.031528 magit-insert-staged-changes 0.017763 magit-insert-stashes 0.028514 magit-insert-unpushed-to-pushremote 0.911193 magit-insert-unpushed-to-upstream-or-recent 0.497709 magit-insert-unpulled-from-pushremote 7.2e-05 magit-insert-unpulled-from-upstream 0.446168 Refreshing buffer ‘magit: example-repo’...done (4.003s) 문제가 되는 (특별하게 느려지는) 부분은 속도 뒤에 !! 표시가 되는데 해당 함수들이 사용하는데 잘 쓰이지 않는다면 과감하게 아래와 같이 제거해주자. 필자는 변경된 코드 확인과 커밋 메시지만을 주로 확인하므로 아래와 같이 설정하였다.\n;; remove unnecessary magit function on revision section hook (remove-hook \u0026#39;magit-revision-sections-hook \u0026#39;magit-insert-revision-headers) (remove-hook \u0026#39;magit-revision-sections-hook \u0026#39;magit-insert-revision-notes) (remove-hook \u0026#39;magit-revision-sections-hook \u0026#39;magit-insert-revision-diff) (remove-hook \u0026#39;magit-revision-sections-hook \u0026#39;magit-insert-revision-tag) (remove-hook \u0026#39;magit-revision-sections-hook \u0026#39;magit-insert-xref-buttons) (add-hook \u0026#39;magit-revision-sections-hook \u0026#39;magit-insert-revision-message) ;; remove unnecessary magit function on showing status (remove-hook \u0026#39;magit-status-sections-hook \u0026#39;magit-insert-tags-header) (remove-hook \u0026#39;magit-status-sections-hook \u0026#39;magit-insert-status-headers) 커널 공부를 위해 커밋 메시지 살펴볼 볼때마다 엄청나게 걸리던 시간은 1초도 안되는 시간에 매끄럽게 볼 수 있게 되었다. 성능 이슈가 생길 정도로 너무 많은 기능을 넣을 필요는 없는데 magit은 이러한 부분에 있어서 너무 매정한 것 같다.\n", 
        "url": "\/\/localhost:1313\/posts\/emacs%EC%97%90%EC%84%9C-magit-%EC%B5%9C%EC%A0%81%ED%99%94%ED%95%98%EA%B8%B0\/"
    },
    
    "\/\/localhost:1313\/tags\/magit\/": {
        "title": "Magit",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/magit\/"
    },
    
    "\/\/localhost:1313\/tags\/homebrew\/": {
        "title": "Homebrew",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/homebrew\/"
    },
    
    "\/\/localhost:1313\/posts\/homebrew-python-%EC%84%A4%EC%B9%98%ED%95%98%EA%B8%B0\/": {
        "title": "Homebrew python 설치하기",
        "tags": ["mac","homebrew",],
        "content": "Homebrew로 설치한 파이썬의 버전이나 경로가 기본 파이썬을 실행했을 때와 다를 경우 링크된 파일이 다른지 먼저 확인해야 한다. 이를 테면, pip 패키지 매니저로 확장 라이브러리를 설치해도 찾지 못하는 경우이다. 문제가 발생할 때 파이썬의 경로를 살펴보면, `/usr/bin/python3\u0026rsquo; 등으로 /usr/bin 아래에 놓여있는 것을 알 수 있다.\n이러한 경우, 아래와 같이 brew 명령어를 통해 다시 심볼릭 링크를 재정의해주자.\nbrew link python@3.9 ", 
        "url": "\/\/localhost:1313\/posts\/homebrew-python-%EC%84%A4%EC%B9%98%ED%95%98%EA%B8%B0\/"
    },
    
    "\/\/localhost:1313\/tags\/mac\/": {
        "title": "Mac",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/mac\/"
    },
    
    "\/\/localhost:1313\/categories\/python\/": {
        "title": "Python",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/categories\/python\/"
    },
    
    "\/\/localhost:1313\/tags\/file2alias\/": {
        "title": "File2alias",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/file2alias\/"
    },
    
    "\/\/localhost:1313\/categories\/mac\/": {
        "title": "Mac",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/categories\/mac\/"
    },
    
    "\/\/localhost:1313\/tags\/uuid\/": {
        "title": "Uuid",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/uuid\/"
    },
    
    "\/\/localhost:1313\/posts\/%EB%A6%AC%EB%88%85%EC%8A%A4-%EC%BB%A4%EB%84%90-%EB%B9%8C%EB%93%9C-%EC%8B%9C-%EB%A7%A5-o\/s-uuid_t-%ED%98%B8%ED%99%98%EC%84%B1\/": {
        "title": "리눅스 커널 빌드 시 맥 O\/S uuid_t 호환성",
        "tags": ["uuid","kernel","file2alias","mac_os",],
        "content": "개요 BSD 기반의 맥에서 리눅스 커널 빌드가 안될리 없다고 생각하고 나서 어떻게든 맥에서 리눅스 커널을 빌드하기 위해 이런저런 삽질을 했다. 회사에서의 BSP 업무는 모두 리눅스 환경 아래에서 작업하기 때문에 사실상 큰 의미는 없겠으나, 빌드만을 위해서 맥에서 도커까지 사용해야 하는 솔루션을 납득할 수 없었다.\n이에, 지난 포스팅 에서 열심히 정리해놓은 가이드를 따라 크로스 컴파일러 준비 후, 커널 빌드를 시도하였으나 어찌된 영문인지 file2alias.c 파일에서 계속해서 빌드 에러가 났다. commit 로그를 뒤져보았는데 uuid_t 관련해서 크게 달라진 점은 없었다. 어째서 라즈베리파이 bsp에서는 정상적으로 빌드되는데 공식 커널에서는 빌드되지 않는 것일까.\n빌드 시 나오는 에러는 아래와 같다.\nHOSTCC scripts/mod/file2alias.o scripts/mod/file2alias.c:47:3: error: typedef redefinition with different types (\u0026#39;struct uuid_t\u0026#39; vs \u0026#39;__darwin_uuid_t\u0026#39; (aka \u0026#39;unsigned char [16]\u0026#39;)) } uuid_t; ^ /Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX.sdk/usr/include/sys/_types/_uuid_t.h:31:25: note: previous definition is here typedef __darwin_uuid_t uuid_t; ^ scripts/mod/file2alias.c:1305:42: error: array initializer must be an initializer list or string literal DEF_FIELD(symval, tee_client_device_id, uuid); ^ 2 errors generated. 이에 여기저기 뒤져보니 10달 전에 이미 관련된 패치 가 OpenWRT에 나와있었다. (역시 안될 리가 없다.)\ndiff --git a/scripts/mod/file2alias.c b/scripts/mod/file2alias.c index c91eba751804..e756fd80b721 100644 --- a/scripts/mod/file2alias.c +++ b/scripts/mod/file2alias.c @@ -38,6 +38,9 @@ typedef struct { __u8 b[16]; } guid_t; +#ifdef __APPLE__ +#define uuid_t compat_uuid_t +#endif /* backwards compatibility, don\u0026#39;t use in new code */ typedef struct { __u8 b[16]; -- 2.21.1 (Apple Git-122.3) 맥이 가지고 있는 시스템 라이브러리의 uuid_t 대신 compat_uuid_t 를 사용하겠다는 건데 이렇게 define 매크로를 사용함으로써 typedef`으로 정의되는 타입명 자체를 바꿔버리는 것이 신선하게 다가왔다.\n이제 file2alias.c 파일을 고친 뒤에 다시 빌드를 해보자. 커널 master 브랜치에서 문제 없이 빌드가 되는 것을 확인할 수 있다.\n", 
        "url": "\/\/localhost:1313\/posts\/%EB%A6%AC%EB%88%85%EC%8A%A4-%EC%BB%A4%EB%84%90-%EB%B9%8C%EB%93%9C-%EC%8B%9C-%EB%A7%A5-o\/s-uuid_t-%ED%98%B8%ED%99%98%EC%84%B1\/"
    },
    
    "\/\/localhost:1313\/tags\/vmmem\/": {
        "title": "Vmmem",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/vmmem\/"
    },
    
    "\/\/localhost:1313\/tags\/wsl\/": {
        "title": "WSL",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/wsl\/"
    },
    
    "\/\/localhost:1313\/posts\/wsl-%EA%B0%80%EC%83%81-%EB%94%94%EC%8A%A4%ED%81%AC-%ED%8C%8C%EC%9D%BC-d-%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B8%8C%EC%97%90-%EC%98%AE%EA%B8%B0%EA%B8%B0\/": {
        "title": "WSL 가상 디스크 파일 D 드라이브에 옮기기",
        "tags": ["WSL","vmmem",],
        "content": "lxrunoffline WSL 을 사용하다 보면 디스크 용량이 커져 C 드라이브의 용량이 부족해진다. 용량이 넉넉하면 문제가 없겠지만 필자와 같이 C 드라이브는 O/S만 설치하고 D 드라이브에 다른 것들을 모두 설치하도록 환경을 설정한 경우에는 이러한 저장 방식은 꽤 부담스러워진다. 예전에는 이러한 문제에 대해 해결 방법이 따로 없었던 것으로 알고 있었는데 찾아보니 lxrunoffline이라는 툴로 간단하게 해결할 수 있었다.\n윈도우즈 패키지 매니저인 choco 를 이용하여 lxrunoffline을 설치하고 이를 이용하여 WSL에 사용되는 가상 디스크 파일을 다른 드라이브로 옮길 수 있다.\n\u0026gt; choco install lxrunoffline \u0026gt; lxrunoffline move -n Ubuntu -d D:\\wsl\\ ", 
        "url": "\/\/localhost:1313\/posts\/wsl-%EA%B0%80%EC%83%81-%EB%94%94%EC%8A%A4%ED%81%AC-%ED%8C%8C%EC%9D%BC-d-%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B8%8C%EC%97%90-%EC%98%AE%EA%B8%B0%EA%B8%B0\/"
    },
    
    "\/\/localhost:1313\/tags\/sysrq\/": {
        "title": "Sysrq",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/sysrq\/"
    },
    
    "\/\/localhost:1313\/posts\/sysrq-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EC%BB%A4%EB%84%90-%ED%8C%A8%EB%8B%89-%EB%B0%9C%EC%83%9D%EC%8B%9C%ED%82%A4%EA%B8%B0\/": {
        "title": "sysrq 이용한 커널 패닉 발생시키기",
        "tags": ["sysrq",],
        "content": "개요 현업에서 커널 패닉을 고의로 생성하기 위해서 찾아보던 중 sysrq 를 이용하는 방법이 있다는 것을 알게 되었다. 본 페이지에서는 sysrq에 대한 개념과 사용 방법 등에 대해서 정리하도록 한다.\n기능 활성화 SysRq 이 컴파일된 커널을 실행하면 /proc/sys/kernel/sysrq 를 통해서 원하는 기능을 활성화/비활성화 할 수 있다. 사용 가능한 sysrq 값들은 /proc/sys/kernel/sysrq 내에 정의되어 있으며 기본값들은 CONFIG_MAGIC_SYSRQ_DEFAULT_ENABLE 값으로 설정한다.\n0 - disable sysrq completely\n1 - enable all functions of sysrq\n\u0026gt;1 - bitmask of allowed sysrq functions (see below for detailed function description):\n2 = 0x2 - enable control of console logging level 4 = 0x4 - enable control of keyboard (SAK, unraw) 8 = 0x8 - enable debugging dumps of processes etc. 16 = 0x10 - enable sync command 32 = 0x20 - enable remount read-only 64 = 0x40 - enable signalling of processes (term, kill, oom-kill) 128 = 0x80 - allow reboot/poweroff 256 = 0x100 - allow nicing of all RT tasks 사용법 https://www.kernel.org/doc/html/v4.11/admin-guide/sysrq.html 페이지에 따르면 /proc/sysrq-trigger 에 아래와 같이 특정 캐릭터를 사용하여 커맨드 키를 호출할 수 있다.\necho t \u0026gt; /proc/sysrq-trigger 통상 커널 디버깅을 위해 사용한다고 생각하며, 다양한 디버깅 상황을 발생시키기 위해 유용하게 사용할 수 있다.\n출처 https://www.kernel.org/doc/html/v4.11/admin-guide/sysrq.html https://unix.stackexchange.com/questions/66197/how-to-cause-kernel-panic-with-a-single-command ", 
        "url": "\/\/localhost:1313\/posts\/sysrq-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EC%BB%A4%EB%84%90-%ED%8C%A8%EB%8B%89-%EB%B0%9C%EC%83%9D%EC%8B%9C%ED%82%A4%EA%B8%B0\/"
    },
    
    "\/\/localhost:1313\/posts\/wsl-%EB%A6%AC%EC%86%8C%EC%8A%A4-%EC%A1%B0%EC%A0%95%ED%95%98%EA%B8%B0\/": {
        "title": "WSL 리소스 조정하기",
        "tags": ["WSL","vmmem",],
        "content": " 개요 커널 분석을 위해 WSL을 이용하도록 환경을 구성하자 서브시스템에서 사용하는 메모리나 CPU 자원이 상당하였다. 16기가 램 중에 무려 12기가를 사용하는 일이 비일비재했다. Vmmem 이라는 프로세스로 엄청난 리소스가 소모되고 있었고 이러한 내용은 다른 블로그 에서도 동일하게 재현되고 있었다.\n위 블로그에 나와있는 이 문제에 대한 해결법은 간단하다. 아래와 같이 WSL 설정파일을 하나 생성해서 사용할 리소스를 직접 명시해주면 된다.\nWSL 리소스 설정 먼저, 실행 중인 WSL을 확인한 후 아래와 같이 종료한다.\nPS C:\\Windows\\system32\u0026gt; wsl -l -v NAME STATE VERSION * Ubuntu Running 2 PS C:\\Windows\\system32\u0026gt; wsl -t Ubuntu --shutdown PS C:\\Windows\\system32\u0026gt; PS C:\\Windows\\system32\u0026gt; wsl -l -v NAME STATE VERSION * Ubuntu Stopped 2 종료한 후에는 아래 경로에 새 파일을 만들어주고 아래와 같이 작성해주자.\nPS C:\\Windows\\system32\u0026gt; notepad $env:USERPROFILE\\.wslconfig 이제 아래와 같이 설정해주자. 이제 WSL2에서 사용할 메모리는 최대 4G로 제한된다.\n[wsl2] #kernel=\u0026lt;path\u0026gt; # An absolute Windows path to a custom Linux kernel. memory=4G # How much memory to assign to the WSL2 VM. #processors=\u0026lt;number\u0026gt; # How many processors to assign to the WSL2 VM. #swap=\u0026lt;size\u0026gt; # How much swap space to add to the WSL2 VM. 0 for no swap file. #swapFile=\u0026lt;path\u0026gt; # An absolute Windows path to the swap vhd. #localhostForwarding=\u0026lt;bool\u0026gt; # Boolean specifying if ports bound to wildcard or localhost in the WSL2 VM should be connectable from the host via localhost:port (default true). # \u0026lt;path\u0026gt; entries must be absolute Windows paths with escaped backslashes, for example C:\\\\Users\\\\Ben\\\\kernel # \u0026lt;size\u0026gt; entries must be size followed by unit, for example 8GB or 512MB 이제 설치한 WSL의 바로가기를 눌러 WSL을 다시 실행시킨다. 윈도우즈의 Task Manager를 보면 아래와 같이 설정한 값으로 메모리 사용이 제한되는 것을 알 수 있다.\n출처 https://treeroad.tistory.com/entry/Flash-Memory%EC%99%80-EEPROM-%EC%B0%A8%EC%9D%B4%EC%A0%90 ", 
        "url": "\/\/localhost:1313\/posts\/wsl-%EB%A6%AC%EC%86%8C%EC%8A%A4-%EC%A1%B0%EC%A0%95%ED%95%98%EA%B8%B0\/"
    },
    
    "\/\/localhost:1313\/tags\/cache\/": {
        "title": "Cache",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/cache\/"
    },
    
    "\/\/localhost:1313\/tags\/cache-line\/": {
        "title": "Cache-Line",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/cache-line\/"
    },
    
    "\/\/localhost:1313\/posts\/%EC%BA%90%EC%8B%9C%EB%9D%BC%EC%9D%B8cache-line-%EA%B5%AC%EC%A1%B0-%232\/": {
        "title": "캐시라인(cache-line) 구조 #2",
        "tags": ["cache","cache-line",],
        "content": "개요 \u0026laquo;코드로 알아보는 ARM 리눅스 커널\u0026raquo;이라는 커널 분석 책으로 스터디를 진행하였다. 예전에 ARM64 아키텍처 부분을 공부한 터라 이해하는 데 문제가 없을 거라 생각했는데 역시나 기억력에 한계가 있었다. 개인적으로는 책의 그림과 내용이 직관적으로 이해가 되지 않고 방해가 방해가 되었다. 나중에 캐시라인의 구조에 대해 분명히 다시 살펴볼 것이므로 아래와 같이 짧게 재 정리해놓고자 한다.\n캐시 구조 캐시에 관련된 주요 용어는 아래와 같다.\n태그 캐시라인 인덱스 웨이와 세트 태그 태그는 캐시 탐색 시에 저장되는 메모리 주소의 일부를 만한다.\n캐시라인 태그를 각 워드마다 붙이는 것은 비효율적이므로 여러 워드를 함께 묶는다. 캐시라인은 load, flush와 같은 캐시 연산의 가장 작은 단위이며, 메모리에서 연속된 워드 블록이다. 캐시 라인이 캐싱된 데이터나 인스트럭션을 포함하고 있을 때 유효하며, 그렇지 않을 때 무효하다.\n인덱스 캐시 라인을 찾기 위해 사용되는 메모리 주소의 일부 비트를 인덱스라고 한다.\n웨이와 세트 캐시 탐색 과정에서 태그가 일치할 확률을 높이기 위해 동일 인덱스에 해당하는 태그/캐시라인을 여러 쌍 두는데 이 때 태그/캐시라인 쌍 각각을 웨이라고 한다.\n링크 에서의 그림과 같은 내용이며, 한 개의 인덱스에 대해 여러 개의 캐시라인을 사용하는 개념을 알 수 있다. 위 그림에서는 Index #0 에 대해서 2개의 캐시라인을 사용하는 것을 알 수 있는데 이 경우, 2-way로 구성되어 있다고 말할 수 있다. 즉, 메인 메모리에서의 64비트 주소로 매핑될 때 사용되는 인덱스 #n 에 대해 몇 개의 캐시라인을 사용했는지를 way라는 용어로 나타내는 것이고 같은 index 로 묶인 이러한 웨이들의 집합을 set 이라고 일컫는다.\n64비트 주소 64비트 주소는 아래 구조로 구성되어 있다.\n태그와 인덱스 그리고 오프셋으로 주소로 이루어져 있으며 위의 캐시 구조를 살펴보면 각각이 어떻게 응용되는지 미리 예상해볼 수 있다.\n", 
        "url": "\/\/localhost:1313\/posts\/%EC%BA%90%EC%8B%9C%EB%9D%BC%EC%9D%B8cache-line-%EA%B5%AC%EC%A1%B0-%232\/"
    },
    
    "\/\/localhost:1313\/tags\/ghcup\/": {
        "title": "Ghcup",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/ghcup\/"
    },
    
    "\/\/localhost:1313\/categories\/haskell\/": {
        "title": "Haskell",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/categories\/haskell\/"
    },
    
    "\/\/localhost:1313\/posts\/%ED%95%98%EC%8A%A4%EC%BC%88-%EB%B2%84%EC%A0%84-%EA%B4%80%EB%A6%AC\/": {
        "title": "하스켈 버전 관리",
        "tags": ["ghcup",],
        "content": "개요 IDE 환경을 구성하기 위해 haskell-ide-engine을 설치하였지만 한 가지 문제가 발생하였다. 현재 기준 8.8.3 까지 지원하고 있는 ide-engine 버전이 8.10 까지 버전업 되어 있는 ghc 환경과 호환이 안되는 문제가 발생하였다. 이를 위해 하스켈의 전체적인 버전 관리를 해주는 패키지 매니저가 있을 것이라 생각하고 관련 내용으로 검색해보니 ghcup 이라는 버전 관리 매니저가 있었다.\nghcup은 아치리눅스의 경우 AUR에서 ghcup-hs-bin 패키지로 설치할 수 있으며 아래와 같이 원하는 ghc 버전들을 간편하게 설치하고 메인으로 관리할 수 있다. 먼저, ghcup list 명령어를 통해 ghc 로 설치된 패키지들의 목록을 확인해보자.\npublic git:6c65fe3 ❯ ghcup list Tool Version Tags Notes ✗ ghc 7.10.3 base-4.8.2.0 ✗ ghc 8.0.2 base-4.9.1.0 ✗ ghc 8.2.2 base-4.10.1.0 ✗ ghc 8.4.1 base-4.11.0.0 ✗ ghc 8.4.2 base-4.11.1.0 ✗ ghc 8.4.3 base-4.11.1.0 ✗ ghc 8.4.4 base-4.11.1.0 ✗ ghc 8.6.1 base-4.12.0.0 ✗ ghc 8.6.2 base-4.12.0.0 ✗ ghc 8.6.3 base-4.12.0.0 ✗ ghc 8.6.4 base-4.12.0.0 ✗ ghc 8.6.5 base-4.12.0.0 ✗ ghc 8.8.1 base-4.13.0.0 ✗ ghc 8.8.2 base-4.13.0.0 ✔✔ ghc 8.8.3 base-4.13.0.0 ✗ ghc 8.8.4 recommended,base-4.13.0.0 ✗ ghc 8.10.1 base-4.14.0.0 ✓ ghc 8.10.2 latest,base-4.14.1.0 ✗ ghc 9.0.1-alpha1 prerelease,base-4.15.0.0 ✗ cabal 2.4.1.0 ✗ cabal 3.0.0.0 ✗ cabal 3.2.0.0 latest,recommended ✗ cabal 3.4.0.0-rc2 prerelease ✔✔ ghcup 0.1.10 latest,recommended 현재 기본으로 설정되어 있는 ghc 버전과 ghcup 버전 등을 리스트로 확인할 수 있다. 새로 ghc 버전을 설치하기 위해서는 아래 명령어로 버전을 간편하게 설치할 수 있다.\n$ ghcup install ghc 8.8.2 # install ghc of version 8.8.2 $ ghcup set ghc 8.8.2 # set default version to 8.8.2 중요한 것은 ghcup을 통해 설치하는 모든 바이너리는 $HOME/.ghcup/bin 경로에 설치되어 있으므로 셸 스크립트를 통해 해당 경로를 참조할 수 있도록 $PATH 환경변수를 조정해줘야 한다.\n참고 링크 https://gitlab.haskell.org/haskell/ghcup-hs ", 
        "url": "\/\/localhost:1313\/posts\/%ED%95%98%EC%8A%A4%EC%BC%88-%EB%B2%84%EC%A0%84-%EA%B4%80%EB%A6%AC\/"
    },
    
    "\/\/localhost:1313\/tags\/coc\/": {
        "title": "Coc",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/coc\/"
    },
    
    "\/\/localhost:1313\/posts\/%EB%A6%AC%EB%88%84%EC%A6%88%EA%B2%8C%EC%9D%B4%ED%8A%B8%EC%99%80-coc\/": {
        "title": "리누즈게이트와 CoC",
        "tags": ["일상","coc",],
        "content": "자유 발언 구독 중인 «LXer Linux News»에서 Linusgate Leaked 라는 제목으로 글 하나가 올라왔다. 글을 읽고 나서 최근 고민하던 문제인 \u0026lsquo;개발자에게 중요한 것은 무엇인가\u0026rsquo; 에 대해서 다시 한번 생각하게 되었다. 해당 사건은 평소 과감한 발언으로 자주 회자되었던 리누즈 토발즈가 자유소프트웨어 진영을 벌레같다고 표현하며 공격적으로 발언하는 그의 태도를 두고 이를 커뮤니티 행동강령 위반으로서 문제삼았다는 것이 주 내용이다. 이 사건을 두고 어떤 사람은 리누즈의 그러한 발언 조차 \u0026lsquo;자유 발언\u0026rsquo;으로서 존중 받아야 할 것이다라고 얘기할 것이다. 혹자는 아래와 같이 생각할 수도 있다.\nThis is what free speech is about, but not everyone shares that view; to some people, only \u0026lsquo;comfortable\u0026rsquo; or 100% politically-correct messages are tolerable, acceptable and permissible. So it\u0026rsquo;s basically a trap, or a sort of \u0026lsquo;set-up\u0026rsquo; for canceling or banishing people.\n자유에는 나의 행동에 대한 자유와 상대방의 것에 대한 존중이 함께 들어가 있다고 생각한다. \u0026lsquo;자유발언\u0026rsquo;이라고 해서 상대방이 발언하지 못하도록 하거나 무시하는 듯의 태도는 자유발언이라고 말할 수 없다. 찬성과 반대의 표현에도 최소한의 존중은 들어가 있어야 하지 않을까.\n공격적인 리더쉽 유독 개발자들 중에서는 공격적인 사람들이 많은 것 같다. 야근을 많이 해서일까, 아는 게 많아서일까, 할 일이 많아서일까? 협업이 기본일 수밖에 없는 프로그래밍 영역에서 개발 능력과 커뮤니케이션 능력은 기본이다. 아는 것이 많을수록 올바른 방향으로 갈 수 있게 사람들을 이끄는 능력이 필요하지 않을까. \u0026lsquo;너 인성에 문제있어?\u0026rsquo; 라는 소리를 듣지 않으면서 말이다.\n", 
        "url": "\/\/localhost:1313\/posts\/%EB%A6%AC%EB%88%84%EC%A6%88%EA%B2%8C%EC%9D%B4%ED%8A%B8%EC%99%80-coc\/"
    },
    
    "\/\/localhost:1313\/tags\/%EC%9D%BC%EC%83%81\/": {
        "title": "일상",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/%EC%9D%BC%EC%83%81\/"
    },
    
    "\/\/localhost:1313\/posts\/mac-os%EC%97%90%EC%84%9C%EC%9D%98-%EC%9E%90%EB%AA%A8-%EB%B6%84%EB%A6%AC-%ED%98%84%EC%83%81-%ED%95%B4%EA%B2%B0%ED%95%98%EA%B8%B0\/": {
        "title": "Mac OS에서의 자모 분리 현상 해결하기",
        "tags": ["nfd","nfc",],
        "content": "개요 드롭박스를 사용하던 중 구글 드라이브에서 가져온 파일이 동기화되지 않는 문제가 발생했다. 파일은 존재하는데 동기화를 하지 못하는 문제였다. 또 한가지는 반대로 맥에서 동기화한 파일을 윈도우즈나 리눅스에서 보면 한글 자모가 분리되는 현상이 있었다. 자세히 살펴보니 유니코드 정규화 방식의 차이 때문이었다.\n유니코드 정규화 유니코드 정규화는 모양이 같은 여러 문자들이 있을 경우 이를 기준에 따라 하나로 통합해주는 역할을 하며 해당 기준으로 NFD, NFC, NFKD, NFKC 등이 이다. 윈도우즈나 리눅스에서는 NFC 를 기준으로 유니코드 정규화를 하지만 맥에서는 이와 다른 NFD 를 기준으로 유니코드를 정규화하기에 이와 같은 문제가 발생하는 것이다.\n이런 문제를 막기 위해서는 번거롭기는 하지만 동기화를 할 때마다 문제가 되는 파일에 대해 아래와 같이 convmv 명령어를 통해 포맷을 바꿔줘야 한다.\n$ convmv -r \u0026ndash;nfd \u0026ndash;nosmart -f UTF-8 -t UTF-8 \u0026ndash;notest ./\n", 
        "url": "\/\/localhost:1313\/posts\/mac-os%EC%97%90%EC%84%9C%EC%9D%98-%EC%9E%90%EB%AA%A8-%EB%B6%84%EB%A6%AC-%ED%98%84%EC%83%81-%ED%95%B4%EA%B2%B0%ED%95%98%EA%B8%B0\/"
    },
    
    "\/\/localhost:1313\/tags\/nfc\/": {
        "title": "Nfc",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/nfc\/"
    },
    
    "\/\/localhost:1313\/tags\/nfd\/": {
        "title": "Nfd",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/nfd\/"
    },
    
    "\/\/localhost:1313\/posts\/.-%EC%99%80-%EC%9D%98-%EC%B0%A8%EC%9D%B4\/": {
        "title": "(.) 와 ($) 의 차이",
        "tags": ["function application",],
        "content": "개요 최근 하스켈로 조그만 프로젝트를 시작하기 위해 Real World Haskell 이라는 책 한 권을 구입하여 공부하기 시작했다. Functional Programming 패러다임 자체가 익숙하지 않기에 주어진 문제를 해결하기 위한 접근 방식부터 차근차근 배워나가려 하고 있지만 쉽지 않다. 여타 주요 언어에서 제공하는 loop의 개념이 없고 함수만으로 이루어진 recursive와 pattern matching 또는 function application 등으로 표현되기에 쉽지가 않다. Higher-order function으로 이루어지는 하스켈의 함수에서는 currying을 통해 여러 개의 인자를 갖는 함수를 표현할 수 있다. 이러한 개념과 함께 설명되는 것이 포스팅의 타이틀에 해당하는 (.)과 ($) 연산자인데 각각 Function Composition과 Function Application을 의미한다.\nHigher-Order Function 어째서 higher-order functiion 이라는 이름을 갖게 되었을까. 고차함수라 부르는 이 함수는 Function Proramming 에서는 적어도 아래 조건 중 하나 이상을 만족하면 고차함수 HOF 라 한다.\n하나 이상의 함수를 인자로 받는다. 함수를 결과로 반환한다. Function Application \u0026amp; Function Composition 이제 ($) 연산자를 살펴보자. 연산자는 아래와 같이 정의된다.\n($) :: (a -\u0026gt; r) -\u0026gt; a -\u0026gt; r ($) 연산자는 함수를 인자로 전달받으며 해당 함수를 인자로써 적용한다. 이러한 의미로 f $ a는 f a와 같다. 그렇다면, 연산자 (.)는 어떨까?\n(.) :: (b -\u0026gt; c) -\u0026gt; (a -\u0026gt; b) -\u0026gt; (a -\u0026gt; c) (.) 연산자는 함수 두 개를 인자로써 전달받고 두 개를 합성하는 역할을 한다. 때문에 (f . g) a 는 f (g a)와 같으며 앞서 함수를 인자로써 넘기는 것과 확연히 다른 의미를 갖는다. 이러한 Function Composition을 이용하면 여러 개의 함수를 합성하여 아래와 같이 사용할 수 있다.\n\u0026gt; getDefineName = head . tail . words \u0026gt; getDefineName \u0026quot;define DLT_CHAOS 5\u0026quot; \u0026quot;DLT_CHAOS\u0026quot; 참고 자료 https://www.quora.com/What-is-the-difference-between-and-in-Haskell https://medium.com/@la.place/higher-order-function-%EC%9D%B4%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80-1c61e0bea79 ", 
        "url": "\/\/localhost:1313\/posts\/.-%EC%99%80-%EC%9D%98-%EC%B0%A8%EC%9D%B4\/"
    },
    
    "\/\/localhost:1313\/tags\/function-application\/": {
        "title": "Function Application",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/function-application\/"
    },
    
    "\/\/localhost:1313\/categories\/design-pattern\/": {
        "title": "Design Pattern",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/categories\/design-pattern\/"
    },
    
    "\/\/localhost:1313\/categories\/linux\/": {
        "title": "Linux",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/categories\/linux\/"
    },
    
    "\/\/localhost:1313\/categories\/oop\/": {
        "title": "Oop",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/categories\/oop\/"
    },
    
    "\/\/localhost:1313\/posts\/%EC%BB%A4%EB%84%90%EC%97%90%EC%84%9C%EC%9D%98-object-oriented-design-pattern\/": {
        "title": "커널에서의 Object-Oriented Design Pattern",
        "tags": ["kernel",],
        "content": "개요 현업에서 BSP 코드를 수정하다가 문득 든 생각은 \u0026lsquo;왜 객체 지향의 디자인 패턴을 적용하지 않는 걸까?\u0026rsquo; 라는 것이다. 그러한 디자인 패턴은 이미 오래 전부터 적용되어 왔지만 BSP에 포함된 솔루션 코드로서 추가되는 코드에는 그러한 디자인 패턴이 보이지 않는다.\nJava나 C++, 그리고 완전하지는 않지만 prototype을 이용한 Javascript에서도 객체 지향적인 디자인 패턴이 적용되어 있다. 그렇다면, C와 어셈블리어로 짜여진 커널에서는 이러한 디자인 패턴이 어떻게 적용되어 있을까. 여기에 대한 좋은 참고 자료로서 LWN의 한 기사를 찾을 수 있었다.\nhttps://lwn.net/Articles/446317/ 학부 시절부터 오랫동안 들어온 객체의 정의는 state와 behavior이다. 이들은 각각 클래스의 멤버 변수와 메서드 형태로 구현되는데, 이러한 디자인 패턴 자체는 C를 이용해서도 표현이 가능하다. 멤버와 메서드는 각각 구조체 멤버와 vtable(virtual function table) 형태로 표현될 수 있다. 그리고 데이터 상속의 개념으로서 union과 void*, embedded structure 등의 기법을 이용한다.\n이 포스팅에서는 커널 코드에서 활용하는 객체지향 디자인 패턴의 기본적인 개념만을 언급한다. 좀 더 자세한 내용이나 실제 코드는 참고자료로 활용한 링크와 커널 코드를 살펴보자.\n메서드 일반적으로 메서드를 생각하면, C에서 함수 포인터를 구조체에 정의하는 것을 떠올린다. 하지만 커널에서는 직접적으로 구조체 안에 함수 포인터를 사용하는 대신에 vtable을 만들어 _ops 으로 명명한 별도의 함수 테이블을 사용한다. 예를 들어, media framework로 유명한 V4L2를 이용하는 videobuf2를 살펴보자. 영상 프레임을 관리하는 큐에서 메모리 관리에 관련된 메서드는 아래와 같이 정의하여 사용한다.\nstruct vb2_queue { unsigned int\ttype; unsigned int\tio_modes; struct device\t*dev; unsigned long\tdma_attrs; const struct vb2_ops\t*ops; const struct vb2_mem_ops\t*mem_ops; ... 그리고 vb2_queue에서 메서드 dispatch를 위해서 사용하는 메모리 관련 메서드는 아래와 같이 정의한다.\nstruct vb2_mem_ops { void\t*(*alloc)(struct device *dev, unsigned long attrs, unsigned long size, enum dma_data_direction dma_dir, gfp_t gfp_flags); void\t(*put)(void *buf_priv); struct dma_buf *(*get_dmabuf)(void *buf_priv, unsigned long flags); void\t*(*get_userptr)(struct device *dev, unsigned long vaddr, unsigned long size, enum dma_data_direction dma_dir); void\t(*put_userptr)(void *buf_priv); void\t(*prepare)(void *buf_priv); void\t(*finish)(void *buf_priv); void\t*(*attach_dmabuf)(struct device *dev, struct dma_buf *dbuf, unsigned long size, enum dma_data_direction dma_dir); void\t(*detach_dmabuf)(void *buf_priv); int\t(*map_dmabuf)(void *buf_priv); void\t(*unmap_dmabuf)(void *buf_priv); void\t*(*vaddr)(void *buf_priv); void\t*(*cookie)(void *buf_priv); unsigned int\t(*num_users)(void *buf_priv); int\t(*mmap)(void *buf_priv, struct vm_area_struct *vma); }; virtual function table을 사용할 경우 객체별로 사용할 수 있는 메서드들에 대한 인터페이스만 정의하고 실제 메서드에 대한 내용은 별도로 구현하여 사용할 수 있다. 즉, 클래스로 정의된 메서드 내용은 같지만 구현 내용은 객체마다 서로 다르게 할 수 있다는 장점이 있다.\n그리고 vtable은 메서드에 대한 다중상속을 가능하게 하는데, closure와 같은 다른 언어에서 mixin이라 표현하는 것처럼 응용할 수 있다. 서로 다른 객체에 대해 같은 메서드를 사용할 수 있도록 하는 방법이다.\n데이터 상속 예전부터 데이터 상속은 여러 형태로 존재해왔는데, 여기서는 아래 세 가지 형태의 데이터 상속을 다루도록 한다.\nunion을 이용한 데이터 상속 void*를 이용한 데이터 상속 상속하고자 하는 데이터 직접 내포 union을 이용한 데이터 상속 struct inode를 살펴보면 아래와 같은 코드를 살펴볼 수 있다.\nunion { struct minix_inode_info minix_i; struct ext_inode_info ext_i; struct msdos_inode_info msdos_i; } u; inode 안에서 union을 이용하여 노드에 대한 정보를 관리한다고 했을 때, 해당 inode 클래스는 상기 세 가지 중 하나에 대한 데이터를 상속하게 된다. 이는 직관적으로 코드를 이해할 수 있다는 장점이 있지만, union을 사용하는 까닭에 padding을 위해 필요한 메모리 낭비로 이어질 수 있다.\nvoid* 이용한 데이터 상속 커널에 정의된 프레임워크를 이용하다 보면 종종 void* private; 으로 정의된 것이 구조체 안에 정의되어 있는 것을 알 수 있다. 위에서 union을 사용한 것과 달리 void* 사용하게 되면 불필요한 메모리는 줄일 수 있고 데이터 상속에 대한 유연성을 갖출 수 있지만 **실제로 어떤 데이터를 사용해야 하는가?**에 대한 질문에 직관적인 해석을 가져다 주지 못한다. 여전히 V4L 프레임워크와 같이 몇 군데에서 void* private; 형태로 사용되고 있지만 문서화와 쉽게 코드를 파악할 수 있는 구조가 아니라면, 이러한 포인터 형태는 지양되어야 한다.\nembedded structure 직접적으로 필요한 데이터들을 구조체 안에 멤버 변수로 정의하고, container_of 매크로를 통해 부모 객체에 접근하도록 구현하는 방법이다. void* 에 비해 유연성은 떨어지지만 명시적으로 어떤 데이터를 상속하는지 나타낼 수 있고 매크로를 통해 부모에 정의되어 있는 함수 테이블을 이용하는 등 객체 지향 패턴을 적용하는데 무리가 없다. 여러 파일시스템의 코드를 살펴보면 아래와 같이 기본적인 inode에 대한 데이터 자체를 아래와 같이 내포한 형태로 사용하는 것을 알 수 있다.\n/* in memory btrfs inode */ struct btrfs_inode { /* which subvolume this inode belongs to */ struct btrfs_root *root; /* key used to find this inode on disk. This is used by the code * to read in roots of subvolumes */ struct btrfs_key location; ... struct inode vfs_inode; }; 결론 객체지향 패턴을 적용하는 것은 중요하지만 만능은 아니다. 모든 곳에 이러한 객체 지향 디자인 패턴을 적용해야 하는 것은 잘못된 것이고 오히려 분석을 어렵게 만들 수도 있다. 디자인 패턴은 어디까지나 디자인 패턴일 뿐. 언어 때문에 특정 디자인 패턴을 적용할 수 없다는 얘기도 반은 맞고 반은 틀렸다. 커널에서 사용되는 kref 형태의 reference count 또한 특정 언어에 국한된 설계 패턴이 아니다. 타겟과 개발 환경에 따라 필요성과 효율성이 달라지는 것일 뿐 정답은 없다.\n", 
        "url": "\/\/localhost:1313\/posts\/%EC%BB%A4%EB%84%90%EC%97%90%EC%84%9C%EC%9D%98-object-oriented-design-pattern\/"
    },
    
    "\/\/localhost:1313\/tags\/computer-science\/": {
        "title": "Computer Science",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/computer-science\/"
    },
    
    "\/\/localhost:1313\/posts\/%EB%A7%A5-os%EC%97%90%EC%84%9C-%EB%A6%AC%EB%88%85%EC%8A%A4-%EC%BB%A4%EB%84%90-%EC%BB%B4%ED%8C%8C%EC%9D%BC%ED%95%98%EA%B8%B0\/": {
        "title": "맥 OS에서 리눅스 커널 컴파일하기",
        "tags": ["Computer Science",],
        "content": "개요 맥 환경에서 bare-metal이 아닌 리눅스 커널로 컴파일하는 것은 생각했던 것보다 까다롭다. BSD 계열이기는 하지만 커널 컴파일에 필요한 라이브러리와 경로들이 리눅스 환경과 다르기 때문에, 일반적인 리눅스 배포판에서처럼 크로스 컴파일러를 바이너리 형태로 설치한 뒤에 곧바로 커널 빌드를 할 수는 없다.\n이 포스팅은 맥에서 리눅스 커널을 빌드하려는 사람들을 위해 작성한 것으로, 아래 구성으로 간략하게 기술하겠다.\n준비 사항 및 제약 사항 크로스 컴파일러 빌드 커널 빌드 끝맺음 본 포스팅에서 확인한 맥 환경은 아래와 같으며 단순 바이너리만 받고자 하는 경우 아래 링크에서 받도록 한다. https://github.com/seokbeomKim/armv8-rpi3-linux-gnueabihf 준비 사항 및 제약 사항 맥에서 리눅스 커널을 컴파일을 할 경우에는 크로스 컴파일러를 직접 빌드해줘야 한다. 우분투와 같이 바이너리 형태로 패키지 관리자에서 제공해주는 경우에는 손쉽게 받을 수 있지만 그렇지 않은 배포판이나 맥의 경우에는 직접 만들어 사용해야 한다.\n맥에서의 대표적인 패키지 매니저는 애플에서 공식적으로 제공하지는 않지만 homebrew이다. 이 패키지 매니저를 통해 bare-metal 크로스 컴파일러는 다운로드 할 수 있지만 GLIBC가 포함된 크로스 컴파일러는 제공되지 않으므로 라즈베리파이를 위한 커널 컴파일 시에 크로스 컴파일러를 직접 빌드해야 한다.\n크로스 컴파일러 빌드 환경 구성 시 맥에서는 파티션 포맷 제약사항이 있다. 맥에서 기본으로 사용하는 파일시스템인 APFS는 기본으로 case-insensitive 이므로 반드시 case-sensitive 파티션을 추가로 구성한 후 해당 파티션에서 빌드를 진행해야 한다. 필자는 맥 설치 시에 파티션의 포맷 자체를 case-sensitive 방식으로 지정하고 포맷해주었기 때문에 관련된 문제는 발생하지 않았다.\n그리고 기본 컴파일러로서 GNU gcc가 아닌 clang을 이용하므로, homebrew 패키지 관리자를 통해 openssl, gcc 등을 설치해야 한다. openssl은 반드시 openssl@1.1 을 설치한다.\n$ brew install openssl@1.1 gcc 크로스 컴파일러 빌드: ct-ng 크로스 컴파일러는 Crosstool-NG(ct-ng)를 이용하여 빌드한다.\n$ brew install ct-ng $ mkdir -p ~/workspace/ct-ng-rpi3 \u0026amp;\u0026amp; cd ~/workspace/ct-ng-rpi3 $ ct-ng armv8-rpi3-linux-gnueabihf armv8-rpi3-linux-gnueabihf는 ct-ng에서 제공하는 샘플 중 하나로서 샘플들은 아래와 같이 확인할 수 있다.\n$ ct-ng list-samples gdb 빌드 도중에 발생하는 파이썬 에러를 방지하기 위해 아래와 같이 menuconfig를 통해 파이썬의 바이너리 경로를 설정해주자.\n$ ct-ng menuconfig # menuconfig 창에서 아래 설정 메뉴를 통해 파이썬 바이너리 경로를 설정한다. Debug facilities -\u0026gt; gdb -\u0026gt; Python binary to use (/usr/bin/python) 또한, 필자처럼 binutils 에서 string 관련 에러가 난다면, 아래와 같이 직접 \u0026lt;string\u0026gt; 헤더파일을 include 하도록 수정해줘야 한다.\n$ $home/workspace/ct-ng/rpi3b/.build/src/binutils-2.32 $ vi gold/errors.h // 아래 include 에 \u0026lt;string\u0026gt;을 추가한다. // MA 02110-1301, USA. #ifndef GOLD_ERRORS_H #define GOLD_ERRORS_H #include \u0026lt;cstdarg\u0026gt; #include \u0026lt;string\u0026gt; #include \u0026#34;gold-threads.h\u0026#34; 크로스 컴파일러 빌드가 완료되었다면 $HOME/x-tools 경로 아래에 컴파일러가 생성된 것을 확인할 수 있다. 크로스 컴파일러를 빌드하면서 발생하는 에러는 맥 운영체제로 인한 것이 아니라 크로스 컴파일 환경과 타겟 gcc 버전의 호환성 문제로 인한 것이 대부분이다. 이러한 문제들은 구글링으로 관련 정보를 손쉽게 찾을 수 있다.\n라즈베리파이 커널 빌드 이제 라즈베리파이 커널을 예로 빌드해보자. 맥에서 커널 빌드시 첫 번째로 문제가 되는 것은 elf.h 파일이다. 맥에서는 이 파일을 사용하지 않기 때문에 기본으로 include 경로에 포함되어 있지 않다. 빌드 시에는 필수 파일이므로, 경로(https://www.rockbox.org/tracker/task/9006?getfile=16683 )에서 다운로드 하여 include 경로(/usr/local/include)에 추가해준다.\n이제 마지막으로 라이브러리와 INCLUDE 경로를 설정해주며 빌드를 해주자.\n$ KERNEL=kernel7 ARCH=arm make bcm2709_defconfig $ KERNEL=kernel7 ARCH=arm HOSTCFLAGS=\u0026#34;-I/usr/local/include -I/usr/local/opt/openssl@1.1/include -L/usr/local/opt/openssl/lib\u0026#34; make -j4 빌드가 완료되면, 아래와 같이 커널 이미지(zImage)가 만들어진 것을 확인할 수 있다.\n끝맺음 위의 과정들을 하지 않고 단순하게 바이너리만 받고 싶다면 https://github.com/seokbeomKim/armv8-rpi3-linux-gnueabihf 에서 받도록 한다. 앞서 기술한 방법대로 빌드한 크로스 컴파일러로서 성능에 따라 컴파일 타임은 차이가 나지만 CPU 성능이 안좋은 경우 필자처럼 한 시간이 넘게 걸리기도 한다.\n분명, 나중에 본 포스팅을 다시 참고하게 될 날이 올 것이다. 정리를 해도 매번 잊어버리는게 습관이고 매번 지난 글을 뒤적이는 게 습관이다. 하지만, 이번처럼 맥/BSD에서 리눅스의 ELF로 동작하거나 컴파일할 수 있도록 고생했던 삽질의 내용들은 되도록이면 오랫동안 기억할 수 있었으면 좋겠다.\n참고 자료 https://www.rockbox.org/tracker/task/9006 https://wiki.osdev.org/GCC_Cross-Compiler https://github.com/raspberrypi/linux https://github.com/crosstool-ng/crosstool-ng/issues/844 https://www.jaredwolff.com/cross-compiling-on-mac-osx-for-raspberry-pi/ ", 
        "url": "\/\/localhost:1313\/posts\/%EB%A7%A5-os%EC%97%90%EC%84%9C-%EB%A6%AC%EB%88%85%EC%8A%A4-%EC%BB%A4%EB%84%90-%EC%BB%B4%ED%8C%8C%EC%9D%BC%ED%95%98%EA%B8%B0\/"
    },
    
    "\/\/localhost:1313\/posts\/%EB%A7%A5%EB%B6%81%EC%97%90-%EB%A6%AC%EB%88%85%EC%8A%A4-%EC%84%A4%EC%B9%98%ED%95%98%EA%B8%B0\/": {
        "title": "맥북에 리눅스 설치하기",
        "tags": [],
        "content": "개요 맥을 사용하면서 한 가지 느낀 점은 맥 운영체제가 로우 레벨 개발자에게는 그닥 좋지 않은 운영체제라는 것이다. 일반 애플리케이션이나 웹 분야의 개발자라면 각종 IDE나 유닉스 환경의 맥에서도 충분히 리눅스 계열의 애플리케이션들을 포팅해서 사용할 수 있기에 제약사항이 많지 않다. 하지만 리눅스 커널 컴파일을 하려면 얘기가 달라진다. GNU GCC 자체도 포팅된 것을 준비해야할 뿐 아니라, glibc를 포함한 build essentials 패키지들이 리눅스용으로 설치되어 있어야 한다.\n그리고 시리얼 통신을 지원하는 애플리케이션의 경우 맥에서 사용할 수 있는게 많지 않다. 멀티미디어 분야를 주력으로 애플리케이션을 이용하는 사용자에게는 매력적이겠지만, 로우레벨의 개발자들에게는 지원이 너무나도 적은 운영체제이다.\n아직까지 x86 계열의 17년형 맥북이므로 ARM 계열까지 바뀌려면 아직 시간이 남았다. 부트캠프도 지원하는 마당에 리눅스도 함께 설치할 수 있을거라 생각했는데 다행히 여러 곳에서 이미 진행이 되고 있었고 고질적인 문제들이 몇 개 남아있지만 크리티컬한 것은 아니어서 아주 만족스럽다.\n아래는 리눅스를 설치하는데 참고한 사이트이다. 기존에는 리눅스를 설치했을 때 오디오나 터치패드 사용에 문제가 있었는데 블로그의 내용 덕분에 필요한 삽질들을 줄일 수 있었다.\n참고 사이트는 https://dev.to/cmiranda/linux-on-macbook-pro-2016-1onb 와 https://github.com/Dunedan/mbp-2016-linux 사이트이다.\n설치 환경 및 이슈 설치 환경은 17년형 논터치바 13인치 맥북(mbp14,1)이다. 아래와 같은 사소한 이슈들이 있지만 개발 환경 구성에 크게 문제되지는 않는다. 나타나는 이슈들은 맥북 모델에 따라 다르고 실험적인 환경 구성이기 때문에 안정화 되어 있지 않다.\nWired connection / Wireless connection: 시스템 suspend 나 재시작 시 인터넷 문제 발생 (둘 중 하나는 사용 불가)\n5GHz WiFi 사용 불가능\n원래는 젠투나 아치리눅스를 설치하려고 했지만, 어떤 문제가 나타날지도 모르는 상태에서 오디오 패치나 펌웨어 설정 등 데비안 계열로 누군가가 작업해놓은 것을 적용하기 위해서 민트 리눅스를 새로 사용해보기로 했다. 오디오는 cirrus 드라이버 설치로 문제를 해결할 수 있었고 https://github.com/leifliddy/snd_hda_macbookpro.git git에서 받을 수 있다.\n", 
        "url": "\/\/localhost:1313\/posts\/%EB%A7%A5%EB%B6%81%EC%97%90-%EB%A6%AC%EB%88%85%EC%8A%A4-%EC%84%A4%EC%B9%98%ED%95%98%EA%B8%B0\/"
    },
    
    "\/\/localhost:1313\/tags\/git\/": {
        "title": "Git",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/git\/"
    },
    
    "\/\/localhost:1313\/categories\/git\/": {
        "title": "Git",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/categories\/git\/"
    },
    
    "\/\/localhost:1313\/posts\/git-prune\/": {
        "title": "git prune",
        "tags": ["git",],
        "content": "git prune 명령어는 unreachable한 git object를 로컬에서 제거하는 명령어이다.\ngit prune 수행 시에 --dry-run --verbose 옵션을 주면 실제 수행되지는 않고 어떻게 수행될지에 대한 시뮬레이션을 볼 수 있다.\n$ git remote prune --dry-run --verbose ", 
        "url": "\/\/localhost:1313\/posts\/git-prune\/"
    },
    
    "\/\/localhost:1313\/tags\/mu4e\/": {
        "title": "Mu4e",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/mu4e\/"
    },
    
    "\/\/localhost:1313\/posts\/mu4e-%EC%84%A4%EC%A0%95%ED%95%98%EA%B8%B0\/": {
        "title": "mu4e 설정하기",
        "tags": ["mu4e",],
        "content": "개요 이맥스에서는 이메일 클라이언트로서 사용할 수 있는 mu4e라는 패키지가 있다. 일반적으로 isync (mbsync) 라는 프로그램과 함께 사용하며 mu 를 설치하면 설치 디렉토리 내에 함께 포함되어 있다. mu는 메일 수신 및 분류, 저장 보다는 인덱싱과 검색을 위한 프로그램이기 때문에 원하는 메일을 빠르게 검색하고 org mode와 함께 사용하여 필요한 내용을 간단하게 메일 형태로 만들어낼 수 있는 것이 특징이다.\n그런데 outlook 이나 mailbird, thunderbird 등의 메일 클라이언트가 존재하는데 굳이 mu 를 사용하는 이유가 있을까? 일반 사용자라면 mu 보다는 앞서 언급된 메일 클라이언트를 사용하는 것이 좋다. 하지만 리눅스 커널 프로젝트에 패치를 보내고 다른 사람들이 보낸 패치를 받아 적용하는 등의 작업이 필요하다면 일반적인 클라이언트보다는 mu 를 사용하는 것이 좋다. 이메일을 통해 패치 파일을 inline 형태로 보내야 하는데 첨부 형식을 보장하는 메일 클라이언트는 몇 개 남아있지 않다. 또한 일부 메일 클라이언트는 간단한 평문을 보내더라도 html 를 이용해 태그가 붙어 있는 경우도 있다.\nPatches for the Linux kernel are submitted via email, preferably as inline text in the body of the email. Some maintainers accept attachments, but then the attachments should have content-type text/plain. However, attachments are generally frowned upon because it makes quoting portions of the patch more difficult in the patch review process.\nmu4e를 설정하는 방법은 공식 문서와 몇몇 블로그 포스팅을 통해 관련 내용들을 찾을 수 있다.\nhttps://rakhim.org/fastmail-setup-with-emacs-mu4e-and-mbsync-on-macos/ http://pragmaticemacs.com/mu4e-tutorials/ 환경 구성 mu4e를 구성 \u0026amp; 설정하는 순서는 대략 아래와 같다.\nmbsync (isync 패키지 내에 포함된 메일 fetcher) 설정 및 local inbox 생성 mu 인덱싱 구성 mu4e 사용하기 위한 이맥스 설정 isync 설치 mbsync 명령어는 isync 패키지 안에 포함된다. 우분투 기준으로 아래와 같이 공식 repo 를 통해 설치 가능하다.\n$ sudo apt install isync isync 설정 아치리눅스 위키 페이지에 패키지에 대한 설명이 자세히 기술되어 있다. 설치 후에는 아래 링크 또는 포스팅에 있는 예제 파일들을 참고하여 설정 파일을 작성한다.\nhttps://wiki.archlinux.org/index.php/Isync # GMAIL #1 IMAPAccount sukbeom.kim Host imap.gmail.com User sukbeom.kim@gmail.com PassCmd \u0026#34;gpg --batch --passphrase mu4e --no-tty -qd ~/.sbk.gpg\u0026#34; AuthMechs LOGIN SSLType IMAPS SSLVersions TLSv1.3 CertificateFile /etc/ssl/certs/ca-certificates.crt IMAPStore gmail-remote Account sukbeom.kim MaildirStore gmail-local Path ~/mbsync/sukbeom.kim@gmail.com/ Inbox ~/mbsync/sukbeom.kim@gmail.com/INBOX Channel gmail-inbox Master :gmail-remote: Slave :gmail-local: Patterns \u0026#34;INBOX\u0026#34; Create Both Expunge Both SyncState * Channel gmail-trash Master :gmail-remote:\u0026#34;[Gmail]/Bin\u0026#34; Slave :gmail-local:\u0026#34;[Gmail].Bin\u0026#34; Create Both Expunge Both SyncState * Channel gmail-sent Master :gmail-remote:\u0026#34;[Gmail]/Sent Mail\u0026#34; Slave :gmail-local:\u0026#34;[Gmail].Sent Mail\u0026#34; Create Both Expunge Both SyncState * Channel gmail-all Master :gmail-remote:\u0026#34;[Gmail]/All Mail\u0026#34; Slave :gmail-local:\u0026#34;[Gmail].All Mail\u0026#34; Create Both Expunge Both SyncState * Channel gmail-starred Master :gmail-remote:\u0026#34;[Gmail]/Starred\u0026#34; Slave :gmail-local:\u0026#34;[Gmail].Starred\u0026#34; Create Both Expunge Both SyncState * Group gmail Channel gmail-inbox Channel gmail-sent Channel gmail-trash Channel gmail-all Channel gmail-starred IMAPAccount chaoxifer Host imap.gmail.com User chaoxifer@gmail.com PassCmd \u0026#34;gpg --batch --passphrase mu4e --no-tty -qd ~/.chaoxifer.gpg\u0026#34; AuthMechs LOGIN SSLType IMAPS SSLVersions SSLv3 CertificateFile /etc/ssl/certs/ca-bundle.crt IMAPStore chaoxifer-remote Account chaoxifer MaildirStore chaoxifer-local Path ~/mbsync/chaoxifer@gmail.com/ Inbox ~/mbsync/chaoxifer@gmail.com/INBOX Channel chaoxifer-inbox Master :chaoxifer-remote: Slave :chaoxifer-local: Patterns \u0026#34;INBOX\u0026#34; Create Both Expunge Both SyncState * Channel chaoxifer-trash Master :chaoxifer-remote:\u0026#34;[Gmail]/Bin\u0026#34; Slave :chaoxifer-local:\u0026#34;[Gmail].Bin\u0026#34; Create Both Expunge Both SyncState * Channel chaoxifer-sent Master :chaoxifer-remote:\u0026#34;[Gmail]/Sent Mail\u0026#34; Slave :chaoxifer-local:\u0026#34;[Gmail].Sent Mail\u0026#34; Create Both Expunge Both SyncState * Channel chaoxifer-all Master :chaoxifer-remote:\u0026#34;[Gmail]/All Mail\u0026#34; Slave :chaoxifer-local:\u0026#34;[Gmail].All Mail\u0026#34; Create Both Expunge Both SyncState * Channel chaoxifer-starred Master :chaoxifer-remote:\u0026#34;[Gmail]/Starred\u0026#34; Slave :chaoxifer-local:\u0026#34;[Gmail].Starred\u0026#34; Create Both Expunge Both SyncState * Group chaoxifer Channel chaoxifer-inbox Channel chaoxifer-sent Channel chaoxifer-trash Channel chaoxifer-all Channel chaoxifer-starred 설정에서 gpg를 이용하여 계정 암호를 설정파일에 그대로 넣지 않고 암호화한 파일을 사용하도록 한다. gpg 파일을 만드는 방법은 아래와 같다.\n$ echo \u0026#34;mypassword!\u0026#34; \u0026gt; ~/.mbsyncpass # 입력 창에 passphrase 를 입력한다. 위 설정에서는 mu4e를 passphase로 입력했다. $ gpg --output ~/.emacs.d/personal/chaoxifer.gpg --symmetric ~/.mbsyncpass 이제 mbsync를 사용하기 위한 설정이 완료되었다. 필요한 디렉토리들을 생성 후 mbsync -a를 통해 메일 박스를 동기화한다.\n$ mkdir -p $HOME/mbsync/{chaoxifer@gmail.com,sukbeom.kim@gmail.com} $ mbsync -a mu 설치 mu4e 사용을 위해, mu 패키지를 설치한다. 이 때 컴파일 옵션으로 이맥스 옵션이 있으니 확인하자. 기본적으로 이맥스 옵션이 활성화되어 패키지가 설치되므로 특별한 경우가 아니라면 별도로 설정하지 않아도 된다. mu 는 공식 repo 대신 https://github.com/djcb/mu/releases/tag/v1.8.14 에서 직접 받아서 설치하였다. 우분투의 공식 repo 에서 제공하는 버전과 꽤 차이가 나고 몇몇 버그 패치가 되지 않았기 때문에 직접 받아서 설치하는 것을 권장한다.\n$ tar xf mu-1.8.14.tar.xz $ cd mu-1.8.14 $ ./autogen.sh --prefix=$HOME/opt $ make \u0026amp;\u0026amp; make install 필자는 로컬 유저에서 받아서 설치하는 것은 모두 $HOME/opt 경로로 설치하는 타입이라 위와 같이 configure 을 진행하였다. 설치하고 난 뒤 디렉토리에 가보면 아래와 같이 mu4e 가 설치되어 있는 것을 볼 수 있다.\nsukbeom@LAPTOP-R4FQS2C5:~$ ls opt/share/emacs/site-lisp/mu4e/ mu4e-actions.el mu4e-context.el mu4e-headers.elc mu4e-mark.el mu4e-server.elc mu4e.el mu4e-actions.elc mu4e-context.elc mu4e-helpers.el mu4e-mark.elc mu4e-speedbar.el mu4e.elc mu4e-bookmarks.el mu4e-contrib.el mu4e-helpers.elc mu4e-message.el mu4e-speedbar.elc org-mu4e.el mu4e-bookmarks.elc mu4e-contrib.elc mu4e-icalendar.el mu4e-message.elc mu4e-update.el org-mu4e.elc mu4e-compose.el mu4e-draft.el mu4e-icalendar.elc mu4e-org.el mu4e-update.elc mu4e-compose.elc mu4e-draft.elc mu4e-lists.el mu4e-org.elc mu4e-vars.el mu4e-config.el mu4e-folders.el mu4e-lists.elc mu4e-search.el mu4e-vars.elc mu4e-contacts.el mu4e-folders.elc mu4e-main.el mu4e-search.elc mu4e-view.el mu4e-contacts.elc mu4e-headers.el mu4e-main.elc mu4e-server.el mu4e-view.elc mu 설정 이제 mbsync 로 생성해놓았던 디렉토리를 기준으로 초기화한다.\n$ mu init --maildir=$HOME/mbsync $ mu info +-------------------+--------------------------------+ | maildir | /home/sukbeom/mbsync | +-------------------+--------------------------------+ | database-path | /home/sukbeom/.cache/mu/xapian | +-------------------+--------------------------------+ | schema-version | 465 | +-------------------+--------------------------------+ | max-message-size | 100000000 | +-------------------+--------------------------------+ | batch-size | 250000 | +-------------------+--------------------------------+ | created | Thu Mar 16 23:23:40 2023 | +-------------------+--------------------------------+ | personal-address | | +-------------------+--------------------------------+ | messages in store | 38 | +-------------------+--------------------------------+ | last-change | Sat Mar 18 07:09:36 2023 | +-------------------+--------------------------------+ | last-index | Sat Mar 18 07:09:32 2023 | +-------------------+--------------------------------+ 이맥스 설정 이제 마지막으로, 아래와 같이 mu4e를 로드한다. mu4e에 대한 추가 설정은 mu4e의 공식 문서에 자세히 기술되어 있다.\n; 설치한 경로로 설정한다. (add-load-path! \u0026#34;/home/sukbeom/opt/share/emacs/site-lisp/mu4e/\u0026#34;) (require \u0026#39;mu4e) (require \u0026#39;mu4e-contrib) (require \u0026#39;gnutls) (setq mu4e-sent-folder \u0026#34;/sent\u0026#34;\t;; folder for sent messages mu4e-drafts-folder \u0026#34;/drafts\u0026#34;\t;; unfinished messages mu4e-trash-folder \u0026#34;/trash\u0026#34;\t;; trashed messages mu4e-refile-folder \u0026#34;/archive\u0026#34;\t;; saved messages user-mail-address \u0026#34;메일 주소\u0026#34; smtpmail-default-smtp-server \u0026#34;smtp.gmail.com\u0026#34; smtpmail-smtp-server \u0026#34;smtp.gmail.com\u0026#34; smtpmail-smtp-service 587) (setq mu4e-index-cleanup nil\t;; don\u0026#39;t do a full cleanup check mu4e-index-lazy-check t) ;; don\u0026#39;t consider up-to-date dirs (setq mu4e-maildir \u0026#34;~/mbsync/\u0026#34; mu4e-attachment-dir \u0026#34;~/mbsync/attachments\u0026#34; user-full-name \u0026#34;Sukbeom Kim\u0026#34;) (setq mu4e-get-mail-command \u0026#34;mbsync -a\u0026#34; mu4e-change-filenames-when-moving t mu4e-view-show-images t mu4e-view-show-addresses t mu4e-update-interval (* 20 60)) (setq mu4e-html2text-command \u0026#39;mu4e-shr2text shr-color-visible-luminance-min 80 shr-color-visible-distance-min 5)) 마치며 이제 필요한 설정이 모두 마무리 되었다. M-x mu4e를 실행해 제대로 메일이 보이는지 확인한다.\nmu4e 를 통해 이제 수신한 메일들을 관리하거나 지정되어 있는 smtp 서버를 통해 메일을 주고받을 수 있다. mu4e는 thunderbird의 마크다운 지원하는 메일 작성의 개념과 비슷하지만 모드를 자유자재로 선택할 수 있다는 점에서 더 유연하다는 장점이 있다. 또한 snippet 과 코드를 직접 메일에 붙일 수 있기에 메일 송수신 시에 매우 유용하다.\n", 
        "url": "\/\/localhost:1313\/posts\/mu4e-%EC%84%A4%EC%A0%95%ED%95%98%EA%B8%B0\/"
    },
    
    "\/\/localhost:1313\/posts\/%EC%BA%90%EC%8B%9C-%EB%9D%BC%EC%9D%B8cache-line\/": {
        "title": "캐시 라인(Cache-Line)",
        "tags": ["cache",],
        "content": "캐시를 꼭 사용해야 하나 SMP 환경에서 일어나는 커널 패닉 문제를 다루다 보면 메모리 배리어를 비롯하여 캐시와 주 메모리 간의 동기화를 다루는 내용이 주를 이룬다. 하지만 매번 간략하게 정리한 문서가 없다보니 기억에 잘 남지도 않고 전체적인 흐름을 읽지 못하는 것 같아 이번 포스팅을 통해서 대략적으로나마 정리하고자 한다.\n프로세서 캐시와 메모리의 불일치 문제는 다루기 까다로운 문제이다. 단순하게 프로세서가 싱글 코어라면 문제가 없겠지만 여러 개의 코어를 가지고 있는 프로세서라면 각각의 코어가 가지고 있는 캐시와 주 메모리를 동기화 시켜주어야 한다. 그러한 동기화로 인해 캐시 성능이 제대로 나오지 않을 것 같은데도 \u0026lsquo;코어 ~ 캐시\u0026rsquo;의 액세스 속도와 \u0026lsquo;코어 ~ 주 메모리\u0026rsquo; 간의 속도 차이가 그러한 동기화 이슈를 가릴 정도로 충분히 크기 때문에 여전히 필자가 보이게 변태같은 구조로 열심히 캐시를 사용하고 있다.\n주 메모리의 내용을 캐시에 저장하기 위해서 주 메모리의 데이터를 읽어와야 한다. 이 때, 읽어들이는 최소 단위를 캐시라인(cahce-line)이라 하며, 이렇게 읽어들인 데이터는 캐시의 data block을 구성하게 된다. 캐시는 아래와 같이 간략하게 나타낼 수 있다.\n+------------------------ cache --------------------------------------+ | +-----------------------------------------------------------------+ | | | memory address | status bit | data block | | | +-----------------------------------------------------------------+ | | +-----------------------------------------------------------------+ | | | memory address | status bit | data block | | | +-----------------------------------------------------------------+ | | +-----------------------------------------------------------------+ | | | memory address | status bit | data block | | | +-----------------------------------------------------------------+ | | +-----------------------------------------------------------------+ | | | memory address | status bit | data block | | | +-----------------------------------------------------------------+ | | +-----------------------------------------------------------------+ | | | memory address | status bit | data block | | | +-----------------------------------------------------------------+ | | +-----------------------------------------------------------------+ | | | memory address | status bit | data block | | | +-----------------------------------------------------------------+ | | | | | | | +---------------------------------------------------------------------+ 메모리 주소의 일부를 이용하여 만든 태그와 해당 캐시라인의 상태(invalid or modified)를 나타내는 상태 비트, 그리고 데이터 블록으로 구성된다. 캐시 정책에 따라 구조가 조금씩 달라지기는 하지만 대략적인 개념은 위와 같으며, 포인트는 메모리의 데이터를 워드 단위가 아닌 \u0026ldquo;cache-line\u0026quot;의 크기 단위로 읽어들이며, 해당 데이터의 메모리 주소를 이용하여 태그로써 관리한다는 점이다.\n캐시 관리 방법 캐시를 관리하는 방식으로 Full Associative, Set Associative, Direct Mapped 등이 있다.\n먼저 제일 간단한 \u0026ldquo;direct map\u0026rdquo; 방식부터 살펴보면, 메모리 주소의 일부를 인덱스(위에서 태그로 표현한 부분)로 사용하여 관리하는 방식이다. 이러한 방식 때문에, 같은 인덱스를 갖는 메모리가 인접하여 액세스되는 경우 cache miss가 발생할 수 있다. 데이터 블록 크기만큼 캐시되지 않으면서, 주소의 부분만을 인덱스로 사용하는 방식이기 때문에 캐시되지 않은 인접 데이터로 인해 cache miss가 발생하는 시나리오다.\n다음은 \u0026ldquo;Full Associative\u0026rdquo; 방식으로 쉽게 설명하면 비어있는 캐시메모리가 있으면 그냥 마음대로 주소를 저장하는 방식이다. 이 때문에, 반드시 DRAM이 아닌 SRAM을 사용해야 하며 저장할 때는 크게 알고리즘이 필요없어서 간단한데, 찾을 때가 문제다. \u0026ldquo;어떠한 조건이나 규칙이 없으니 특정 캐쉬 Set내의 모든 블럭을 한 번에 찾아 원하는 데이터가 있는지 검사해야 하며\u0026rdquo; 이를 위해 CAM (Content Addressable Memory) 라는 특수한 형태의 메모리 구조를 사용해야 한다. 여기서는 인덱스 대신 index + tag 조합으로 된 태그 부분으로 구성된다.\n마지막으로 Set Associative 방식이다. 여러 개의 Direct Mapped 방식으로 나누어 사용한다고 생각하면 된다. 본래 메모리 주소의 일부만을 이용하여 태그로 사용하는 Direct Mapped 방식이기에 이를 2개, 4개로 구성한 여러 개의 Direct Mapped 방식에 Multiplexer와 OR gate를 조합하여 아래와 같이 구성한다.\n+---------------------+-----------------+----------------+--------------+ | memory address | | | | +----------+----------+--------+--------+--------+-------+--------------+ | | | | | | | | | | | | v v +---------v +----------+----------+ +-----+---------------+ +---------------------+ | direct|mapped cache | | direct|mapped cache | | direct|mapped cache | +----------+----------+ +----------+----------+ +----------+----------+ | | | +------------------------+---------+--------------+ | | +----------v-------------------------+ +---v-----+ | multiplexer | | OR Gate | +------------------------------------+ +---------+ 출처 https://sonofgodcom.wordpress.com/2017/12/04/%EC%BB%B4%ED%93%A8%ED%84%B0%EA%B5%AC%EC%A1%B0-3-3-%EC%BA%90%EC%8B%9C-%EB%A9%94%EB%AA%A8%EB%A6%AC-%EA%B3%84%EC%B8%B5-set-associative/ https://en.wikipedia.org/wiki/CPU_cache ", 
        "url": "\/\/localhost:1313\/posts\/%EC%BA%90%EC%8B%9C-%EB%9D%BC%EC%9D%B8cache-line\/"
    },
    
    "\/\/localhost:1313\/tags\/vmlinux\/": {
        "title": "Vmlinux",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/vmlinux\/"
    },
    
    "\/\/localhost:1313\/posts\/vmlinux\/": {
        "title": "Vmlinux",
        "tags": ["vmlinux",],
        "content": "커널 패닉 정보를 바탕으로 패닉이 정확히 어디서 발생했는지를 알아내기 위해 vmlinux 파일을 이용하는 방법을 알게 되었다. addr2line 명령어를 이용하여 PC 또는 LR에 들어있던 주소를 이용하여 실행된 명령어가 코드상으로 어디에 위치했는지를 알아내는 것인데 어떤 이유로 디버깅이 가능한지 궁금했기에 이 포스팅에서는 addr2line을 이용한 커널패닉 분석 방법과 vmlinux에 관련된 파일 종류에 대해 간단히 언급하고자 한다.\n커널 패닉 메시지 분석하기 출처로 명시된 페이지에서 기술되어 있듯 아래와 같은 커널 패닉 메시지가 나왔다고 가정했을 때 addr2line 명령어와 vmlinux 파일을 이용해 실행 위치를 알아낼 수 있다.\n[ 0.167728] BUG: unable to handle kernel NULL pointer dereference at 0000000000000050 [ 0.167733] IP: [\u0026lt;ffffffff810b6fda\u0026gt;] task_tick_fair+0xea/0x9e0 [ 0.167734] PGD 0 [ 0.167736] Oops: 0000 [#1] SMP [ 0.167737] Modules linked in: [ 0.167739] CPU: 0 PID: 1 Comm: swapper/0 Not tainted 4.6.4-dwrr #11 [ 0.167740] Hardware name: Supermicro H8SGL/H8SGL, BIOS 2.0a 11/11/2011 [ 0.167741] task: ffff88080d8e8000 ti: ffff88040dd88000 task.ti: ffff88040dd88000 [ 0.167743] RIP: 0010:[\u0026lt;ffffffff810b6fda\u0026gt;] [\u0026lt;ffffffff810b6fda\u0026gt;] task_tick_fair+0xea/0x9e0 [ 0.167744] RSP: 0018:ffff88040fc03dc0 EFLAGS: 00010046 [ 0.167745] RAX: 00000000005b8d7e RBX: ffff88040fc16c80 RCX: 00000000010e1c7a [ 0.167746] RDX: 0000000000000000 RSI: ffff88040fc16e48 RDI: 00000000005b8d80 [ 0.167747] RBP: ffff88040fc03e30 R08: ffff88040fc16d00 R09: 0000000000000001 [ 0.167747] R10: 0000000000000000 R11: ffff88040fc16c80 R12: ffff88040fc16d00 [ 0.167748] R13: 00000000000003cc R14: ffff88080d8e8080 R15: 00000000051be773 [ 0.167750] FS: 0000000000000000(0000) GS:ffff88040fc00000(0000) knlGS:0000000000000000 [ 0.167750] CS: 0010 DS: 0000 ES: 0000 CR0: 0000000080050033 [ 0.167751] CR2: 0000000000000050 CR3: 0000000001c06000 CR4: 00000000000406f0 [ 0.167752] Stack: [ 0.167754] ffff88080d8e8000 ffff88040fc16c80 ffff88040fc16c80 0000000000000000 [ 0.167756] 0000000000000400 0000000000000001 000000000558e4e3 0000000000000000 [ 0.167757] ffffffff810ac372 ffff88040fc16c80 0000000000000000 0000000000016c80 [ 0.167757] Call Trace: [ 0.167762] \u0026lt;IRQ\u0026gt; [ 0.167763] [\u0026lt;ffffffff810ac372\u0026gt;] ? sched_clock_cpu+0x72/0xa0 [ 0.167765] [\u0026lt;ffffffff810a81dc\u0026gt;] scheduler_tick+0x5c/0xd0 [ 0.167767] [\u0026lt;ffffffff810eab71\u0026gt;] update_process_times+0x51/0x60 [ 0.167769] [\u0026lt;ffffffff810f7c9b\u0026gt;] tick_periodic+0x2b/0x80 [ 0.167771] [\u0026lt;ffffffff810f7d15\u0026gt;] tick_handle_periodic+0x25/0x70 [ 0.167774] [\u0026lt;ffffffff810306f5\u0026gt;] timer_interrupt+0x15/0x20 [ 0.167776] [\u0026lt;ffffffff810d7fc4\u0026gt;] handle_irq_event_percpu+0x44/0x1c0 [ 0.167778] [\u0026lt;ffffffff810d817e\u0026gt;] handle_irq_event+0x3e/0x60 [ 0.167779] [\u0026lt;ffffffff810db4d1\u0026gt;] handle_level_irq+0x91/0x110 [ 0.167781] [\u0026lt;ffffffff8103008d\u0026gt;] handle_irq+0x1d/0x30 [ 0.167785] [\u0026lt;ffffffff817df7fd\u0026gt;] do_IRQ+0x4d/0xd0 [ 0.167787] [\u0026lt;ffffffff817dd902\u0026gt;] common_interrupt+0x82/0x82 [ 0.167789] \u0026lt;EOI\u0026gt; [ 0.167789] [\u0026lt;ffffffff810d665d\u0026gt;] ? console_unlock+0x4ad/0x550 [ 0.167791] [\u0026lt;ffffffff810d6925\u0026gt;] vprintk_emit+0x225/0x480 [ 0.167792] [\u0026lt;ffffffff810d6cd9\u0026gt;] vprintk_default+0x29/0x40 [ 0.167794] [\u0026lt;ffffffff81184d59\u0026gt;] printk+0x4d/0x4f [ 0.167798] [\u0026lt;ffffffff81d6ef43\u0026gt;] smp_store_boot_cpu_info+0xf7/0x19e [ 0.167800] [\u0026lt;ffffffff81d6f049\u0026gt;] native_smp_prepare_cpus+0x5f/0x3d8 [ 0.167802] [\u0026lt;ffffffff81d59204\u0026gt;] kernel_init_freeable+0xb5/0x21b [ 0.167805] [\u0026lt;ffffffff817d0a5e\u0026gt;] kernel_init+0xe/0x110 [ 0.167806] [\u0026lt;ffffffff817dd1e2\u0026gt;] ret_from_fork+0x22/0x40 [ 0.167808] [\u0026lt;ffffffff817d0a50\u0026gt;] ? rest_init+0x80/0x80 [ 0.167824] Code: 38 0f 84 d7 06 00 00 8b 0d 90 5e b9 00 48 39 ca 72 28 49 8b 4c 24 40 48 8d 51 f0 48 85 c9 b9 00 00 00 00 48 0f 44 d1 49 8b 4e 50 \u0026lt;48\u0026gt; 2b 4a 50 78 09 48 39 c8 0f 82 55 06 00 00 4d 8b b6 50 01 00 [ 0.167826] RIP [\u0026lt;ffffffff810b6fda\u0026gt;] task_tick_fair+0xea/0x9e0 [ 0.167826] RSP \u0026lt;ffff88040fc03dc0\u0026gt; [ 0.167827] CR2: 0000000000000050 [ 0.167832] ---[ end trace 0cf8749a36857b7f ]--- [ 0.167833] Kernel panic - not syncing: Fatal exception in interrupt [ 0.477095] ---[ end Kernel panic - not syncing: Fatal exception in interrupt 여기서는 X86 계열의 명령어가 보이므로 IP(Instruction Pointer)에 저장된 주소를 이용해야 한다. IP가 나타내고 있는 주소값을 이용하여 아래와 같이 명령어를 이용하면 코드 레벨에서 어느 위치를 실행하고 있었는지를 단번에 알아낼 수 있다.\naddr2line -e vmlinux ffffffff810b6fda /home/xpenguin/groupamp/linux-4.6.4/kernel/sched/fair.c:3705 그렇다면, vmlinux 파일이 무엇이길래 이러한 디버깅이 가능한 것일까? 여기에 대해 간단하게 명시된 스택오버플로우 페이지가 있었다. 출처로써 명시하기에는 민망하지만, vmlinux 관련 포맷에 대해 간략하게 설명되어 있기에 출처로써 작성하였다.\nvmlinux : Linux kernel in an statically linked executable file format. (raw vmlinux 파일의 경우 디버깅 목적으로 매우 유용하게 사용될 수 있음)\nvmlinux.bin: vmlinux의 bootable raw binary 파일 포맷 버전으로, vmlinux 파일을 이용하여 objcopy -O binary vmlinux vmlinux.bin 명령어를 통해 생성해낼 수 있다. 이 파일에는 모든 심볼 정보와 재배치 정보가 삭제되어 있다.\nvmlinuz: vmlinux의 zlib을 이용해 압축된 형태\nzImage: small kernel(compressed, below 512KB)의 오래된 버전\nbzImage: \u0026lsquo;big zImage\u0026rsquo; 의미로서 bzip2와는 무관하며, (compressed, over 512KB) zImage에 비해 용량이 큰 버전이다.\n출처 https://unix.stackexchange.com/questions/5518/what-is-the-difference-between-the-following-kernel-makefile-terms-vmlinux-vml http://egloos.zum.com/holypsycho/v/3548805 ", 
        "url": "\/\/localhost:1313\/posts\/vmlinux\/"
    },
    
    "\/\/localhost:1313\/tags\/page-table\/": {
        "title": "Page Table",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/page-table\/"
    },
    
    "\/\/localhost:1313\/tags\/va_bits\/": {
        "title": "Va_bits",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/va_bits\/"
    },
    
    "\/\/localhost:1313\/tags\/virtual-address\/": {
        "title": "Virtual Address",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/virtual-address\/"
    },
    
    "\/\/localhost:1313\/posts\/%EA%B0%80%EC%83%81%EC%A3%BC%EC%86%8C%EC%99%80-%ED%8E%98%EC%9D%B4%EC%A7%80-%ED%85%8C%EC%9D%B4%EB%B8%94-%ED%81%AC%EA%B8%B0\/": {
        "title": "가상주소와 페이지 테이블 크기",
        "tags": ["page table","va_bits","virtual address",],
        "content": "각 태스트(유저 레벨에서 프로세스라고 일컫는)에는 각각의 가상주소를 갖는다. 사용되는 가상주소는 메모리 상에서 MMU에 의해 물리 메모리로 접근이 가능한데 이 때 MMU에 \u0026lsquo;어떻게 가상 주소를 맵핑할 것인지\u0026rsquo;에 대한 정보를 전달해줘야 한다. 이러한 정보는 \u0026lsquo;페이지 테이블\u0026rsquo;이라는 것으로 표현되어 그 물리주소를 MMU에서 참고하는 레지스터 정보에 저장하게 된다. 포스팅에서 기준으로 삼은 아키텍처는 ARM64이다. ARM64 커널에서는 사용 가능한 페이지의 크기로서 4K, 16K, 64K 등을 사용하고 있고 가상주소 비트 수(VA_BITS)로서 36, 39, 42, 47, 48비트 등을 사용한다. 여기서 잠깐 짚고 넘어가자면 커널에서 가상주소를 전부 사용하지 않는다. 예를 들어, 64비트 환경에서는 주소 체계가 64비트 기반이므로 가상주소의 경우에도 64비트 0x0000000000000000 ~ 0xffffffffffffffff 를 모두 사용할 것 같지만 실제로는 그렇지 않다. 페이지 단위로 관리되고 있는 메모리 체계에서 커널은 VA_BITS 상수를 통해 전체 64비트 중에서 실제 가상주소로 사용하기 위한 비트 개수를 정의한다.\n예를 들어, VA_BITS = 36의 경우 가상주소의 범위는 0xffff 0000 0000 0000 0000 ~ 0xffff ffff ffff ffff 가 된다. 즉, 페이지 테이블을 이용하여 2^36 = 64G 의 범위를 아우를 수 있으면 된다. 그런데 64G 가상주소를 위해 모든 페이지 엔트리를 나타내기에는 페이지 테이블의 크기가 커지기 때문에 페이지 테이블을 여러 레벨에 걸쳐 나타내게 된다. 다시 말해, 실제 36비트 중 페이지 엔트리로 사용할 비트를 제외한 나머지 (VA_BITS - log(PAGE_TABLE_ENTRY 크기) 비트를 이용해 64G 범위를 나타낼 수 있는 페이지 엔트리를 확보한다.\n앞서 설명했던 VA_BITS = 36 일 때의 경우를 살펴보자.\n가상주소를 위한 36비트에서 pgd(11bits), 16K(14bits)를 제외한 나머지 11비트를 어떻게 이용하느냐에 따라, 64G의 범위를 다룰 수 있는지 여부를 판단할 수 있다. 이렇게 가상 주소 비트는 페이지 테이블의 레벨이 많아질수록, 테이블의 엔트리 수가 많아질수록 가상주소를 맵핑하기 위한 작업이 까다로워지고 더 많은 사이클을 필요로 하게 된다. 이 때문에 나온 개념이 TLB이며 적절한 캐시를 통해 오버헤드가 많아지지 않도록 처리하는 것이 중요하다.\n최근에는 4-level paging에서 5-level paging으로 변화하고 있는 추세(https://lwn.net/Articles/717293/ )이다. 레벨 4를 가지는 페이징 구조가 가지는 최대 메모리 크기 128 PiB를 넘어가기 때문에 가상 주소로 사용하는 VA_BITS를 늘려 지원하는 가상 메모리의 크기를 늘리기 위한 패치가 나온 상태이다.\n", 
        "url": "\/\/localhost:1313\/posts\/%EA%B0%80%EC%83%81%EC%A3%BC%EC%86%8C%EC%99%80-%ED%8E%98%EC%9D%B4%EC%A7%80-%ED%85%8C%EC%9D%B4%EB%B8%94-%ED%81%AC%EA%B8%B0\/"
    },
    
    "\/\/localhost:1313\/categories\/c\/": {
        "title": "C",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/categories\/c\/"
    },
    
    "\/\/localhost:1313\/posts\/c%EC%96%B8%EC%96%B4%EC%97%90%EC%84%9C%EC%9D%98-type-check\/": {
        "title": "C언어에서의 type-check",
        "tags": ["type check",],
        "content": "개요 리눅스 커널을 살펴보다 보면 독특한 형태의 매크로 또는 타입 정의를 통해 타입 체크를 하는 것을 알 수 있다. 이 페이지에서는 앞으로 커널 분석 중에 자주 보게될 타입 체크에 대해 정리하고 내용이 추가될 때마다 페이지를 업데이트 하도록 한다.\n페이지 테이블에서의 타입 체크 ARM64 커널 소스에 있는 MMU 코드를 살펴보니 아래와 같이 특이하게 작성되어 있는 부분을 찾을 수 있었다.\ntypedef u64 pteval_t; typedef u64 pmdval_t; typedef u64 pudval_t; typedef u64 pgdval_t; ... /* * These are used to make use of C type-checking.. */ typedef struct { pteval_t pte; } pte_t; #define pte_val(x)\t((x).pte) #define __pte(x)\t((pte_t) { (x) } ) ... typedef struct { pgdval_t pgd; } pgd_t; #define pgd_val(x)\t((x).pgd) #define __pgd(x)\t((pgd_t) { (x) } ) 왜 enum을 사용하지 않고 struct에 멤버변수를 이용해서 사용하는 걸까? 하는 의문에 간단히 답을 찾을 수 있었다. enum의 경우 정수형 값과 호환되기에 컴파일러의 type check 루틴에 강제할 수 없다. 컴파일 타임에서 타입 체크를 하도록 강제할 수 있지만 syntax 상으로 강제하기 위해서 위와 같이 한 개의 멤버를 가지는 구조체 타입을 이용하고 매크로를 정의하여 안전하게 타입 체크를 강제할 수 있도록 구현하는 것을 볼 수 있다.\nprimitive type 커널 내의 minmax.h 파일을 살펴보면 아래와 같이 정의된 매크로가 있다. 단순하게 인자로 전달된 변수들의 타입 포인터로 캐스팅한 후 각각의 포인터 변수 크기를 비교한다. 이는 런타임에 인자로 전달된 x, y를 비교하기 위함이 아니라 컴파일 타임에 에러를 발생시키기 위한 용도이며 !! 를 통해서 두 개 변수의 크기를 비교하여 안전하게 boolean 형태로 변환하는 것도 확인할 수 있다.\n#define __typecheck(x, y) \\ (!!(sizeof((typeof(x) *)1 == (typeof(y) *)1))) ", 
        "url": "\/\/localhost:1313\/posts\/c%EC%96%B8%EC%96%B4%EC%97%90%EC%84%9C%EC%9D%98-type-check\/"
    },
    
    "\/\/localhost:1313\/tags\/type-check\/": {
        "title": "Type Check",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/type-check\/"
    },
    
    "\/\/localhost:1313\/tags\/benchmark\/": {
        "title": "Benchmark",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/benchmark\/"
    },
    
    "\/\/localhost:1313\/tags\/dmips\/": {
        "title": "Dmips",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/dmips\/"
    },
    
    "\/\/localhost:1313\/posts\/dmips-dhrystone-million-instructions-per-second\/": {
        "title": "DMIPS (Dhrystone Million Instructions Per Second)",
        "tags": ["dmips","benchmark",],
        "content": "개요 프로세서의 성능을 나타내는 지표로서 DMIPS라는 것을 사용하게 되었다. 벤치마크로 MIPS만 알고 있었는데 실제로 업무에서 사용했던 것은 DMIPS라는 것이어서 이번에 확실하게 정리하고 가고자 한다.\nDMIPS는 드라이스톤(Dhrystone) 벤치마크 테스트의 결과를 정수화해서 이를 하중한 값으로 비교하는 방법으로 프로세서 성능 비교에 이용하는 지표이다. 예를 들어, ARM 32bit Cortex-M3 CPU - 72 MHz maximum frequency 모델의 경우 1.25 DMIS/MHz 인데 여기에 CPU 사용량을 이용하면 해당 프로세서가 가지는 Full DMIPS (72MHz 일 때의 DMIPS) 대비 측정하고자 하는 프로세스에 대한 DMIPS 지표를 얻을 수 있다.\n예를 들어, A 프로세스의 평균 CPU 사용량이 10% 라고 가정하면, 위의 Cortex-M3 의 Full DMIPS는 72 * 1.25 = 90 DMIPS 이고, DMIPS * 0.1 = 9 DMIPS 라는 값을 산출해낼 수 있다.\n링크 http://blog.daum.net/trts1004/12109217 ", 
        "url": "\/\/localhost:1313\/posts\/dmips-dhrystone-million-instructions-per-second\/"
    },
    
    "\/\/localhost:1313\/tags\/futex\/": {
        "title": "Futex",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/futex\/"
    },
    
    "\/\/localhost:1313\/posts\/futex\/": {
        "title": "Futex",
        "tags": ["futex",],
        "content": "Futex (Fast Userspace Mutexes) futex는 전통적인 UNIX 커널에서 사용되고 있는 sleep/wakup과 매우 비슷한 동기 기구를 userland에 대해 제공한다. 주로 NPTL(Native POSIX Thread Library) 등의 라이브러리의 구현에 사용되기 때문에 애플리케이션으로부터 직접 이용하는 경우는 별로 없다고 생각되지만 POSIX Thread는 Java 스레드의 구현 등에도 이용되고 있어 동기 처리를 많이 이용하는 애플리케이션에는 이익이 있을 것이다. futex 시스템 콜의 주된 기능은 FUTEX_WAIT와 FUTEX_WAKE이다.\npthread_mutex_lock 아토믹 명령을 사용하여 lock을 시도 1에서 lock이 성공되면 종료 FUTEX_WAIT를 사용하여 pthread_mutex_t의 주소 상에서 슬립한다 시동 후, 1로 돌아간다 pthread_mutex_unlock 아토믹 명령을 사용해 unlock 처리를 실시 1의 결과, 슬립하고 있는 스레드가 있다면 FUTEX_WAKE를 사용하여 wakeup한다. 참고 https://miruel.tistory.com/entry/FutexSpinlock-%EC%86%8C%EA%B0%9C%EC%99%80-%EB%AA%B0%EB%9E%90%EB%8D%98-%EC%82%AC%EC%8B%A4 ", 
        "url": "\/\/localhost:1313\/posts\/futex\/"
    },
    
    "\/\/localhost:1313\/posts\/do-while\/": {
        "title": "Do While",
        "tags": ["do_while",],
        "content": "개요 커널 매크로에 do { .. } while(0) 구문을 사용하는 것을 보게 되었다. 사내 코드에서도 이러한 구문들이 많이 보였는데 처음에는 의미없이 이상하게 짜여진 코드라고 생각했다. 그런데 그런 구문에도 의미가 있었다. https://kernelnewbies.org/FAQ/DoWhile0 링크를 보면 이러한 구문을 만든 이유가 기술되어 있다. 이러한 구문에 대한 이유는 아래와 같다.\n빈 구문(empty statement)는 컴파일러가 경고를 낸다. 지역 변수를 선언할 수 있는 구역을 만들어준다. 조건문을 포함한 코드에서 복잡한 형태의 매크로를 사용할 수 있도록 해준다. 조건문을 포함한 코드에서의 매크로 사용 #define FOO(x) \\ printf(\u0026#34;arg is %s\\n\u0026#34;, x); \\ do_something_useful(x); 이 때 위처럼 정의한 매크로를 조건문과 함께 사용하게 된다면 아래와 같이 사용하게 된다.\nif (blah == 2) { FOO(blah); } 그리고 이 구문에 매크로가 적용된 것을 살펴보면,\nif (blah == 2) printf(\u0026#34;arg is %s\\n\u0026#34;, blah); do_something_useful(blah);; 위의 코드처럼 적용될 것이다. 이 때 문제가 되는 것은 do_something_useful(blah);가 조건에 관계없이 수행된다는 점이다. 이러한 매크로가 조건문에서 싱글라인 구문으로 사용된다면 문제가 될 수 있기에, do { \u0026hellip; } while(0) 을 사용하여 이러한 문제를 방지한다.\nif (blah == 2) do { printf(\u0026#34;arg is %s\\n\u0026#34;, blah); do_something_useful(blah); } while(0); 아래와 같이 일반적인 블록 구문을 사용한다고 가정했을 때, 특정한 경우에 위 코드는 동작하지 않는다.\n#define exch(x,y) { int tmp; tmp=x; x=y; y=tmp; } if (x \u0026gt; y) exch(x,y); else do_something(); 이 때, 매크로를 적용하면 아래와 같이 적용되어 버린다.\nif (x \u0026gt; y) { int tmp; tmp = x; x = y; y = tmp; } ; // 빈 구문 else do_something(); if문 블록 다음에 나오는 세미콜론으로 인해 \u0026ldquo;parse error before else\u0026rdquo; 문제가 발생하게 된다. 이 때 do {\u0026hellip;} while(0) 구문을 이용하여 매크로를 정의하면 아래와 같이 관련 에러를 피할 수 있다.\nif (x \u0026gt; y) do { int tmp; tmp = x; x = y; y = tmp; } while(0); else do_something(); 대체 구문 gcc에서 이 do-while-0 구문을 대체할 수 있는 구문 표현을 추가했다. 아래와 같은 이러한 표현은 언급한 모든 이점을 갖는 동시에 가독성도 보장된다.\n#define FOO(arg) ({ typeof(arg) lcl; lcl = bar(arg); lcl; }) ", 
        "url": "\/\/localhost:1313\/posts\/do-while\/"
    },
    
    "\/\/localhost:1313\/tags\/do_while\/": {
        "title": "Do_while",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/do_while\/"
    },
    
    "\/\/localhost:1313\/tags\/bsd\/": {
        "title": "Bsd",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/bsd\/"
    },
    
    "\/\/localhost:1313\/posts\/bsd-%EB%B2%84%EC%A0%84-linked-list\/": {
        "title": "BSD 버전 Linked List",
        "tags": ["queue","bsd","list",],
        "content": "개요 사내에서 소스 파일에 대한 라이센스를 정리하기 시작하면서 기존 애플리케이션에서 리눅스의 pipe 를 이용하여 구현된 메세지 큐를 연결 리스트로 재작성하는 작업을 맡게 되었다. 처음에는 리눅스의 커널에서 제공하는 list.h 를 사용하지 못해서 연결 리스트를 학부시절에 사용하던 방식으로 직접 구현하고자 하였다. 하지만 조금 더 찾아보니 BSD 버전의 연결 리스트가 \u0026lt;sys/queue.h\u0026gt; 의 형태로 존재하고 있었고 현재 FreeBSD에 포함되어 있는 queue.h 와는 다르지만 오래 전 공유하던 레거시 코드로서 여전히 리눅스 커널 내에 BSD 커널 라이브러리를 간직하고 있었다. 라이센스에 전혀 문제가 되지 않을 뿐만 아니라 필요한 메시지 큐를 구현하기 위한 매크로가 알기 쉽게 정의되어 있어 작성하는데에는 크게 어렵지 않았다. 대신, 불필요하게 잘못된 메모리 접근으로 인한 코드를 디버깅하는데 시간이 많이 걸렸다.\nqueue.h 작업에 사용했던 queue.h 파일(https://github.com/freebsd/freebsd/blob/master/sys/sys/queue.h 참고)에는 LIST와 TAILQ, CIRCLEQ가 구현되어 있었다. 링크는 최신버전의 라이브러리라 Circular Queue가 사라져있을 것이다. 리눅스의 list.h와 마찬가지로 BSD의 queue.h도 리스트를 사용하기 위해 재미있는 방법을 사용한다. 먼저 man-page에 기술되어 있는 예시를 시작으로 하나씩 살펴보자.\nTAILQ_HEAD(tailhead, entry) head = TAILQ_HEAD_INITIALIZER(head); struct tailhead *headp;\t/*\tTail queue head. */ struct entry { ... TAILQ_ENTRY(entry)\tentries; /*\tTail queue. */ ... } *n1, *n2, *n3, *np; TAILQ_INIT(\u0026amp;head);\t/*\tInitialize the queue. */ n1\t= malloc(sizeof(struct entry));\t/*\tInsert at the head. */ TAILQ_INSERT_HEAD(\u0026amp;head, n1, entries); n1\t= malloc(sizeof(struct entry));\t/*\tInsert at the tail. */ TAILQ_INSERT_TAIL(\u0026amp;head, n1, entries); n2\t= malloc(sizeof(struct entry));\t/*\tInsert after. */ TAILQ_INSERT_AFTER(\u0026amp;head, n1, n2, entries); n3\t= malloc(sizeof(struct entry));\t/*\tInsert before. */ TAILQ_INSERT_BEFORE(n2, n3, entries); TAILQ_REMOVE(\u0026amp;head, n2, entries);\t/*\tDeletion. */ free(n2); /*\tForward\ttraversal. */ TAILQ_FOREACH(np, \u0026amp;head, entries) np-\u0026gt; ... 먼저, TAILQ_HEAD부터 살펴보면 매크로를 통해 인자로 넘긴 이름으로 구조체를 하나 설정하는 것을 알 수 있다. 예를 들어 아래와 같이 정의된 자료형을 TAILQ 형태로 연결하고 싶다면,\nstruct message { int idx; TAILQ_ENTRY(message) entries; }; TAILQ_HEAD(msg_head, message) head; // struct msg_head head 와 같다. #define\tTAILQ_ENTRY(type)\t\\ struct {\t\\ struct type *tqe_next;\t/* next element */\t\\ struct type **tqe_prev;\t/* address of previous next element */\t\\ TRACEBUF\t\\ } 위의 코드처럼 정의하여 사용할 수 있다. 자료구조 안에 TAILQ_ENTRY를 사용함으로써 링크 객체를 포함하는 방식으로 구현한다. 위 예제에서, 연결 리스트는 struct msg_head* head 를 통해 접근할 수 있으며, head에 연결되는 노드들의 실제 데이터 struct message 자체는 *head가 갖는 *tqh_first, **tqh_last를 통해 얻을 수 있다. 위에서 TAILQ_HEAD 매크로를 통해 얻은 구조체의 구조는 아래와 같다.\n#define\tTAILQ_CLASS_HEAD(name, type)\t\\ struct name {\t\\ class type *tqh_first;\t/* first element */\t\\ class type **tqh_last;\t/* addr of last next element */\t\\ TRACEBUF\t\\ } 전체적인 연결을 다이어그램으로 나타내면 아래와 같다. ", 
        "url": "\/\/localhost:1313\/posts\/bsd-%EB%B2%84%EC%A0%84-linked-list\/"
    },
    
    "\/\/localhost:1313\/tags\/list\/": {
        "title": "List",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/list\/"
    },
    
    "\/\/localhost:1313\/tags\/queue\/": {
        "title": "Queue",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/queue\/"
    },
    
    "\/\/localhost:1313\/tags\/array_size\/": {
        "title": "ARRAY_SIZE",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/array_size\/"
    },
    
    "\/\/localhost:1313\/posts\/c-array_size-%EB%A7%A4%ED%81%AC%EB%A1%9C%EC%99%80-%ED%8F%AC%EC%9D%B8%ED%84%B0-%EA%B8%B0%EC%B4%88\/": {
        "title": "C ARRAY_SIZE 매크로와 포인터 기초",
        "tags": ["ARRAY_SIZE",],
        "content": "개요 칩 검증 코드를 수정하기 위해 간단한 루틴을 작성하고 있던 도중 커널에서 제공하는 ARRAY_SIZE 매크로가 의도한대로 동작하지 않아 살펴보기 시작했다. 그러던 도중 https://kldp.org/node/34268 과 같은 원인이라는 것을 알게되고 C 기초를 또 다시 한번 공부하게 되었다.\n커널에서 사용하는 ARRAY_SIZE는 아래와 같이 구현되어 있다.\n#define ARRAY_SIZE(x) (sizeof(x)/sizeof(x[0])) // examples for (idx = 0; idx \u0026lt; ARRAY_SIZE(pArr); idx++) { // do something }) 본래 의도한대로라면, 배열의 크기만큼 for 구문을 반복해서 수행해야 하지만, pArr 자체가 함수의 인자로써 전달된 것이라면 얘기가 달라진다. 함수의 인자, 즉 포인터의 사이즈를 받게 되므로 ARRAY_SIZE(x)는 포인터 변수 자체의 크기를 첫 번째 요소의 크기로 나눈 것이 된다. 설명보다 직접 코드를 통해 증명해보자.\n예제 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #define ARRAY_SIZE(x) (sizeof(x)/sizeof(x[0])) void tc1(void) { int arr[] = {1, 2, 3}; printf(\u0026#34;ARRAY_SIZE = %ld\\n\u0026#34;, ARRAY_SIZE(arr)); } void tc2(int *pArr) { printf(\u0026#34;ARRAY_SIZE = %ld\\n\u0026#34;, ARRAY_SIZE(pArr)); printf(\u0026#34;sizeof(x) = %d, sizeof(x[0]) = %d\\n\u0026#34;, sizeof(pArr), sizeof(pArr[0])); } void tc3(char *pArr) { printf(\u0026#34;ARRAY_SIZE = %ld\\n\u0026#34;, ARRAY_SIZE(pArr)); printf(\u0026#34;sizeof(x) = %d, sizeof(x[0]) = %d\\n\u0026#34;, sizeof(pArr), sizeof(pArr[0])); } void tc4(unsigned long *pArr) { printf(\u0026#34;ARRAY_SIZE = %ld\\n\u0026#34;, ARRAY_SIZE(pArr)); printf(\u0026#34;sizeof(x) = %d, sizeof(x[0]) = %d\\n\u0026#34;, sizeof(pArr), sizeof(pArr[0])); } int main(void) { int arrInt[] = {4, 5, 6, 7}; printf(\u0026#34;size of int = %d, size of long = %d\\n\u0026#34;, sizeof(int), sizeof(long)); printf(\u0026#34;size of pointer value = %d\\n\u0026#34;, sizeof(arrInt[0])); printf(\u0026#34;[Test case 1]\\n\u0026#34;); tc1(); printf(\u0026#34;[Test case 2]\\n\u0026#34;); tc2(arrInt); printf(\u0026#34;[Test case 3]\\n\u0026#34;); tc3(arrInt); printf(\u0026#34;[Test case 4]\\n\u0026#34;); tc4(arrInt); } 위 코드를 실행하면 아래와 같은 결과를 얻는다.\nsize of int = 4, size of long = 8 size of pointer value = 4 [Test case 1] ARRAY_SIZE = 3 [Test case 2] ARRAY_SIZE = 2 sizeof(x) = 8, sizeof(x[0]) = 4 [Test case 3] ARRAY_SIZE = 8 sizeof(x) = 8, sizeof(x[0]) = 1 [Test case 4] ARRAY_SIZE = 1 sizeof(x) = 8, sizeof(x[0]) = 8 함수의 파라미터로 정의되어 있는 포인터 변수들은 프로세서 아키텍처의 주소 크기만큼을 갖는다. 테스트는 64비트에서 이루어졌으므로 첫 번째 테스트케이스를 제외한 나머지에서 sizeof(x)는 모두 8(64비트)를 갖는다. 그리고 포인터의 타입에 따라 첫 번째 인자가 갖는 크기는 달라지게 되므로 결과값은 모두 달라지게 된다. 때문에 ARRAY_SIZE 매크로는 함수 내에 지역 변수로서 정의한 경우에는 사용할 수 있지만 함수로 넘겨서 사용하는 경우에는 사용이 불가능하다. 배열의 크기를 반드시 명시적으로 전달해줘야 전달받은 루틴에서 정상적으로 그 크기를 다룰 수 있다.\n기초적인 내용인데도 불구하고 막상 문제에 닥치니 제대로 알기가 어렵다. 아직 한참 멀었다.\n", 
        "url": "\/\/localhost:1313\/posts\/c-array_size-%EB%A7%A4%ED%81%AC%EB%A1%9C%EC%99%80-%ED%8F%AC%EC%9D%B8%ED%84%B0-%EA%B8%B0%EC%B4%88\/"
    },
    
    "\/\/localhost:1313\/tags\/goto\/": {
        "title": "Goto",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/goto\/"
    },
    
    "\/\/localhost:1313\/posts\/goto-statement\/": {
        "title": "goto statement",
        "tags": ["goto",],
        "content": "개요 학부 시절 c언어를 배울 때 \u0026lsquo;goto\u0026rsquo; 문을 사용하는 것을 터부시할 정도로 절대 사용하면 안되는 문법으로 배웠다. 그 이유는 자세하게 알려주지 않았지만 되도록이면 goto 를 사용하지 않고 분기나 객체를 이용하도록 코딩을 했었고 저학년에서 고학년으로 올라갈수록 c언어 대신 자바나 c++, c# 등을 배우면서 goto는 머릿속에서 잊혀져갔다.\n입사하고 나서 드라이버 코드를 보니 간간히 goto 문이 사용된 것들을 보고 이것이 과연 리팩토링을 해야하는 대상인가에 대해 잠시 생각해보았다. 만약 아래와 같이 코드가 진행될 경우, 반드시 나머지 코드를 분기문으로 처리해야할 필요가 있을까? 아니면 여기저기에 return 문을 사용해서 에러가 발생할 경우에 곧바로 함수를 빠져나가게 해야할까?\nfunc() { int ret = 0; ret = check_something(); if (ret \u0026lt; 0) { printk(KERN_ERR \u0026#34;ERROR!\\n\u0026#34;); } else { // remains.. } } func2() { int ret = 0; ret = check_something(); if (ret \u0026lt; 0) { printk(KERN_ERR \u0026#34;ERROR!\\n\u0026#34;); return -EINVAL; } // remains.. return ret; } Stack Overflow에서 관련 내용을 찾아보니 재미있는 코드를 발견했다. 코드 곳곳에서 보이던 do - while(0) 이 대체 왜 사용되는 걸까 하고 궁금했었는데, [[링크|https://stackoverflow.com/questions/243967/do-you-consider-this-technique-bad ]]를\u0026gt; 보니 이제서야 왜 사용했는지를 알 수 있었다. 정답은 없지만 최대한 간결하고 이해할 수 있도록 짤 수만 있다면 그러한 문법에 무슨 문제가 있을까? 무조건 사용을 하지 말아야 할 것이 아니라, 때에 따라서 적절하게 사용하고, 문제가 되지 않는다면 goto로 간결하게 짤 수 있을 것이다. 함수가 길지 않고 여기 저기에 사용되지 않는다면 goto 문을 사용할 수 있지만 그렇지 않을 경우에는 되도록이면 분기문을 통해 적절하게 처리해야할 것이다.\n", 
        "url": "\/\/localhost:1313\/posts\/goto-statement\/"
    },
    
    "\/\/localhost:1313\/categories\/android\/": {
        "title": "Android",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/categories\/android\/"
    },
    
    "\/\/localhost:1313\/tags\/partition\/": {
        "title": "Partition",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/partition\/"
    },
    
    "\/\/localhost:1313\/posts\/%EC%95%88%EB%93%9C%EB%A1%9C%EC%9D%B4%EB%93%9C-%ED%8C%8C%ED%8B%B0%EC%85%98\/": {
        "title": "안드로이드 파티션",
        "tags": ["partition",],
        "content": "개요 안드로이드 기본 내용이지만 파티션을 착각한 탓에 수정 사항을 반영하지 못해 시간을 날리고 말았다. 커널의 수정사항이라 부트 이미지만 적용하면 변경 사항이 적용될 거라 생각했지만 커널 모듈로 빌드되는 경우 vendor 이미지로 별도로 관리될 수도 있기 때문에 해당 이미지도 함께 추가해서 해당 파티션도 업데이트 해줘야 한다.\n안드로이드 파티션 안드로이드 레퍼런스 문서에 따르면 boot, system, vendor, radio를 다음과 같이 설명하고 있다.\nboot: boot 파티션에는 mkbootimg를 통해 결합된 커널 이미지와 RAM 디스크가 포함된다. 새 boot 파티션을 플래시하지 않고 직접 커널을 플래시 하기 위해 가상 파티션을 사용할 수 있다.\nsystem: 안드로이드 프레임워크\nrecovery: OTA 프로세스 중에 부팅된 복구 이미지를 저장한다. 기기가 A/B 업데이트를 지원하는 경우 복구는 별도의 이미지가 아닌 부팅 이미지에 포함된 RAM 디스크일 수 있다.\ncache: 임시 데이터를 저장하며 기기가 A/B 업데이트를 사용하는 경우에는 선택사항이다. 50 ~ 100MB 정도면 된다.\nmisc: 파티션 복구에 사용되며 4KB 이상이다.\nuserdata: 맞춤설정 데이터를 포함하여 사용자가 설치한 애플리케이션과 데이터가 포함된다.\nmetadata: 기기가 암호화되고 16MB 이상인 경우에만 사용한다.\nvendor: AOSP에 배포할 수 없는 바이너리들이 포함된다.\nradio: 무선 이미지가 포함된다.\ntos: Trusty OS 바이너리 이미지를 저장하며 기기에 Trusty가 있는 경우에만 사용한다.\n위에서 언급되었듯이 vendor 파티션에는 안드로이드 오픈소스 프로젝트에 배포할 수 없는 바이너리가 포함된다. 때문에 자사 드라이버 모듈들이나 바이너리 형태로 제공되는 드라이버의 경우 vendor 이미지로 관리되어야 한다. 만일 특정 벤더에서 사용하는 커널 드라이버나 바이너리가 업데이트 되는 경우 boot 이미지 뿐만 아니라 vendor 이미지도 함께 업데이트 하는 것이 확실한 방법이다.\n출처 https://source.android.com/devices/bootloader/partitions-images#images ", 
        "url": "\/\/localhost:1313\/posts\/%EC%95%88%EB%93%9C%EB%A1%9C%EC%9D%B4%EB%93%9C-%ED%8C%8C%ED%8B%B0%EC%85%98\/"
    },
    
    "\/\/localhost:1313\/tags\/poll\/": {
        "title": "Poll",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/poll\/"
    },
    
    "\/\/localhost:1313\/tags\/poll_wait\/": {
        "title": "Poll_wait",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/poll_wait\/"
    },
    
    "\/\/localhost:1313\/posts\/poll-%EA%B7%B8%EB%A6%AC%EA%B3%A0-poll_wait\/": {
        "title": "poll() 그리고 poll_wait()",
        "tags": ["poll","poll_wait",],
        "content": "개요 업무 중에 카메라 드라이버에 관련된 이슈를 보다가 poll()과 poll_wait()이 지속적으로 사용되는 것을 볼 수 있었다. 이슈에 관련된 커널이 다소 오래되었기에 살펴보아야 하는 드라이버 코드도 레거시에 가까웠지만 poll을 이용하여 디바이스 드라이버의 인터럽트를 처리하는 것으로 확인하여 관련 내용을 정리하고자 한다.\n리눅스 커널에서 제공하는 poll 함수에 대해서 원문으로 작성된 여러 출처들이 있었지만 아무래도 처음부터 원문을 읽고 이해하기에는 다소 어려움이 있었다. falinux에 작성된 문서를 기반으로 아래와 같이 개념적인 내용만 재정리하는 방식으로 포스팅을 작성하려 한다.\npoll()과 select() non-blocking I/O를 사용하는 유저 레벨 애플리케이션은 종종 poll() 과 select() 시스템 콜을 사용한다. poll, select는 기본적으로 같은 기능을 한다. 둘 다 blocking 없이 하나 이상의 파일들을 읽거나 쓸 수 있도록 프로세스가 결정할 수 있도록 한다. 이러한 특성으로 인해 블로킹 없이 input 또는 output 스트림을 사용해야 하는 애플리케이션에서 종종 사용된다. select와 poll은 같은 기능이지만 각각 BSD Unix, System V solution이라는 두 개 그룹에서 구현되면서 각기 다른 이름을 가지게 되었다.\n2.x 리눅스 커널에서는 select를 모델로 한 디바이스 모델 기반이었지만 2.1.23 버전으로 되면서 poll 시스템 콜이 새롭게 소개되었다.\nunsigned int (*poll) (struct file *, poll_table *); 동작 방식 (사용자) 애플리케이션에서는 poll 함수를 이용해 디바이스 장치 노드파일의 파일 연산으로 정의된 poll 인터페이스를 호출한다.\n아래의 예제 코드를 보면 직관적으로 동작 시나리오가 와닿을 것이다. 먼저, 애플리케이션의 코드를 먼저 살펴보자.\nstruct pollfd { int fd;\t// 파일 디스크립터 번호를 등록한다. short events;\t// 요구하는 이벤트 short revents;\t// 반환된 이벤트 } #include \u0026lt;sys/poll.h\u0026gt; int main( int argc, char **argv ) { int fd1,fd2; int\tretval; struct pollfd Events[2]; fd1 = open(\u0026#34;디바이스1\u0026#34;, O_RDWR | O_NOCTTY ); fd2 = open(\u0026#34;디바이스2\u0026#34;, O_RDWR | O_NOCTTY ); memset ( Events, 0 ,sizeof(Events) ); Events[0].fd = fd1; Events[0].events = POLLIN | POLLERR; Events[1].fd = fd2; Events[1].events = POLLOUT; while( 1 ) { retval = poll( (struct pollfd *)\u0026amp;Events, 2, 5000); if( retval \u0026lt; 0 ) printf(\u0026#34;poll err\\n\u0026#34;); else if( 0 == retval ) printf(\u0026#34;No event!!\\n\u0026#34;)l else { if( POLLERR \u0026amp; Events[0].revents ) printf(\u0026#34;장치 에러\\n\u0026#34;); else if( POLLIN \u0026amp; Events[0].revents ) { read(fd1, ~,~); } else if (POLLOUT \u0026amp; Events[1].revents ) { write(fd2, ~, ~); } } } close(fd1); close(fd2); } 위 예제는 출처에 작성된 예제 코드 그대로이다. 장치 노드를 open 시스템 콜을 통해 파일 디스크립터 형태로 받아놓고 해당 파일 디스크립터를 poll 함수에 넘겨 디바이스 드라이버에 정의되어 있는 poll 함수를 호출하는 방식이다. 이 때, 디바이스 드라이버에서 poll 요청을 어떻게 처리하는냐에 따라, 그리고 해당 디바이스에 대한 이벤트에 따라 poll의 반환값이 달라지게 된다.\n그렇다면, 디바이스 드라이버 내에서는 어떤 방식으로 구현되는지 아래 코드를 살펴보자.\nstruct file_operations kerneltimer_fops = { .owner = THIS_MODULE, .read = kerneltimer_read, .write = kerneltimer_write, .poll = kerneltimer_poll, .open = kerneltimer_open, .release = kerneltimer_release, }; DECLARE_WAIT_QUEUE_HEAD(WaitQueue_Read); unsigned int XXX_poll( struct file *file, poll_tablr *wait) { int mask = 0; poll_wait( file, \u0026amp;WaitQueue_Read, wait ); if( 쓰기 가능 ) mask |= (POLLIN | POLLRDNORM ); return mask; } 디바이스 드라이버는 읽기나 쓰기가 가능해지면 이에 따라 깨어나 디바이스 드라이버에 맞는 수행을 하게 되는데, 이러한 이벤트에 따라 동작 시나리오를 구현하기 위해 디바이스 드라이버 내에 poll 함수를 등록해두어야 한다.\npoll() 함수를 호출하는 애플리케이션에서 커널이 주기적으로 어떤 이벤트를 모니터링 할지를 디바이스 드라이버에 전달해주면, 디바이스 드라이버에서는 해당 이벤트가 발생할 때까지 기다리다가, ISR 등으로 인해 디바이스 드라이버의 poll_wait 이 풀리게 되면 poll() 호출로 sleep 상태에 있던 프로세스를 깨우게 된다.\n이러한 poll 시스템 콜이 필요한 이유는 디바이스 드라이버에서 입출력에 필요한 데이터가 준비될 때까지 프로세서가 대기하지 않도록 하기 위함이다.\n애플리케이션은 디바이스 드라이버에게 \u0026ldquo;A, B, C 사건이 일어날 때 알려줘. 그 때까지 좀 잘게\u0026rdquo; 라고 말하는 것이 poll() 이고, 디바이스 드라이버는 해당 이벤트가 일어날 때까지 기다리고 있다가 ISR에 의해 이벤트 조건이 충족되면 POLLOUT, POLLIN, POLLERR, POLLWRNORM, \u0026hellip; 과 같은 마스킹 정보를 통해 애플리케이션에게 디바이스 드라이버의 데이터가 준비되었음을 알리는 것이 매커니즘의 핵심이다.\n출처 http://forum.falinux.com/zbxe/index.php?document_srl=567919\u0026mid=lecture_tip ", 
        "url": "\/\/localhost:1313\/posts\/poll-%EA%B7%B8%EB%A6%AC%EA%B3%A0-poll_wait\/"
    },
    
    "\/\/localhost:1313\/tags\/micom\/": {
        "title": "Micom",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/micom\/"
    },
    
    "\/\/localhost:1313\/posts\/micom%EA%B3%BC-mpu\/": {
        "title": "MICOM과 MPU",
        "tags": ["micom","mpu",],
        "content": "개요 카메라 관련 업무로 ARM Cortex A7 Single 모델을 주로 사용하지만 A53, MICOM 과의 인터럽트 충돌 문제가 일어날 때가 있다. UART 부트 및 LVDS 채널 변경, 더 나아가 카메라 센서의 초기화까지 마이컴에서 담당하게 되는 시나리오가 있어 마이컴이 정확히 어떤 것인지 한번은 짚고 넘어가야 했다.\n마침 한 블로그에 마이컴에 대해 간략하게 정리가 되어 있고 연관된 토픽 별로 정리해놓은 페이지가 있어 필요한 부분만 간추려 본 페이지로 정리하기로 한다.\n본문 출처 내용 그대로 uC는 MicroController, uP는 MicroProcessor, \u0026ldquo;Adv\u0026quot;는 Advanced Topic의 줄임말로서 사용한다.\n[uC] 마이컴 vs. 마이크로 프로세서 마이컴은 마이크로컨트롤러(Microcontroller)를 줄여서 부르는 말이다. 마이컴은 소형 범용 컴퓨터라고 할 수 있다. 마이크로프로세서와 마이컴의 가장 큰 차이점은 마이크로프로세서는 주변 장치(메모리, I/O 등등)와 함께 하나의 시스템을 구성하도록 설계되어 있으나 마이컴은 앞서 언급한 것처럼 그 자체가 하나의 컴퓨터로서 동작할 수 있도록 설계되어 있다.\nPC의 성능/기능을 기대할 수는 없지만 프로그래밍이 가능한 롬이 있기 때문에 마이컴칩과 간단한 주변회로 만으로 특정 기능을 수행할 수 있다. 마이컴은 주변에서 흔하게 찾아볼 수 있는데 냉장고, 세탁기, TV, VCR, 마이크로웨이브 오븐 등 임베디드 시스템 환경에 적용되어 있다.\n[uC] 마이컴 주변회로 8비트 마이컴은 연산을 처리하는 ALU와 메모리 인터페이스 부분 그리고 주변회로로 이루어져 있다. 주변회로에는 인터럽트, 타이머, UART 등이 있다. 32비트 마이컴의 주변 기기도 8비트와 크게 다르지 않다.\n[uC] 타이머(Timer) 타이머는 흔히 알고 있는 스톱워치와 비슷한 기능을 가지고 있으나 처이가 있다. 스톱워지는 시간이 얼마나 흘렀는지를 측정하지만 타이머는 지정한 시간이 경과하면 알려주게 되어있다. 예를 들어 마이컴이 1/100초에 한번씩 키패드를 스캔해야 한다고 가정하면 마이컴은 키가 언제 눌러지는지 알 수 없으므로 항상 키패드를 일정 시간 간격으로 스캔(polling)해야 한다.\n타이머가 없다면 프로세서는 시간이 얼마나 흘렀는지 알기 위해 자기가 수행한 명령어를 카운트해야 하는데 이 일이 프로세서에게는 상당히 비효율적이기 때문에 인터럽트를 제공하는 타이머를 사용한다.\n[uC] Reset Vector 리셋 벡터는 마이컴에 리셋이 인가되었을 때 마이컴이 처음으로 실행되는 어드레스를 말한다. 마이컴에 전원이 인가되었을 때 마이컴이 어떻게 동작하는지 살펴보려면 리셋 벡터를 공부해야한다. 이 용어에서의 벡터는 물리시간에 배운 크기와 방향을 같는 의미의 벡터와는 무관하다.\n[uP] Register File 레지스터가 여러 개 모여있는 것을 레지스터 파일이라고 부른다.\n[uP] DMA 주변 기기에서 메모리로 데이터로 옮길 때는 CPU가 주변기기에서 데이터를 CPU로 가져온 다음에 메모리를 써야 한다. 즉, 주변기기에서 데이터를 읽어온 뒤 메모리로 보내는 작업을 프로세서가 해야하는데 이렇게 데이터를 옮기는 동안에는 CPU가 다른 작업을 할 수 없다. 때문에 DMA는 별도의 하드웨어에 이 작업을 맡기는 것이고 CPU는 주변장치로부터의 데이터를 메모리에 쓰기 위해 사이클을 낭비할 필요가 없어진다.\n[uP] Super Scalar 벡터(Vector) 프로세서의 반대말이 스칼라 프로세서이다. 스칼라 프로세서는 한번에 하나의 명령어만 실행하며 슈퍼스칼라는 벡터와 스칼라 중간 정도에 해당한다. 즉 한번에 여러 개의 명령어를 실행할 수 있다.\n[uP] Trap and Interrupt 프로그램 실행 중에 오버플로우나 언더플로우 같은 특이한 사항이 발생하는 경우 프로그램은 미리 설정해 놓은 특정 번지로 점프하여 그곳의 명령을 실행하게 되는데 이렇게 프로그램이 자동으로 특정 번지로 점프하여 그곳의 명령을 실행하는 것을 트랩이라 하고 해당 번지에 있는 핸들러를 트랩 핸들러라고 한다.\n이러한 트랩과 인터럽트의 주된 차이점은 트랩은 어떤 프로그램을 실행 시켰을 때 항상 고정된 위치에서 일어나나 (외부)인터럽트는 언제 발생할지 모른다는 점이다.\n출처 https://rakuraku.tistory.com/147 ", 
        "url": "\/\/localhost:1313\/posts\/micom%EA%B3%BC-mpu\/"
    },
    
    "\/\/localhost:1313\/tags\/mpu\/": {
        "title": "Mpu",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/mpu\/"
    },
    
    "\/\/localhost:1313\/tags\/interrupt\/": {
        "title": "Interrupt",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/interrupt\/"
    },
    
    "\/\/localhost:1313\/tags\/ioctl\/": {
        "title": "Ioctl",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/ioctl\/"
    },
    
    "\/\/localhost:1313\/posts\/ioctl%EA%B3%BC-%EC%9D%B8%ED%84%B0%EB%9F%BD%ED%8A%B8\/": {
        "title": "IOCTL과 인터럽트",
        "tags": ["ioctl","interrupt",],
        "content": "개요 디바이스 드라이버와 인터럽트 핸들러 간의 동기화 때문에 머리가 아팠다. 현재도 해결하지 못하고 있는 이슈가 있어 계속해서 찾아보고 있는 와중에 StackOverflow에서 재미있는 질문을 찾았다.\n[[https://stackoverflow.com/questions/60088342/does-context-switching-occurs-when-ioctl-is-issued-from-user-space-while-kernel]] 문제 자체는 커널 모듈 안에 있는 critical section에서 스핀락을 사용하지 않을 때 irq나 softirq를 비활성화하지 않고도 데드락에 빠지지 않고 정상적으로 동작이 가능한가에 대한 질문이다. 질문에 대한 답변부터 살펴보면,\nIRQ/SoftIRQ는 시스템 콜과 아무런 영향이 없다. 단지 인터럽트 컨텍스트 안에서 사용되는 데이터 구조들을 보호하기 위해 IRQ와 softIRQ를 비활성화하는 것 뿐이다.\nioctl 시스템 호출이 일어나면 user-mode에서의 컨텍스트는 kernel-mode 컨텍스트로 진입하게 된다. 하지만 커널 모드에서 스핀락을 holding 하고 있는 경우 컨텍스트 스위칭은 일어날 수 없는데, 이유는 스핀락 자체가 preemption을 막기 때문이다.\nstatic inline void __raw_spin_lock(raw_spinlock_t *lock) { preempt_disable(); spin_acquire(\u0026amp;lock-\u0026gt;dep_map, 0, 0, _RET_IP_); LOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock); } 락을 사용하는 경우에 대해서는 좀 더 살펴봐야 하겠지만, 인터럽트 컨텍스트와 공유하는 데이터 구조들을 사용하기 위해서는 spin_lock과 같은 락을 사용하고 스핀락으로 락을 갖고 있을 때 선점될 수 없다는 점을 기억하자. 다음 포스팅에서는 러셀이 정리한 커널에서 제공하는 락의 종류가 어떨 때 사용해야 하는지를 정리해보도록 하겠다.\n출처 https://stackoverflow.com/questions/60088342/does-context-switching-occurs-when-ioctl-is-issued-from-user-space-while-kernel ", 
        "url": "\/\/localhost:1313\/posts\/ioctl%EA%B3%BC-%EC%9D%B8%ED%84%B0%EB%9F%BD%ED%8A%B8\/"
    },
    
    "\/\/localhost:1313\/tags\/hyper-v\/": {
        "title": "Hyper-V",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/hyper-v\/"
    },
    
    "\/\/localhost:1313\/posts\/hyper-v-%EA%B0%80%EC%83%81-%ED%99%98%EA%B2%BD%EC%97%90%EC%84%9C-%EA%B3%A0%EC%A0%95-%EC%95%84%EC%9D%B4%ED%94%BC-%EC%A3%BC%EC%86%8C-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0\/": {
        "title": "Hyper-V 가상 환경에서 고정 아이피 주소 사용하기",
        "tags": ["hyper-v",],
        "content": "개요 윈도우즈에서 리눅스 환경을 이용하기 위해서는 docker 컨테이너를 이용하거나 hyper-v, vmware, virtualbox 등과 같은 가상머신을 이용해야 한다. 이번에는 hyper-v를 이용해 리눅스 환경을 구축하고 SSH를 통해 접속하여 필요한 작업을 하려 했는데 문제는 IP가 계속해서 동적으로 바뀌는 것이었다. 이를 해결하기 위해 네트워크 구성 방법과 간단한 가이드를 작성하고자 한다. 향후 Hyper-V 를 사용하면서 요구되는 시나리오가 추가되면 본 포스팅 문서를 수정하여 정리하도록 한다.\n가상머신 클라이언트에 static ip 할당하기 가상머신에 고정 아이피를 할당하기 위해서는 가상 스위치 장치를 이용해야 한다. 가상 스위치의 패킷을 실제 네트워크 어댑터(이더넷 또는 와이파이)와 공유하도록 하고 가상 스위치의 아이피를 가상 머신에서 사용하는 게이트웨이로 지정하여 호스트에서 SSH로 접속할 수 있는 환경을 구성한다.\n작업 \u0026gt; 가상 스위치 관리자 현재 Default Switch로 되어 있는 스위치가 내부 네트워크로 되어 있는지 확인한다. \u0026lsquo;내부 네트워크\u0026rsquo;로 선택되어 있는 경우라면 가상 스위치를 추가할 필요가 없지만 만약 선택되어 있다면 이 단계는 넘어가자. 스위치가 없는 경우 \u0026lsquo;새 가상 네트워크 스위치\u0026rsquo;를 선택하여 내부 타입의 가상 스위치를 하나 생성한다. 네트워크 설정 \u0026lsquo;제어판 - 네트워크 및 인터넷 - 네트워크 설정\u0026rsquo; 에서 내부 가상 스위치의 속성으로 들어가 고정아이피를 직접 할당한다. 아래는 직접 사용한 설정 정보이다. IP: 192.168.137.1 subnet mask: 255.255.255.0 스위치에 대한 네트워크 설정을 마쳤으면 이더넷 또는 와이파이 어댑터의 속성에서 공유 탭의 \u0026lsquo;인터넷 연결 공유\u0026rsquo;에 \u0026lsquo;다른 네트워크 사용자가 이 컴퓨터의 인터넷 연결을 통해 연결할 수 있도록 허용\u0026rsquo; 옵션을 활성화해준다.\n가상머신에서 네트워크 설정 이제 거의 끝났다. 가상머신에서 직접 아래와 같이 네트워크 설정을 해준다. 위에서 설정한 스위치 아이피를 gateway로 설정하고 원하는 고정아이피로 설정하면 끝이다. $ ifconfig eth0: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 inet 192.168.137.10 netmask 255.255.255.0 broadcast 192.168.137.255 inet6 fe80::87d1:e5b6:b588:1e48 prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; ether 00:15:5d:99:75:00 txqueuelen 1000 (Ethernet) RX packets 30118 bytes 13541072 (13.5 MB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 19934 bytes 6482177 (6.4 MB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 lo: flags=73\u0026lt;UP,LOOPBACK,RUNNING\u0026gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10\u0026lt;host\u0026gt; loop txqueuelen 1000 (Local Loopback) RX packets 44920 bytes 18330975 (18.3 MB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 44920 bytes 18330975 (18.3 MB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 $ ip route show default via 192.168.137.1 dev eth0 proto static metric 100 169.254.0.0/16 dev eth0 scope link metric 1000 192.168.137.0/24 dev eth0 proto kernel scope link src 192.168.137.10 metric 100 이제 호스트 윈도우즈에서 클라이언트로 SSH를 통해 접속할 수 있는 고정아이피가 완성되었다.\n출처 https://medium.com/@maxtortime_88708/hyper-v-%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC-%EC%84%A4%EC%A0%95%ED%95%98%EA%B8%B0-b459a7b0bd11 ", 
        "url": "\/\/localhost:1313\/posts\/hyper-v-%EA%B0%80%EC%83%81-%ED%99%98%EA%B2%BD%EC%97%90%EC%84%9C-%EA%B3%A0%EC%A0%95-%EC%95%84%EC%9D%B4%ED%94%BC-%EC%A3%BC%EC%86%8C-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0\/"
    },
    
    "\/\/localhost:1313\/categories\/virtualization\/": {
        "title": "Virtualization",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/categories\/virtualization\/"
    },
    
    "\/\/localhost:1313\/tags\/buffering\/": {
        "title": "Buffering",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/buffering\/"
    },
    
    "\/\/localhost:1313\/posts\/%EB%A6%AC%EB%88%85%EC%8A%A4%EC%97%90%EC%84%9C%EC%9D%98-%EB%B2%84%ED%8D%BC%EB%A7%81-%EB%B0%A9%EC%8B%9D\/": {
        "title": "리눅스에서의 버퍼링 방식",
        "tags": ["buffering",],
        "content": "리눅스에서의 버퍼링 정책 리눅스에서는 파일 입출력을 할 때 물리적인 파일에 조회 및 기록의 횟수를 최소화하여 성능을 높이기 위해 버퍼링 정책을 사용하고 있다.\n만약 write 호출로 데이터를 쓰기 명령을 전달하면 해당 파일 작업을 위한 버퍼에 기록을 해 두었다가 정책에 따라 특정 시점에 물리적인 파일에 기록을 수행한다. 리눅스에서 제공하는 버퍼링 정책에는 버퍼가 꽉 차면 물리적인 파일에 기록하는 Full Buffering과 꽉 차거나 개행문자가 오면 처리하는 Line Buffering, 버퍼를 사용하는 않는 Null Buffering 정책을 제공하고 있다.\n디폴트 버퍼링 정책은 Full Buffering이며, character 장치 파일에 대한 작업은 Line Buffering 정책을 사용한다. 그리고 오류를 출력하는 stderr 파일 시스템은 Null Buffering을 사용한다.\n아래의 예제를 살펴보자.\n#include \u0026lt;stdio.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; int main() { putchar(‘e’); sleep(1); fputc(‘H’, stderr); putchar(‘h’); sleep(1); fputc(‘e’, stderr); putchar(‘\\n’); sleep(1); fputc(‘l’, stderr); putchar(‘p’); sleep(1); fputc(‘l’, stderr); putchar(‘u’); sleep(1); fputc(‘o’, stderr); putchar(‘b’); sleep(1); fputc(‘!’, stderr); putchar(‘\\n’); sleep(1); fputc(‘\\n’, stderr); return 0; } 표준 출력의 경우 Line Buffering을 사용하기 때문에 putchar(\u0026rsquo;e\u0026rsquo;) 구문을 호출하면 stdout 버퍼에 문자 e를 기록하지만 아직 개행 문자를 기록한 것은 아니므로 콘솔 화면에 출력하지 않는다. 이 후 fputc(\u0026lsquo;H\u0026rsquo;, stderr); 구문에 의해 \u0026lsquo;H\u0026rsquo;를 표준 에러에 출력하지만 표준 에러는 Null Buffering 이므로 콘솔 화면에 \u0026lsquo;H\u0026rsquo; 문자를 바로 출력한다.\n출처 http://ehpub.co.kr/tag/line-buffering/ ", 
        "url": "\/\/localhost:1313\/posts\/%EB%A6%AC%EB%88%85%EC%8A%A4%EC%97%90%EC%84%9C%EC%9D%98-%EB%B2%84%ED%8D%BC%EB%A7%81-%EB%B0%A9%EC%8B%9D\/"
    },
    
    "\/\/localhost:1313\/posts\/ioctl%EC%97%90-%EC%A0%84%EB%8B%AC%EB%90%98%EB%8A%94-cmd-%EA%B4%80%EB%A0%A8-%EB%A7%A4%ED%81%AC%EB%A1%9C-%ED%95%A8%EC%88%98\/": {
        "title": "ioctl()에 전달되는 cmd 관련 매크로 함수",
        "tags": ["ioctl","macro",],
        "content": "드라이버 코드 내에서 ioctl commands 를 정의하기 위해 단순 integer를 사용하는 게 아니라 매크로를 통해 생성을 하고 있었다. 이에 관련 내용을 포스팅과 함께 정리하고자 한다.\n저수준 파일 입출력 함수인 ioctl()을 디바이스 파일에 적용시키면 디바이스 파일에 연결된 디바이스 드라이버의 file_operation 구조체의 ioctl 필드에 선언된 함수가 호출된다. ioctl 함수는 디바이스마다 고유하게 선언하여 사용하는데, 일반적으로 하드에워의 제어나 상태를 얻기 위해 사용한다.\ncmd 구성 디바이스 드라이버의 ioctl() 함수에 전달되는 매개변수 cmd는 응용 프로그램이 디바이스 드라이버에게 요구한 처리를 구별하기 위한 구별값이다. cmd에는 단순한 구별 숫자 이외에 처리에 도움을 주는 몇 가지 정보를 포함한 형태로 구성된다. cmd의 크기는 32비트로, 비트 구성은 다음과 같다.\n+---+----------------+------------+---------+ | 2 |\t14 |\t8 |\t8 |\t+---+----------------+------------+---------+ direction size type number 매크로 함수 cmd에는 여러 가지 필드가 있다. 리눅스 커널은 이런 필드 형식에 맞춰 cmd 상수값을 만드는 매크로 함수와 cmd 상수값에서 필요한 필드값을 추출하는 매크로 함수를 제공한다.\ncmd 명령을 만드는 매크로 함수 _IO : 부가적인 데이터가 없는 명령을 만드는 매크로 _IOR : 디바이스 드라이버에서 데이터를 읽어오기(R) 위한 명령을 만드는 매크로 _IOW : 디바이스 드라이버에서 데이터를 써넣기(W) 위한 명령을 만드는 매크로 _IOWR : 디바이스 드라이버에서 데이터를 읽고(R), 쓰기(W)를 수행하기 위한 명령을 만드는 매크로 이 매크로의 형태는 다음과 같은 형식의 값을 입력하도록 되어 있다.\n_IO(매직번호, 구분번호) _IOR(매직번호, 구분번호, 변수형) _IOW(매직번호, 구분번호, 변수형) _IOWR(매직번호, 구분번호, 변수형) cmd 명령을 해석하는 매크로 함수 _IOC_NR : 구분 번호 필드값을 읽는 매크로 _IOC_TYPE : 매직 번호 필드값을 읽는 매크로 _IOC_SIZE : 데이터의 크기 필드값을 읽는 매크로 _IOC_DIR : 읽기와 쓰기 속성 필드값을 읽는 매크로 출처 https://damduc.tistory.com/261 ", 
        "url": "\/\/localhost:1313\/posts\/ioctl%EC%97%90-%EC%A0%84%EB%8B%AC%EB%90%98%EB%8A%94-cmd-%EA%B4%80%EB%A0%A8-%EB%A7%A4%ED%81%AC%EB%A1%9C-%ED%95%A8%EC%88%98\/"
    },
    
    "\/\/localhost:1313\/tags\/macro\/": {
        "title": "Macro",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/macro\/"
    },
    
    "\/\/localhost:1313\/tags\/switch\/": {
        "title": "Switch",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/switch\/"
    },
    
    "\/\/localhost:1313\/posts\/switch-%EA%B5%AC%EB%AC%B8%EA%B3%BC-if-else-%EA%B5%AC%EB%AC%B8\/": {
        "title": "switch 구문과 if-else 구문",
        "tags": ["switch",],
        "content": "개요 실행 시간 단축을 위해 어떤 방법이 가능할지 고민하던 중 아래와 같은 case 구문을 보았다.\nint switch_example(unsigned int flag) { switch (flag) { case FLAG_A: // do A break; case FLAG_B: // do B break; case blabla: // blabla... break; default: printk(KERN_ERR \u0026#34;ERROR!\\n\u0026#34;); } return 0; } 함수 포인터 배열 사용하기 if-else 를 사용하지 않고 굳이 switch 문을 사용하는 이유가 있을까? 학부 시절, switch문은 되도록 지양하고 if-else를 사용해야 한다는 얘기를 들었던 기억이 났다. 하지만 if-else 구문을 사용하면 여러 개의 branch 가 생기기 때문에 처음 위 예제로 간단하게 작성한 코드를 아래와 같이 변환하려 했었다.\nint use_array_example (unsigned int flag) { void* handler[] = { handler_A, handler_B, handler_C }; handler[flag](); } 사용할 flag는 정해져 있고 함수 포인터를 담고 있는 배열을 사용한다면 불필요한 분기로 인한 성능 손실을 없애고 곧바로 호출할 수 있기 때문에 위와 같이 개선하려 했다. 하지만, 막상 switch 문에 대해 구글링 해보니 아래와 같은 답변을 찾을 수 있었다.\nswitch 문과 컴파일러 최적화 https://www.geeksforgeeks.org/switch-vs-else/ 내용에 따르면, switch 문을 사용할 경우 컴파일 시에 \u0026lsquo;jump table(lookup table)\u0026rsquo; 또는 hash list를 만들어 최적화를 시도하기 때문에 단순하게 if-else 를 사용할 때보다 실행 속도 측면에서 더 유리하다고 한다.\n분명 학부시절 때 알아야 했을 기본적인 내용이지만, 몇 년이 지나서도 이러한 기본적인 내용을 계속해서 살펴보게 되는 것 같다. 배우고 기억하려 해도 끝이 보이지 않는다.\n", 
        "url": "\/\/localhost:1313\/posts\/switch-%EA%B5%AC%EB%AC%B8%EA%B3%BC-if-else-%EA%B5%AC%EB%AC%B8\/"
    },
    
    "\/\/localhost:1313\/posts\/identity-mapping\/": {
        "title": "Identity Mapping",
        "tags": ["idmap",],
        "content": "ARM64 페이징을 공부하다보니 idmap (Identity Mapping)이라는 용어가 등장했다. 페이지 테이블이 완전하게 준비가 되지 않았을 때 임시로 사용하는 매핑 방법 중의 하나인데 오늘을 여기에 대해서 정리하고자 한다.\nidmap이 무엇이고 왜 필요한가? idmap 을 사용하는 코드는 어떤 것이 있는가? ARM 리눅스 커널에서는 어떻게 활용하고 있는가? idmap (Identity Mapping) 가상주소와 물리주소가 매핑되는 방식 중의 한 가지다. 리눅스 커널에서 사용하는 주소 매핑 방식을 아래와 같이 3가지로 구분할 수 있다.\nlinear 영역 가상주소 + offset = 물리주소 와 같이 주소 변환이 가능한 방식이다. ARM에서는 섹션에 매핑되는 커널 주소가 이 방식을 사용한다. 커널에서는 linear 영역에 대한 주소변환 함수로 virt_to_phys, phys_to_virt 등을 제공한다.\nnon-linear 영역 가상주소 → page table → 물리주소 등으로 페이지 테이블을 거쳐야만 변환이 가능한 주소 영역이다. 단순히 offset 계산만으로는 알 수 없는 경우이다. 일반적으로 페이지 매핑되는 모든 주소가 non-linear 영역이라고 보면 된다.\nidmap MMU를 활성화하는 코드를 실행해야 한다고 가정했을 때, 가상주소가 곧바로 물리주소로 바로 사용된다. 즉, 페이지 테이블이 준비되지 않은 경우 가상주소와 물리주소를 1:1로 매핑하여 사용하는데 이러한 방식을 idmap 이라 한다.\nidmap 사용 예 리눅스 커널 코드 중에서 arch/arm/ 하위에서 .pushsection .idmap.text,\u0026quot;ax\u0026quot; 등과 같은 문구가 나오는데 이러한 것들이 idmap 코드 영역에 포함되는 코드들이다.\n최초 부팅 시 MMU 활성화 시: MMU 활성화 전까지는 주소 변환이 불가능하므로 당연히 idmap 되어야 한다. suspend-resume 에서 resume 직후 MMU를 활성화 할 때: arch/arm/kernel/sleep.S - cpu_resume_mmu() soft cpu reset 시: reset 수행 전 MMU를 비활성화 ", 
        "url": "\/\/localhost:1313\/posts\/identity-mapping\/"
    },
    
    "\/\/localhost:1313\/tags\/idmap\/": {
        "title": "Idmap",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/idmap\/"
    },
    
    "\/\/localhost:1313\/tags\/linux\/": {
        "title": "Linux",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/linux\/"
    },
    
    "\/\/localhost:1313\/tags\/macbook\/": {
        "title": "Macbook",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/macbook\/"
    },
    
    "\/\/localhost:1313\/tags\/mbp\/": {
        "title": "Mbp",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/mbp\/"
    },
    
    "\/\/localhost:1313\/tags\/ubuntu\/": {
        "title": "Ubuntu",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/ubuntu\/"
    },
    
    "\/\/localhost:1313\/posts\/%EB%A7%A5%EB%B6%81%EC%97%90-%EB%A6%AC%EB%88%85%EC%8A%A4-%EC%84%A4%EC%B9%98-%EA%B7%B8%EB%A6%AC%EA%B3%A0-hyper-v\/": {
        "title": "맥북에 리눅스 설치, 그리고 hyper-v",
        "tags": ["mbp","macbook","linux","ubuntu",],
        "content": "개요 맥을 사용하다보니 점점 운영체제가 갖는 한계가 느껴졌다. 지난 1년 반 동안 사용하면서 느꼈던 점은 윈도우즈와 리눅스의 장점만을 취한 운영체제였지만 동시에 맥 만의 단점을 갖고 있었고 생각보다 매우 영향이 컸다.\nPIM은 기본이고 상위 레벨 프로그래밍을 위해서, 맥 만큼 좋은 운영체제는 없을 것이다. BSD 기반으로 만들어져 완벽하지는 않지만 포팅을 하면 어느정도 리눅스에 가까울 정도로 네이티브 환경에서 실행 가능하고 다양한 분야에서 강력한 유료 애플리케이션이 지원되기 때문에 만족스럽게 사용할 수 있다.\n하지만 맥은 WSL을 도입하면서 윈도우즈가 갖게된 리눅스 플랫폼(?)과의 호환성은 가지고 있지 않다. 포팅은 가능하지만 기본적으로 리눅스 배포판들이 갖는 환경을 구성하기가 매우 힘들다. 게다가 완전한 독점 플랫폼이고 하드웨어조차 security chip을 도입하고 있기 때문에 맥 OS 외에는 설치조차 힘들다.\n또한, 크로스 컴파일러를 이용한 커널 빌드를 위한 환경 구성에도 그닥 좋지 않다. 기본으로 사용하는 파일시스템은 case-sensitive도 아니고, bare-metal 크로스 컴파일러를 구성한다면 상관없겠지만, 커널을 빌드하기 위해 필요한 glibc, openssl, \u0026hellip; 등의 라이브러리들이 포함되기 시작하면 매우 까다로운 작업이 된다. 맥에서 LFS (Linux From Scratch) 프로젝트를 할 것도 아닌데, 그만큼의 노력과 시간이 요구되는 아이러니한 상황이 펼쳐진다.\n맥북과 커널의 호환성 현재, mbp 기준 리눅스 커널의 기본 드라이버들과의 호환성은 그닥 좋지 않다. 호환성은 https://github.com/Dunedan/mbp-2016-linux#bluetooth 페이지에서 확인해볼 수 있다. 논터치바 13인치 모델(13,1)을 제외하고는 와이파이조차 안된다는 점에 유의하자.\n우분투 설치 개인적으로 우분투는 별로 좋아하지 않지만 설치 후 서드 파티 드라이버 설치를 간편하게 하기 위해 우분투를 설치하기로 했다.\n$ hdiutil convert -format UDRW -o ~/Desktop/ubuntu.img ~/Downloads/ubuntu.iso $ dd if=ubuntu.img of=/dev/disk2 bs=1m # /dev/disk2 는 반드시 직접 확인해야 한다. dd 명령어로 img 파일을 USB 에 구울 때 busy resource 에러가 나온다면, 디스크 유틸리티에서 해당 파티션을 unmount 해주고 다시 실행해야 한다.\n나머지는 간단하다. USB를 넣고 맥북 재부팅 시 option키를 눌러 이동식디스크로 부트해주면 설치를 진행할 수 있다.\n호환성 문제 우분투를 설치하고 나서 여러가지 고질적인 하드웨어 문제들이 나타나기 시작했다. 블투루스와 5g 와이파이가 안되는 문제, 소리가 안들리는 문제 등 드라이버를 온전하게 지원하지 않고 있었다. 단순하게 컴파일만을 하기 위해서는 문제가 없었으나 그 이상의 작업을 위해서 불편하게 재부팅해야 하는 문제가 있었다.\n부트캠프 윈도우즈 설치 부트캠프를 이용해 윈도우즈를 설치하고 나머지 맥 파티션은 모두 밀어버렸다. 맥에서의 패러럴은 내게 아무런 장점도 갖고 있지 않았다. VM의 GUI 환경이 필요없는 내게 심리즈 모드나 불필요하게 리소스를 잡아먹는 게 한두가지가 아니었고 virtualbox, vmware 들은 윈도우즈에서 지원하는 hyper-v의 성능의 기대에 크게 못미쳤다.\n부트캠프를 이용해 윈도우즈에서 일어나는 드라이버 이슈를 해결하고 github에서 서드파티로 개발되어 있는 터치패드 드라이버를 설치했다. 이맥스는 터미널 환경에서의 작업을 위해 구성하고 vscode를 이용하여 hyper-v 가상머신으로 접속하니, 맥에서 보던 리소스 문제도 없고 완벽하게 윈하는 만큼의 리눅스 환경을 즐길 수 있게 되었다.\n끝맺음 한동안 맥을 정말 좋아했는데 최근에 들어서는 윈도우즈에 한참 못미치는 모습이다. 대학 시절 우분투, 레드햇, 아치와 젠투 리눅스로 리눅스를 접하고 netbsd와 freebsd 등으로 bsd를 접하다가 마침내 맥을 접했을 때, 내가 원하는 개발 환경을 지원하면서 동시에, 카카오톡과 같은 유틸리티를 wine 없이도 맘껏 돌릴 수 있다는 것이 큰 장점이었다. 하지만 그 매력도 이제는 점점 사라져간다. WSL로 왠만한 리눅스 배포판에 포함되는 tui 툴은 사용가능하고 이마저도 hyper-v를 이용해 가상머신으로 완벽하게 돌릴 수 있다. 거기에 리소스도 일반 가상머신 에뮬레이터에 비해 많이 차지하지 않는다는 점이 정말 매력적이다.\n멀티미디어가 주인 사용자들에게는 맥이 아직도 값어치를 하는 충분히 매력적인 운영체제이겠지만, 개발자에게는 이제 유일한 선택지가 아니다. 오히려 후순위로 밀려나지 않았을까.\n", 
        "url": "\/\/localhost:1313\/posts\/%EB%A7%A5%EB%B6%81%EC%97%90-%EB%A6%AC%EB%88%85%EC%8A%A4-%EC%84%A4%EC%B9%98-%EA%B7%B8%EB%A6%AC%EA%B3%A0-hyper-v\/"
    },
    
    "\/\/localhost:1313\/tags\/bug_on\/": {
        "title": "BUG_ON",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/bug_on\/"
    },
    
    "\/\/localhost:1313\/tags\/warn_on\/": {
        "title": "WARN_ON",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/warn_on\/"
    },
    
    "\/\/localhost:1313\/posts\/warn_on-bug_on-%EB%A7%A4%ED%81%AC%EB%A1%9C\/": {
        "title": "WARN_ON, BUG_ON 매크로",
        "tags": ["WARN_ON","BUG_ON",],
        "content": "개요 업무 중에 예전 SDK에서 커널 패닉이 일어나는 것을 보고 관련 코드를 살펴보니, BUG_ON 매크로 사용에 의한 것으로 파악했다. 커널 패닉을 일으킬 정도로 크리티컬은 아니었기에 해당 매크로를 WARN_ON으로 변경하였다. 변경 이후에 커널 패닉은 일어나지 않았지만 커널 메시지로 보여주는 내용이 조금 달랐다.\n이 글에서는 BUG_ON, WARN_ON 등 커널에서 제공하는 assertion에 해당하는 매크로가 어떤 것들이 있는지 살펴보고 어떤 경우에 사용해야 하는지에 대해 간략히 정리하고자 한다. 이 외에 커널에서는 dump_stack(), save_stack_trace(), dump_trace(), backtrace() 등의 콜 스택 출력 방법을 제공한다.\nWARN_ON, BUG_ON 출처에 따르면, 리눅스 디바이스 드라이버에서 가장 빈번하게 나타나는 매크로는 BUG_ON/BUG와 WARN_ON이라고 설명하고 있다.\nBUG_ON(condition) // 또는 if (condition) BUG() 위 예시처럼 매크로를 사용할 수 있으며 각각의 매크로가 하는 역할은 다음과 같다.\nBUG() 레지스터 내용 출력 Stack Trace 출력 커널 패닉 발생 WARN() 레지스터 내용 출력 Stack Trace 출력 linux/include/asm-generic/bug.h 파일을 살펴보면 BUG() 매크로에 대해 아래와 같이 주석을 달아놓은 것을 확인할 수 있다. 정말로 대안이 없을 경우에만 사용하도록 하며, 정말로 복구할 방법이 없을 때에만 사용하도록 하자.\n/* * Don\u0026#39;t use BUG() or BUG_ON() unless there\u0026#39;s really no way out; one * example might be detecting data structure corruption in the middle * of an operation that can\u0026#39;t be backed out of. If the (sub)system * can somehow continue operating, perhaps with reduced functionality, * it\u0026#39;s probably not BUG-worthy. * * If you\u0026#39;re tempted to BUG(), think again: is completely giving up * really the *only* solution? There are usually better options, where * users don\u0026#39;t need to reboot ASAP and can mostly shut down cleanly. */ 출처 http://embeddedguruji.blogspot.com/2018/12/bugon-vs-warnon-macros-in-linux-kernel.html https://dreamlog.tistory.com/343 ", 
        "url": "\/\/localhost:1313\/posts\/warn_on-bug_on-%EB%A7%A4%ED%81%AC%EB%A1%9C\/"
    },
    
    "\/\/localhost:1313\/tags\/likely\/": {
        "title": "Likely",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/likely\/"
    },
    
    "\/\/localhost:1313\/posts\/likely-and-unlikely\/": {
        "title": "Likely and Unlikely",
        "tags": ["likely","unlikely",],
        "content": "개요 예전에 관리하던 블로그에서 커널에서 사용하는 likely와 unlikely 에 대해서 정리한 포스팅이 있었다. 하지만 시간이 지나 커널 분석 책을 다시 보며 likely, unlikely를 보니 다시 헷갈리기 시작했다.\n회사 업무에서 실행 시간을 줄이는 데에 중요도를 두고 있지만 그 방법에 대해서는 아직 다루지 못하고 있다. 솔루션이 안정화되고 전체적인 업무 내용이 파악되면 본 내용을 정리하면서 실행 시간을 감소할 방법으로 prediction을 이용하는 것을 건의해보고 진행해볼 수 있을 것 같다.\nlikely(), unlikely() 함수의 이름 그대로, 자주 일어날 듯하거나 자주 일어나지 않을 듯한 것을 위한 매크로이다. 출처에는 아래와 같은 예제를 제공하고 있다.\nbvl = bvec_alloc(gfp_mask, nr_iovecs, \u0026amp;idx); if (unlikely(!bvl)) { mempool_free(bio, bio_pool); bio = NULL; goto out; } 특정 condition을 확인하는 용도로 사용하는데 위의 코드에서는 bvec_alloc으로 할당받고 bvl이 유효한 주소값이라면 메모리 해제를 하고 NULL로 변경하는 코드이다.\ninclude/linux/compiler.h 파일에 정의되어 있는 매크로로서 branch prediction 을 위한 용도로 사용된다. 즉, 결과값이 대부분 false로 예상된다면 unlikely()를, true로 예상된다면 likely()를 사용함으로서 컴파일러를 통한 분기 예측을 이용하여 성능 향상을 꾀할 수 있다.\n각각의 정의를 살펴보면, 아래와 같이 되어 있다.\n#define likely(x) __builtin_expect(!!(x), 1) #define unlikely(x) __builtin_expect(!!(x), 0) __built-in function __builtin_expect를 사용하는 것은 컴파일러에게 분기 예측(branch prediction) 정보를 제공하기 위한 것이다. 일반적으로 개발자들은 자신의 프로그램이 어떻게 수행되는지 알기 힘들기 때문에 \u0026lsquo;-fprofile-arcs\u0026rsquo; 옵션을 통해 프로파일을 피드백 받는 것을 선호한다. 하지만 애플리케이션에 따라서 이러한 옵션을 통해 프로파일링이 힘든 경우도 있다.\n예제 아래의 예제를 통해 성능 향상이 어떻게 가능한지 살펴보자.\n#define likely(x) __builtin_expect(!!(x), 1) #define unlikely(x) __builtin_expect(!!(x), 0) int main(int argc, char *argv[]) { int a; a = atoi (argv[1]); if (unlikely (a == 2)) a++; else a--; printf(\u0026#34;%d\\n\u0026#34;, a); return 0; } 위 예제를 컴파일 한 다음, objdump로 살펴보면 아래와 같이 main 부분을 발견할 수 있다.\n$ gcc -o unlikely unlikely.c $ objdump -S unlikely 0000000000001149 \u0026lt;main\u0026gt;: 1149:\t55 push %rbp 114a:\t48 89 e5 mov %rsp,%rbp 114d:\t48 83 ec 20 sub $0x20,%rsp 1151:\t89 7d ec mov %edi,-0x14(%rbp) 1154:\t48 89 75 e0 mov %rsi,-0x20(%rbp) 1158:\t48 8b 45 e0 mov -0x20(%rbp),%rax 115c:\t48 83 c0 08 add $0x8,%rax 1160:\t48 8b 00 mov (%rax),%rax 1163:\t48 89 c7 mov %rax,%rdi 1166:\tb8 00 00 00 00 mov $0x0,%eax 116b:\te8 d0 fe ff ff callq 1040 \u0026lt;atoi@plt\u0026gt; 1170:\t89 45 fc mov %eax,-0x4(%rbp) 1173:\t83 7d fc 02 cmpl $0x2,-0x4(%rbp) 1177:\t0f 94 c0 sete %al 117a:\t0f b6 c0 movzbl %al,%eax 117d:\t48 85 c0 test %rax,%rax 1180:\t74 06 je 1188 \u0026lt;main+0x3f\u0026gt; 1182:\t83 45 fc 01 addl $0x1,-0x4(%rbp) 1186:\teb 04 jmp 118c \u0026lt;main+0x43\u0026gt; 1188:\t83 6d fc 01 subl $0x1,-0x4(%rbp) 118c:\t8b 45 fc mov -0x4(%rbp),%eax 118f:\t89 c6 mov %eax,%esi 1191:\t48 8d 3d 6c 0e 00 00 lea 0xe6c(%rip),%rdi # 2004 \u0026lt;_IO_stdin_used+0x4\u0026gt; 1198:\tb8 00 00 00 00 mov $0x0,%eax 119d:\te8 8e fe ff ff callq 1030 \u0026lt;printf@plt\u0026gt; 11a2:\tb8 00 00 00 00 mov $0x0,%eax 11a7:\tc9 leaveq 11a8:\tc3 retq 11a9:\t0f 1f 80 00 00 00 00 nopl 0x0(%rax) cmpl %0x2, -0x4(%rbp) 에서 보듯 2와 같을 경우에 jump 명령어를 수행하고 같지 않을 경우에는 계속해서 명령어를 순차 진행한다. je 명령어를 실행하지 않으니 pipeline flush가 일어나지 않아 branch prediction을 하지 않았을 때보다 성능 향상을 꾀할 수 있다.\n만약, likely()를 하게 되면 어떨까? 아마 반대로 명령어가 실행될 것이다. 즉, 프로그래머가 예상하는 시나리오로 분기 예측을 하여 최대한 jump 명령어를 수행하지 않도록 하는 기법이다.\n출처 https://woodz.tistory.com/67 ", 
        "url": "\/\/localhost:1313\/posts\/likely-and-unlikely\/"
    },
    
    "\/\/localhost:1313\/tags\/unlikely\/": {
        "title": "Unlikely",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/unlikely\/"
    },
    
    "\/\/localhost:1313\/tags\/typedef\/": {
        "title": "Typedef",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/typedef\/"
    },
    
    "\/\/localhost:1313\/posts\/typedef-is-evil\/": {
        "title": "typedef is evil",
        "tags": ["typedef",],
        "content": "커널 쪽의 코드를 보다가 문득 커널 코드에 적용하는 코딩 규칙에 대해서 궁금해졌다. 관련 내용으로 검색하다보니 재미있는 포스팅 하나를 발견했다. typedef is evil 이라는 제목의 포스팅이었다. 이 포스팅에서는 아래와 같이 사용하는 것을 비판하고 있다. 2000년도 초반에 작성된 것이니, 벌써 20년 가까이 되었는데도 불구하고 여전히 코드에 남아있다는 점이 아이러니하다.\ntypedef struct foo { int bar; int baz; } foot_t, *pfoo_t; 위와 같이 구조체에 대한 포인터를 정의할 때 typedef을 이용하지 말 것을 당부한다. 또한, typedef unsigned long DWORD 와 같은 것도 비판한다. 머신마다 크기가 다르기 때문에라고 하는데, 이 부분에 대해서는 동의하지는 못하겠다. 아키텍처별 코드라면 각 프로세서마다 dword, word의 크기가 정해져 있고 이를 데이터시트에 적합하도록 코딩하기 위해서는 dword와 word 라는 키워드를 정의하여 사용하는 것이 가독성에 유리할 것이라 생각하기 때문이다.\ntypedef struct { pgdval_t pgd; } pgd_t; #define pgd_val(x)\t((x).pgd) #define __pgd(x)\t((pgd_t) { (x) } ) 위는 ARM64 페이징 관련 코드를 살펴보다가 발견한 구조체 정의 부분이다. 위와 같이 구조체 안에 멤버로써 사용하는 타입에 대해서도 u64, u32 대신 pgdval_t를 사용하고 있다. 커널 소스 내의 문서를 살펴보면 이러한 내용에 대해 아래와 같이 정의하고 있으며 좋지 않은 사례로써 pgdval_t 와 유사한 것을 인용하고 있다.\nPlease don\u0026#39;t use things like ``vps_t``. It\u0026#39;s a **mistake** to use typedef for structures and pointers. When you see a vps_t a; in the source, what does it mean? In contrast, if it says struct virtual_container *a; you can actually tell what ``a`` is. Lots of people think that typedefs ``help readability``. Not so. 커널 문서에 따르면, 단순히 typedef을 사용하여 타입을 재정의할 경우 얻을 수 있는 이점이 없다고 얘기한다. 이어서 사용해야할 때를 아래와 같이 설명한다.\n(a) totally opaque objects (where the typedef is actively used to hide what the object is).\nExample: ``pte_t`` etc. opaque objects that you can only access using the proper accessor functions. .. note:: Opaqueness and ``accessor functions`` are not good in themselves. The reason we have them for things like pte_t etc. is that there really is absolutely **zero** portably accessible information there. (b) Clear integer types, where the abstraction helps avoid confusion whether it is int or long.\nu8/u16/u32 are perfectly fine typedefs, although they fit into category (d) better than here. .. note:: Again - there needs to be a **reason** for this. If something is ``unsigned long``, then there's no reason to do typedef unsigned long myflags_t; but if there is a clear reason for why it under certain circumstances might be an ``unsigned int`` and under other configurations might be ``unsigned long``, then by all means go ahead and use a typedef. (c) when you use sparse to literally create a new type for type-checking.\n(d) New types which are identical to standard C99 types, in certain exceptional circumstances.\nAlthough it would only take a short amount of time for the eyes and brain to become accustomed to the standard types like ``uint32_t``, some people object to their use anyway. Therefore, the Linux-specific ``u8/u16/u32/u64`` types and their signed equivalents which are identical to standard types are permitted -- although they are not mandatory in new code of your own. When editing existing code which already uses one or the other set of types, you should conform to the existing choices in that code. (e) Types safe for use in userspace.\nIn certain structures which are visible to userspace, we cannot require C99 types and cannot use the ``u32`` form above. Thus, we use __u32 and similar types in all structures which are shared with userspace. Maybe there are other cases too, but the rule should basically be to NEVER EVER use a typedef unless you can clearly match one of those rules.\nIn general, a pointer, or a struct that has elements that can reasonably be directly accessed should never be a typedef.\n", 
        "url": "\/\/localhost:1313\/posts\/typedef-is-evil\/"
    },
    
    "\/\/localhost:1313\/posts\/%EC%9D%B4%EB%A7%A5%EC%8A%A4-%EA%B8%B0%EB%B3%B8-%EC%97%90%EB%94%94%ED%84%B0%EB%A1%9C-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0\/": {
        "title": "이맥스 기본 에디터로 사용하기",
        "tags": ["emacs",],
        "content": "개요 이맥스를 메인으로 사용하는 환경을 위해서 필요한 몇 가지 설정 값에 대해 포스팅하고자 한다. 이맥스의 경우 다른 vim 과 마찬가지로 초기 로드가 상당히 오래 걸리는 편이다. daemon 형태로 실행한다고 해도, 첫 로드를 위해 필요한 시간은 다른 편집기에 비해서 오래 걸리는 편이다. 때문에 편집기를 실행하고자 하는 때에 초기화를 진행하지 않고 사용자로 로그인하여 부트하는 시간에 편집기의 초기화를 진행하도록 설정할 것이다.\n데몬 형태로 실행하는 것을 사용자 레벨의 systemd로 활성화함으로써 로그인 시에 자동으로 실행되게 한다. 그리고 gnome에서 사용하는 몇 가지 애플리케이션 설정만 바꿔주면 기본적인 파일들에 대한 편집은 이맥스에서 사용할 수 있게 된다.\n설정 환경 본 포스팅 작성에 사용된 리눅스 환경은 아래와 같다.\ndistro: Arch Linux D/E: Gnome 3.34.2 systemd: 244.2-1 systemd 에 emacs 등록하기 현재 대부분의 리눅스 배포판에서는 initrc 에서 systemd 로 바뀌었다. initrc를 고집하던 젠투에서도 systemd를 사용하는 것을 보면, 아마 대부분의 배포판에서 사용하고 있을 거라 생각하며, initrc를 사용하는 시스템이라면 젠투 쪽의 위키페이지를 참고하기를 바란다.\n먼저 ~/.config/systemd/user/emacs.service 파일을 아래와 같이 생성한다.\n[Unit] Description=Emacs text editor Documentation=info:emacs man:emacs(1) https://gnu.org/software/emacs/ [Service] Type=forking ExecStart=/usr/bin/emacs --daemon ExecStop=/usr/bin/emacsclient --eval \u0026#34;(kill-emacs)\u0026#34; Environment=SSH_AUTH_SOCK=%t/keyring/ssh Restart=on-failure [Install] WantedBy=default.target 그 뒤, systemctl 명령어를 이용해 해당 서비스를 활성화한다.\nsystemctl enable --user emacs systemctl disable --user emacs gnome application 아이템 조정하기 gnome에서 파일을 열 때 mime type에 따라 기본으로 열기 위한 애플리케이션을 미리 정의해놓는다. 아래와 같이 /usr/share/applications/emacs.desktop 파일을 열어서 emacsclient를 이용하도록 설정한다.\n[Desktop Entry] Name=Emacs GenericName=Text Editor Comment=Edit text MimeType=text/english;text/plain;text/x-makefile;text/x-c++hdr;text/x-c++src;text/x-chdr;text/x-csrc;text/x-java;text/x-moc;text/x-pascal;text/x-tcl;text/x-tex;application/x-shellscript;text/x-c;text/x-c++; Exec=emacsclient -c -a \u0026#34;\u0026#34; %F Icon=emacs Type=Application Terminal=false Categories=Development;TextEditor; StartupWMClass=Emacs Keywords=Text;Editor; 이제 모든 설정이 끝났다. 파일에 대한 기본 편집툴을 이맥스로 설정하면, 별도의 초기화 과정 없이 곧바로 실행되는 것을 알 수 있다.\n출처 https://www.emacswiki.org/emacs/EmacsAsDaemon ", 
        "url": "\/\/localhost:1313\/posts\/%EC%9D%B4%EB%A7%A5%EC%8A%A4-%EA%B8%B0%EB%B3%B8-%EC%97%90%EB%94%94%ED%84%B0%EB%A1%9C-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0\/"
    },
    
    "\/\/localhost:1313\/tags\/debug\/": {
        "title": "Debug",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/debug\/"
    },
    
    "\/\/localhost:1313\/tags\/ftrace\/": {
        "title": "Ftrace",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/ftrace\/"
    },
    
    "\/\/localhost:1313\/posts\/ftrace-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EC%BB%A4%EB%84%90-%EB%94%94%EB%B2%84%EA%B9%85\/": {
        "title": "ftrace 이용한 커널 디버깅",
        "tags": ["debug","ftrace",],
        "content": "개요 커널 디버깅을 위해 procfs, sysfs, 레지스터 덤프 등의 단순 정보를 제외한 다른 방법은 없을까? 업무에서 문제 원인 파악을 위해서는 디버깅이 중요한데 커널에서는 사용할 수 있는 디버깅 툴이 제한적이다. 또한, 팀 내에서도 로그나 레지스터 외에 별다른 디버깅 도구를 사용하는 것 같지 않아, 다른 방법이 있는지 찾아보던 중 익숙한 이름의 ftrace가 있다는 것을 알게 됐다.\n예전에 젠투 리눅스를 사용할 무렵, menuconfig에서 커널 해킹이라는 흥미로운 메뉴에서 알 수 없는 tracer라는 메뉴로만 본 것으로 이해하지 직접적으로 사용한 적은 없었는데, 실제 사용해보니 생각보다 많은 옵션, 정보들을 보여주었다.\nftrace란? ftrace는 리눅스 커널에서 제공하는 트레이서로, 커널의 세부 동작을 알기 쉽게(?) 출력해주는 도구이다. 특징은 아래와 같다.\n인터럽트, 스케쥴링, 커널 타이머 동작을 상세하게 추적해준다. 함수 필터를 지정하면 자신을 호출한 함수와 전체 콜 스택까지 출력해준다. 이 때, 코드를 수정할 필요가 없다. 함수를 어느 프로세스가 실행하는지 알 수 있다. 함수 실행 시각을 알 수 있다. ftrace 로그를 키면 시스템 동작에 부하를 주지 않는다. 커널에서 ftrace 활성화하기 ftrace를 사용하기 위해서는 관련된 설정 플래그를 활성화해줘야 한다.\nCONFIG_FTRACE=y CONFIG_DYNAMIC_FTRACE=y CONFIG_FUNCTION_TRACER=y CONFIG_FUNCTION_GRAPH_TRACER=y CONFIG_IRQSOFF_TRACER=y CONFIG_SCHED_TRACER=y CONFIG_FUNCTION_PROFILER=y CONFIG_STACK_TRACER=y CONFIG_TRACER_SNAPSHOT=y 라즈베리파이에서는 기본으로 ftrace에 필요한 세부 설정 플래그가 모두 켜져 있다. 또한 ftrace는 리눅스 커널 공통 기능이므로 다른 시스템에서도 사용 가능하다.\n커널 2.6.28 버전부터 포함된 기본 기능으로서 아래와 같이 debugfs를 마운트 시켜서 사용할 수 있다.\nmount -t debugfs nodev /sys/kernel/debug ftracer 설정 방법 아래와 같이 셸 스크립트를 이용하여 설정도 가능하지만 기본적으로는 sysfs를 이용하여 설정을 한다. 설정 시나리오는 tracer를 OFF 한 뒤에 옵션들을 설정해주고 다시 ON하는 방식으로 설정한다.\n#!/bin/sh echo 0 \u0026gt; /sys/kernel/debug/tracing/tracing_on sleep 1 echo \u0026#34;tracing_off\u0026#34; 7 echo 0 \u0026gt; /sys/kernel/debug/tracing/events/enable sleep 1 echo \u0026#34;events disabled\u0026#34; echo secondary_start_kernel \u0026gt; /sys/kernel/debug/tracing/set_ftrace_filter sleep 1 echo \u0026#34;set_ftrace_filter init\u0026#34; echo function \u0026gt; /sys/kernel/debug/tracing/current_tracer sleep 1 echo \u0026#34;function tracer enabled\u0026#34; echo 1 \u0026gt; /sys/kernel/debug/tracing/events/sched/sched_wakeup/enable echo 1 \u0026gt; /sys/kernel/debug/tracing/events/sched/sched_switch/enable echo 1 \u0026gt; /sys/kernel/debug/tracing/events/irq/irq_handler_entry/enable echo 1 \u0026gt; /sys/kernel/debug/tracing/events/irq/irq_handler_exit/enable echo 1 \u0026gt; /sys/kernel/debug/tracing/events/raw_syscalls/enable sleep 1 echo \u0026#34;event enabled\u0026#34; echo schedule ttwu_do_wakeup \u0026gt; /sys/kernel/debug/tracing/set_ftrace_filter sleep 1 echo \u0026#34;set_ftrace_filter enabled\u0026#34; echo 1 \u0026gt; /sys/kernel/debug/tracing/options/func_stack_trace echo 1 \u0026gt; /sys/kernel/debug/tracing/options/sym-offset echo \u0026#34;function stack trace enabled\u0026#34; echo 1 \u0026gt; /sys/kernel/debug/tracing/tracing_on echo \u0026#34;tracing_on\u0026#34; 리눅스에서는 nop, function, function_graph를 포함한 여러가지 트레이서를 제공한다.\nnop: 기본 트레이서로 ftrace 이벤트만 출력 function: set_ftrace_filter로 지정한 함수를 누가 호출하는지 출력한다. function_graph: 함수 실행 시간과 세부 호출 정보를 그래픽 정보를 추가(?)해 출력한다. ftracer 사용 예 아래는 출처에서 기술되어 있는 예시들을 한 데 정리한 것이다. 이 후, 업무에서 사용한 이력이나 팁이 있는 경우에 이 곳에 관련 내용을 추가하도록 한다.\n커널 함수 추적하기 커널 함수들이 호출되는 과정을 살펴본다. 우선 tracing 디렉토리로 이동해서 추적할 수 있는 항목들을 알아보자.\ntracing $ cat available_tracers blk kmemtrace function_graph wakeup_rt wakeup function sysprof sched_switch initcall nop tracing $ echo function \u0026gt; ./current_tracer 위에서처럼 tracer의 모드를 설정한 후 vi로 trace 파일을 열어보면 아래와 같은 내용을 볼 수 있다.\n# tracer: function # # TASK-PID CPU# TIMESTAMP FUNCTION # | | | | | sshd-15219 [000] 159421.106063: __math_state_restore \u0026lt;-__switch_to sshd-15219 [000] 159421.106064: finish_task_switch \u0026lt;-thread_return sshd-15219 [000] 159421.106065: fget_light \u0026lt;-do_select sshd-15219 [000] 159421.106065: sock_poll \u0026lt;-do_select sshd-15219 [000] 159421.106066: tcp_poll \u0026lt;-sock_poll sshd-15219 [000] 159421.106066: fget_light \u0026lt;-do_select sshd-15219 [000] 159421.106066: pipe_poll \u0026lt;-do_select 또한, 프로세스 별로 호출하고 있는 커널 함수를 직접 살펴볼 수도 있다.\n[root@server tracing]# echo function_graph \u0026gt; ./current_tracer 0) | do_vfs_ioctl() { 0) | vfs_ioctl() { 0) | tty_ioctl() { 0) 0.349 us | tty_paranoia_check(); 0) 0.301 us | pty_unix98_ioctl(); 0) | tty_ldisc_ref_wait() { 0) | tty_ldisc_try() { 0) 0.301 us | _spin_lock_irqsave(); 스케쥴링 과정 보기 출처 https://kldp.org/node/161282 https://brunch.co.kr/@alden/24 ", 
        "url": "\/\/localhost:1313\/posts\/ftrace-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EC%BB%A4%EB%84%90-%EB%94%94%EB%B2%84%EA%B9%85\/"
    },
    
    "\/\/localhost:1313\/tags\/raspberry-pi\/": {
        "title": "Raspberry Pi",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/raspberry-pi\/"
    },
    
    "\/\/localhost:1313\/posts\/raspberry-pi%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EC%82%AC%EC%9D%B4%EB%93%9C-%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8\/": {
        "title": "Raspberry Pi를 이용한 사이드 프로젝트",
        "tags": ["side project","raspberry pi",],
        "content": "카메라 포팅 건으로 불필요한 업무가 지속적으로 들어온다. 포팅을 위한 기본적인 것들을 자동화 하거나 유틸리티 형태로 제공할 수 있어야 한다는 것을 뼈저리게 느꼈다. 특히, 오늘은 고객사의 디바이스 트리 내 오타로 인해 Pin Control이 안됐던 원인을 찾고 나니 그러한 유틸리티의 필요성을 뼈저리게 느꼈다.\n사이드 프로젝트를 통해 개발에 대한 갈증을 해소하고 불필요하게 업무에 투입되는 시간을 줄이기 위해서라도 3개월 정도의 시간을 들여 만들고자 한다. 라즈베리파이를 이용해 유틸리티는 디바이스 모듈과 함께 연동되도록 설계하고 ncurses를 이용하여 UI를 제공하도록 할 계획이다.\n힘들겠지만, 개선할 수 있는 여지가 많아 배울 점은 많을 것 같다.\n", 
        "url": "\/\/localhost:1313\/posts\/raspberry-pi%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EC%82%AC%EC%9D%B4%EB%93%9C-%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8\/"
    },
    
    "\/\/localhost:1313\/tags\/side-project\/": {
        "title": "Side Project",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/side-project\/"
    },
    
    "\/\/localhost:1313\/tags\/wayland\/": {
        "title": "Wayland",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/wayland\/"
    },
    
    "\/\/localhost:1313\/posts\/wayland%EA%B3%BC-weston\/": {
        "title": "Wayland과 Weston",
        "tags": ["wayland","weston",],
        "content": "개요 직접적으로 연관된 업무는 아니지만 팀 내에서 wayland, weston 이라는 용어가 자주 들린다. 어렸을 적에 리눅스 데스크탑 환경에 관심이 많아 X11 기반으로 최소한의 작업 환경을 맞추고 gnome이나 kde, xfce가 아닌 fluxbox, blackbox, i3, xmonad, enlightenment 등을 이용해서 이런저런 시도를 해보았던 기억이 난다. 당시에는 그저 설치해서 사용하기에만 급급했지 실제로 업무에서 그러한 것들이 사용될 줄은 꿈에도 몰랐다.\n이번 포스팅에서는 사내 위키의 내용을 출처로 하여, wayland, weston에 대한 구조를 살펴보고 클라이언트 예제를 기술하고자 한다.\nWayland wayland는 display server protocol이다. 윈도우즈와 달리 리눅스는 GUI 환경에 대해서도 server - clients 개념으로 처음 구현되었고 X11 라는 이름으로 사용되었다. 그러나 복잡한 프로토콜과 라이브러리들로 인해 불필요한 호출이 많았고 이를 경량화하기 위한 노력의 결과가 바로 wayland이다.\nX11에서 wayland로의 변화는 아래와 같다.\nX11은 독립적인 프로세스로서 동작하지만 wayland는 라이브러리 형태로 구현되어 오버헤드가 감소\nX11은 server, compositor로 구분되어 있지만 wayland는 server와 compositor가 통합되어 두 컴포넌트 간의 불필요한 통신이 감소\nX11에서 렌더링을 서버가 담당하였지만, Wayland는 클라이언트가 렌더링을 담당하여 서버와의 복잡도가 감소\nX11은 서버가 client, compositor, kernel 간의 모든 동작을 중개하였지만, Wayland는 server가 client, kernel만 중개하여 복잡도가 감소함\nAndroid에서 Wayland를 사용하지 않는 이유 Android Graphic에서는 SurfaceFlinger, Gralloc 솔루션을 사용한다. 각각에 대한 내용은 아래를 살펴보자.\nSurfaceFlinger SurfaceFlinger는 사용자 프로세스나 앱에서 생성한 화면을 관리하고 화면의 위치나 표시 순서, 색상 등을 관리한다. 또한, 커널에 위치한 프레임 버퍼 드라이버와 연동하여 생성된 최종 이미지를 프레임 버퍼 드라이버를 통해 화면에 출력할 수 있도록 프레임 버퍼와 연동하는 역할을 한다. Gralloc Android에서 그래픽 버퍼의 할당과 해제를 담당한다.\n윈도우즈와 맥에서 사용하지 않는 이유 윈도우즈는 DWM (Desktop Window Manager)를, 맥은 Quartz Compositor를 사용한다.\nServer/Client Model Wayland는 여러 개의 인터페이스를 정의하여 사용할 수 있고 각각의 인터페이스는 request와 event로 구성된다.\nkernel의 이벤트를 받으면 wayland compositor로 전달한다. kernel의 드라이버를 재사용할 수 있다. compositor는 여러 클라이언트 중에서 해당 이벤트를 수신할 client를 결정한다. client가 이벤트를 수신하면 렌더링이 발생한다. client는 업데이트 된 화면을 표시하기 위해 compositor에 요청을 보낸다. compositor는 client로부터 요청을 받은 후에 화면을 재구성한다. 그 뒤, ioctl을 통해 KMS에게 재구성 된 화면을 전송한다. 그래픽 라이브러리 종류 출처에서는 모듈이라는 용어를 사용했는데 그래픽 라이브러리가 더 적절할 것이라 생각하여 그래픽 라이브러리로 기술하겠다. 이전에 보지 못했던 라이브러리들(clutter, SDL)이 있어 함께 정리하고자 한다.\nQT5 이전에는 qt4, qt5의 호환성으로 엄청나게 욕을 먹었었는데 최근에는 해결된 것으로 보인다. 리눅스, 안드로이드, 윈도우즈 등의 운영체제를 지원하며, qt의 대표적인 모듈은 아래와 같다.\nQtCore: 컨테이너 관리, 스레드 관리, 이벤트 관리 등을 제공하는 기본 라이브러리 QtGui \u0026amp; QtWidgets: 데스크탑용 GUI 툴킷으로서 응용 프로그램을 위한 그래픽 구성 요소 제공 QtMultimedia: 비디오, 카메라, 오디오 기능을 가진 클래스 제공 QtNetwork: 네트워크 통신을 위한 클래스 제공 QtSQL: ODBC, SQLite, MySQL 사용가능하도록 지원 QtWayland: Wayland 기능 사용할 수 있도록 API 제공 GTK+ GTK+ (GIMP Toolkit+)는 GUI를 위한 라이브러리로서 실제 사용해보면 알겠지만, 제공되는 프레임워크 API나 결과물의 코드가 더럽다. GTK에 대해서는 별도로 언급하지 않겠다.\nClutter 인텔에 합병된 OpenedHand에서 C언어로 구현한 2D 그래픽 라이브러리로 객체 Actor와 이를 그리는 Canvas라는 추상화된 개념이 있다. OpenGL ES를 사용하기 때문에 데스크탑 뿐만 아니라 다양한 플랫폼에 적용할 수 있다.\nSDL (Simple DirectMedia Layer) 오디오, 키보드, 마우스, 조이스틱 등 하드웨어가 OpenGL 및 Direct3D에 대한 접근을 제공하도록 설계된 크로스 플랫폼 라이브러리이다. SDL은 윈도우즈용 게임을 리눅스로 쉽게 포팅하기 위한 라이브러리를 만들기 위해 개발되었고 현재는 멀티 플랫폼 상에서 게임 개발 및 포팅 목적으로 널리 이용되고 있다.\nEFL Enlightenment Foundation Library로 Enlightenment에 코어 그래픽 라이브러리로 사용되는 라이브러리이다.\nWeston weston은 wayland compositor의 참조 구현(하드웨어 또는 소프트웨어 구현을 돕기 위해 제공하는 샘플 프로그램)이다. Desktop, 자동차, 키오스크, 셋톱박스 등을 위한 라이브러리를 제공하며 Wayland API에 대한 기본적인 내용을 담고 있기 때문에 기본적으로 창관리와 composite 기능을 갖고 있다.\n기본 역할 Weston의 기본 역할은 아래와 같다.\n윈도우들을 여러 계층으로 구분하여 관리한다. shell(윈도우 관리 규칙)을 이용해 출력되는 윈도우의 순서를 결정한다. 윈도우를 화면에 나타나게 하거나 위치를 옮기고 화면 크기 변경을 관리한다. 출처 Telechips, Graphic Framework ", 
        "url": "\/\/localhost:1313\/posts\/wayland%EA%B3%BC-weston\/"
    },
    
    "\/\/localhost:1313\/tags\/weston\/": {
        "title": "Weston",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/weston\/"
    },
    
    "\/\/localhost:1313\/posts\/arm64%EC%97%90%EC%84%9C%EC%9D%98-%EC%BB%A4%EB%84%90-%EB%A9%94%EB%AA%A8%EB%A6%AC-%EB%A7%B5\/": {
        "title": "ARM64에서의 커널 메모리 맵",
        "tags": ["memory map","page table","VA_BITS",],
        "content": "ARM64에서의 커널 메모리 맵 \u0026lt;코드로 알아보는 ARM 리눅스 커널\u0026gt; 에서의 \u0026ldquo;페이징과 매핑\u0026quot;이라는 챕터를 읽으면서 커널 메모리 맵에 대한 간략한 설명을 포스팅으로 정리하고자 한다. 물리 메모리와 가상 메모리 주소간의 매핑을 위해 사용하는 테이블을 매핑 테이블이라고 하며, 본 포스팅에서는 기본적인 개념인 가상 주소 공간과 ARM64에서의 커널 메모리 맵 구성을 정리한다.\nARM64 커널에서는 64비트 가상 주소의 시작 부분과 끝부분의 영역을 사용한다. 이 때, 가상 주소 기준으로 각 끝 영역을 사용한다는 점에 유의해야 한다.\n가상 주소 공간 ARM64 커널에서는 64비트 가상 주소의 시작 부분과 끝부분의 가상 주소 공간을 사용한다. 각각의 공간은 서로 다른 TTBR(Translation Table Base Register)를 사용하기 때문에 각자의 페이지 테이블을 갖고 있다. 이 때, 커널 주소 공간은 TTBR1, 사용자 주소 공간은 TTBR0를 사용하며 각각의 레지스터는 MMU 디바이스 내부에 저장된다.\n페이지 테이블을 엔트리 여러 개로 구성되며, 페이지 테이블 엔트리 하나는 페이지 1개에 대한 물리주소로 변환하는데 사용한다. ARM64 커널에서는 페이지 크기로 4KB, 16KB, 64KB 중 하나를 사용할 수 있다(기본 4KB). 중요한 점은 ARM64 커널이 64비트 가상 주소 전체를 사용하는 것이 아니라 36, 39, 42, 47 및 최대 48비트와 같이 아키텍처가 지원하는 비트만을 선택해 사용할 수 있고 이 범위를 가상 주소로 사용한다. 이러한 비트는 VA_BITS 값으로 설정하며 페이지 크기와 페이지 테이블 엔트리 인덱스, 페이지 오프셋 크기 구성에 따라 페이지 테이블 엔트리의 수가 결정된다.\n커널에서 레벨에 따라 페이지 테이블 명칭은 아래와 같이 구분한다.\n1단계: PGD (Page Global Directory) 2단계: PUD (Page Upper Directory) 3단계: PMD (Page Mid-level Directory) 4단계: PTE (Page Table Entry) 예를 들어, VA_BITS 를 36으로 설정한다면, 아래와 같이 64비트 가상주소 중에서 테이블 엔트리 인덱스와 페이지 오프셋을 포함한 비트가 36비트를 차지한다. 그리고 2단계 페이지 테이블을 구성한다면, 16K 페이지를 사용한다고 가정했을 때 아래와 같이 가상주소의 구성을 생각할 수 있다.\nARM64 커널의 기본 구성인 3단계 페이지 테이블 커널의 기본 구성인 4K 페이지와 VA_BITS = 39로 구성된 3단계 페이지 테이블을 사용하는 경우를 살펴보자. 64비트 중 39비트에 포함하지 않는 상위 25비트는 커널(모두 1)과 유저 주소 공간(모두 0) 구분에 사용하고, 나머지 39비트를 각각 9비트/9비트/9비트/12비트(페이지 오프셋용) 으로 나누어 사용한다. 상위 비트에 따라 TTBR을 어떤 것을 사용할지를 결정한다.\nARM64 커널 메모리 맵 커널용 가상 주소 영역도 독특하게 사용한다. 상위 절반은 물리 메모리를 1:1로 미리 매핑하여 사용하며 나머지 절반에 대해 fixmap, vmalloc, vmemmap, pci-iomap 등의 영역과 커널 이미지 영역으로 나누어 사용한다.\nfixmap: 컴파일 테임에 목적에 따라 가상 주소 공간이 이미 결정된 매핑 영역이다. vmalloc: 런타임에 연속된 가상 주소 공간을 자유롭게 매핑할 수 있는 영역이다. vmalloc() 함수가 vmap() 함수를 통해 이용하는 곳이고 ioremap() 역시 사용하는 영역이다. vmemmap: 빠른 접근을 위해 분산된 페이지 구조체들을 이 영역에 매핑하기 위해 사용한다. pci-iomap: PCI 디바이스의 메모리 맵 I/O 영역으로 사용한다. PCI 디바이스만 사용하며, 일반적으로 ioremap 함수를 통해 매핑하는 곳은 vmalloc 영역이다. ", 
        "url": "\/\/localhost:1313\/posts\/arm64%EC%97%90%EC%84%9C%EC%9D%98-%EC%BB%A4%EB%84%90-%EB%A9%94%EB%AA%A8%EB%A6%AC-%EB%A7%B5\/"
    },
    
    "\/\/localhost:1313\/tags\/bitbake\/": {
        "title": "Bitbake",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/bitbake\/"
    },
    
    "\/\/localhost:1313\/tags\/chroot\/": {
        "title": "Chroot",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/chroot\/"
    },
    
    "\/\/localhost:1313\/posts\/chroot-%ED%99%98%EA%B2%BD%EC%9D%84-%EC%9D%B4%EC%9A%A9%ED%95%9C-yocto-%ED%99%98%EA%B2%BD-%EA%B5%AC%EC%84%B1\/": {
        "title": "Chroot 환경을 이용한 Yocto 환경 구성",
        "tags": ["yocto","bitbake","chroot",],
        "content": "우분투 버전과 Yocto 버전 AGL (Automotive Grande Linux) 라는 오픈소스 프로젝트에 참여해보기 위해 개발 환경을 구성하였다. 작업 환경은 가장 최신 버전의 우분투 21.10 버전으로 구성하였다. 개인적으로 Rolling Release 배포판인 아치리눅스나 젠투 리눅스들을 선호하지만 사용자가 많고 프로젝트 대부분에서 데비안 타입의 패키징을 지원하므로, 익숙한 우분투를 사용하기로 했다.\n한 가지 중요한 것은 우분투의 경우 각 버전마다 빌드 환경이 다르고, Yocto의 경우 이러한 빌드 환경에 의존성을 가지고 있다는 것이다. 처음 Ubuntu 21.10 버전에서 레시피를 빌드했을 때 아래와 같은 에러 메시지에 맞딱드렸다.\nIn file included from ../include/QtCore/qfloat16.h:1, from ../include/QtCore/../../../git/src/corelib/global/qendian.h:44, from ../include/QtCore/qendian.h:1, from /home/sukbeom/Workspace/AGL/build/tmp/work/x86_64-linux/qtbase-native/5.14.2+gitAUTOINC+3a6d8df521-r0/git/src/corelib/codecs/qutfcodec.cpp:43: ../include/QtCore/../../../git/src/corelib/global/qfloat16.h:295:7: error: ‘numeric_limits’ is not a class template 295 | class numeric_limits\u0026lt;QT_PREPEND_NAMESPACE(qfloat16)\u0026gt; : public numeric_limits\u0026lt;float\u0026gt; | ^~~~~~~~~~~~~~ ../include/QtCore/../../../git/src/corelib/global/qfloat16.h:295:77: error: expected template-name before ‘\u0026lt;’ token 295 | class numeric_limits\u0026lt;QT_PREPEND_NAMESPACE(qfloat16)\u0026gt; : public numeric_limits\u0026lt;float\u0026gt; | ^ ../include/QtCore/../../../git/src/corelib/global/qfloat16.h:295:77: error: expected ‘{’ before ‘\u0026lt;’ token ../include/QtCore/../../../git/src/corelib/global/qfloat16.h:333:18: error: ‘numeric_limits’ is not a class template 333 | template\u0026lt;\u0026gt; class numeric_limits\u0026lt;const QT_PREPEND_NAMESPACE(qfloat16)\u0026gt; | ^~~~~~~~~~~~~~ ../include/QtCore/../../../git/src/corelib/global/qfloat16.h:333:69: error: ‘std::numeric_limits’ is not a template 333 | template\u0026lt;\u0026gt; class numeric_limits\u0026lt;const QT_PREPEND_NAMESPACE(qfloat16)\u0026gt; 이 문제는 Yocto의 빌드 환경에 대한 의존성 때문이었다. 그리고 이를 해결하기 위해 필요한 것이 바로 chroot 이다. 시스템을 구성하는 coreutils, binutils, gcc, \u0026hellip; 등의 패키지들은 우분투의 경우 버전 별로 다르게 구성되어 있고 각 패키지들끼리도 서로에 대해 어느 정도 의존성을 가진다. 일반적으로 chroot은 리눅스를 처음 설치할 때, LiveCD 환경에서 새로 설치하는 패키지들을 빌드하기 위한 임시 환경을 구성하거나 툴체인을 빌드하기 위한 임시 환경을 구성할 때 사용한다. 위의 문제도 Yocto 버전에 따라 요구하는 빌드 환경이 있으니 서버에 설치되어 있는 시스템에서 chroot을 위한 fakeroot 환경을 구성해주면 된다. 다행이도 우분투에서는 이를 위해 debootstra 이라는 것을 이용하여 간단하게 환경을 만들 수 있었다.\nchroot 환경 구성 debootstrap 이용하기\nchroot 하기 위한 임시 rootfs 구조를 debootstrap 명령어를 이용하여 구성한다. 이 때 중요한 것은 focal 대신에 다른 릴리즈된 이름으로 변경 가능하다는 점이다. https://wiki.ubuntu.com/Releases 페이지를 참고하여 변경하고자 하는 우분투 버전으로 임시 시스템 환경을 구성한다.\n$ sudo debootstrap --variant=buildd focal AGL-chroot $ sudo mount -t proc /proc AGL-chroot/proc $ sudo mount --rbind /sys/ AGL-chroot/sys $ sudo mount --rbind /dev AGL-chroot/dev $ sudo chroot AGL-chroot /bin/bash sources.list 수정하기\nchroot 환경에서 필요한 build 패키지들을 설치하기 위해 아래와 같이 sources.list 를 변경한다.\n$ root@sukbeom-P65-67HSHP:~# vi /etc/apt/sources.list deb http://ftp.daum.net/ubuntu focal main deb http://ftp.daum.net/ubuntu/ focal universe builduser 생성하기\n일반적으로 yocto 에서는 root 유저는 빌드할 수 없다. (sanity.conf 에서 수정할 수는 있지만 기본값대로 진행하기 위해 사용자를 별도로 생성하였다.) 이를 위해 builduser 를 아래와 같이 생성해주자.\n$ useradd builduser $ mkdir -p /home/builduser \u0026amp;\u0026amp; chown -R builduser:builduser /home/builduser $ su - builduser 마무리 이제 빌드를 위한 환경 구성은 마무리 되었다. Yocto 빌드도 컴파일 에러 없이 정상적으로 빌드 되는 것을 확인할 수 있을 것이다. 빌드가 마무리 되고 환경을 정리하려면 앞서 마운트했던 디렉토리들을 그대로 unmount 해주면 된다.\n업무에 관련된 오픈 소스에 참여해볼 수 있는 좋은 기회를 찾은 것 같아 기쁘다. 무엇보다도 지난 2년 반 동안 항상 궁금해했던 질문들, 가령 \u0026ldquo;다른 곳에서는 어떤 개발 프로세스로 릴리즈를 할까?\u0026rdquo;, \u0026ldquo;어떻게 모듈별로 관리하여 Yocto로 통합할까?\u0026rdquo;, \u0026ldquo;테스팅은 어떻게 할까?\u0026rdquo;, \u0026ldquo;아키텍처에 관련된 커널 코드는 어떻게 테스팅을 할까?\u0026rdquo; 등에 대한 답을 찾을 수 있을 것 같다는 기대감이 크다.\n", 
        "url": "\/\/localhost:1313\/posts\/chroot-%ED%99%98%EA%B2%BD%EC%9D%84-%EC%9D%B4%EC%9A%A9%ED%95%9C-yocto-%ED%99%98%EA%B2%BD-%EA%B5%AC%EC%84%B1\/"
    },
    
    "\/\/localhost:1313\/tags\/memory-map\/": {
        "title": "Memory Map",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/memory-map\/"
    },
    
    "\/\/localhost:1313\/tags\/yocto\/": {
        "title": "Yocto",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/yocto\/"
    },
    
    "\/\/localhost:1313\/categories\/yocto\/": {
        "title": "Yocto",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/categories\/yocto\/"
    },
    
    "\/\/localhost:1313\/tags\/gpio\/": {
        "title": "Gpio",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/gpio\/"
    },
    
    "\/\/localhost:1313\/posts\/gpio-mapping\/": {
        "title": "GPIO Mapping",
        "tags": ["gpio",],
        "content": "GPIO 맵핑하기 오늘은 gpio 맵핑을 위해 디바이스 트리를 이용하였지만 정상적으로 설정되지 않는 문제가 있었다. SoC에서 GPIO Enable 에 대한 것이 문제일 것이라 예상되지만, GPIO 맵핑하는 방법으로 디바이스 트리를 이용하는 것 외에 어떤 대안이 있는지 알아보고자 정리하고자 한다.\nGPIO 데이터를 맵핑하여 사용할 수 있는 방법은 아래와 같이 세 가지 방법이 있다.\n디바이스 트리 이용 (최근 트렌드) Legacy Board \u0026amp; Machine Specific Code에서 플랫폼 데이터로 정의 (출처에 따르면) GPIO 맵핑을 플랫폼 데이터에 등록하여 사용하는 경우는 많지 않고 단순하게 핀 번호로 사용하는 경우가 대부분이었다고 한다. ACPI 펌웨어 테이블에 정의 디바이스 트리를 사용하는 GPIO 매핑 디바이스 노드 내부에 \u0026ldquo;gpio-controller\u0026rdquo; 속성이 있으면 GPIO Controller 노드를 의미한다.\ncell 개수 #gpio-cells = \u0026lt;2\u0026gt; 속성은 셀 데이터 2개를 사용한다는 것을 의미한다. 아래 예시에서는 gpio1, gpio2에 대해 각각 다음과 같이 해석 가능하다.\ngpio1 controller는 cell 2개를 사용하여 디바이스 드라이버가 인자 2개를 받아 처리한다. gpio2 controller는 cell 1개를 사용하여 디바이스 드라이버가 인자 1개를 받아 처리한다. 지정되지 않는 경우 2 cell 방식을 사용한다. gpio1: gpio1 { gpio-controller; #gpio-cells = \u0026lt;2\u0026gt;; }; gpio2: gpio2 { gpio-controller; #gpio-cells = \u0026lt;1\u0026gt;; } enable-gpios = \u0026lt;\u0026amp;gpio2 2\u0026gt;; data-gpios = \u0026lt;\u0026amp;gpio1 12 0\u0026gt;, \u0026lt;\u0026amp;gpio1 13 0\u0026gt;, \u0026lt;\u0026amp;gpio1 14 0\u0026gt;, \u0026lt;\u0026amp;gpio1 15 0\u0026gt;; Pin control subsystem과의 연동 pin control subsystem과의 연동은 gpio controller 노드에서 \u0026ldquo;gpio-ranges\u0026rdquo; 속성을 사용한다. \u0026ldquo;gpio-ranges\u0026rdquo; 속성이 가리키는 phandle은 연계된 pin controller 노드를 가리켜야 한다. 그리고 1~3개의 인자를 사용할 수 있으며, 배열 사용을 지원한다.\niomux: iomux@FF10601c { compatible = \u0026#34;abilis,tb10x-iomux\u0026#34;; reg = \u0026lt;0xFF10601c 0x4\u0026gt;; pctl_gpio_a: pctl-gpio-a { abilis,function = \u0026#34;gpioa\u0026#34;; }; pctl_uart0: pctl-uart0 { abilis,function = \u0026#34;uart0\u0026#34;; }; }; uart@FF100000 { compatible = \u0026#34;snps,dw-apb-uart\u0026#34;; reg = \u0026lt;0xFF100000 0x100\u0026gt;; clock-frequency = \u0026lt;166666666\u0026gt;; interrupts = \u0026lt;25 1\u0026gt;; reg-shift = \u0026lt;2\u0026gt;; reg-io-width = \u0026lt;4\u0026gt;; pinctrl-names = \u0026#34;default\u0026#34;; pinctrl-0 = \u0026lt;\u0026amp;pctl_uart0\u0026gt;; }; gpioa: gpio@FF140000 { compatible = \u0026#34;abilis,tb10x-gpio\u0026#34;; reg = \u0026lt;0xFF140000 0x1000\u0026gt;; gpio-controller; #gpio-cells = \u0026lt;2\u0026gt;; ngpios = \u0026lt;3\u0026gt;; gpio-ranges = \u0026lt;\u0026amp;iomux 0 0\u0026gt;; gpio-ranges-group-names = \u0026#34;gpioa\u0026#34;; }; ACPI 펌웨어를 사용하는 GPIO 매핑 디바이스 트리를 사용하는 방법과 유사하게 ACPI 디스크립션을 사용하는 방법이 있다. ACPI 5.1에서 소개된 [[[_DSD (Device Specific Data)|https://www.kernel.org/doc/Documentation/acpi/gpio-properties.txt]]를 참고한다.\nDevice (FOO) { Name (_CRS, ResourceTemplate () { GpioIo (Exclusive, ..., IoRestrictionOutputOnly, \u0026#34;\\\\_SB.GPI0\u0026#34;) {15} // red GpioIo (Exclusive, ..., IoRestrictionOutputOnly, \u0026#34;\\\\_SB.GPI0\u0026#34;) {16} // green GpioIo (Exclusive, ..., IoRestrictionOutputOnly, \u0026#34;\\\\_SB.GPI0\u0026#34;) {17} // blue GpioIo (Exclusive, ..., IoRestrictionOutputOnly, \u0026#34;\\\\_SB.GPI0\u0026#34;) {1} // power }) Name (_DSD, Package () { ToUUID(\u0026#34;daffd814-6eba-4d8c-8a91-bc9bbf4aa301\u0026#34;), Package () { Package () { \u0026#34;led-gpios\u0026#34;, Package () { ^FOO, 0, 0, 1, ^FOO, 1, 0, 1, ^FOO, 2, 0, 1, } }, Package () { \u0026#34;power-gpios\u0026#34;, Package () {^FOO, 3, 0, 0}, }, } }) Name (_CRS, ) } Platform 데이터에 GPIO 맵핑 (deprecated) 일부 시스템에서 아래의 매크로 함수, API를 사용하여 플랫폼 데이터에 저장한 후, 이를 lookup 하여 사용하는데 지금은 사용하지 않는 방법이다.\nGPIO_LOOKUP(chip_label, chip_hwnum, con_id, flag) GPIO_LOOKUP_IDX(chip_label, chip_hwnum, con_id, idx, flags) 아래와 같이 GPIO Lookup Table을 정의한 후에 아래와 같이 사용할 수 있다.\nstruct gpiod_lookup_table gpios_table = { .dev_id = \u0026#34;foo.0\u0026#34;, .table = { GPIO_LOOKUP_IDX(\u0026#34;gpio.0\u0026#34;, 15, \u0026#34;led\u0026#34;, 0, GPIO_ACTIVE_HIGH), GPIO_LOOKUP_IDX(\u0026#34;gpio.0\u0026#34;, 16, \u0026#34;led\u0026#34;, 1, GPIO_ACTIVE_HIGH), GPIO_LOOKUP_IDX(\u0026#34;gpio.0\u0026#34;, 17, \u0026#34;led\u0026#34;, 2, GPIO_ACTIVE_HIGH), GPIO_LOOKUP(\u0026#34;gpio.0\u0026#34;, 1, \u0026#34;power\u0026#34;, GPIO_ACTIVE_LOW), { }, }, }; gpiod_add_lookup_table(\u0026amp;gpios_table); struct gpio_desc *red, *green, *blue, *power; red = gpiod_get_index(dev, \u0026#34;led\u0026#34;, 0, GPIOD_OUT_HIGH); green = gpiod_get_index(dev, \u0026#34;led\u0026#34;, 1, GPIOD_OUT_HIGH); blue = gpiod_get_index(dev, \u0026#34;led\u0026#34;, 2, GPIOD_OUT_HIGH); power = gpiod_get(dev, \u0026#34;power\u0026#34;, GPIOD_OUT_HIGH); 출처 http://jake.dothome.co.kr/gpio-3/ ", 
        "url": "\/\/localhost:1313\/posts\/gpio-mapping\/"
    },
    
    "\/\/localhost:1313\/tags\/uevent\/": {
        "title": "Uevent",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/uevent\/"
    },
    
    "\/\/localhost:1313\/tags\/ueventd\/": {
        "title": "Ueventd",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/ueventd\/"
    },
    
    "\/\/localhost:1313\/posts\/%EC%95%88%EB%93%9C%EB%A1%9C%EC%9D%B4%EB%93%9C%EC%9D%98-uevent-ueventd\/": {
        "title": "안드로이드의 uevent, ueventd",
        "tags": ["uevent","ueventd",],
        "content": "개요 디바이스 트리와 카메라 포팅에 관련된 디바이스 드라이버 코드를 적절하게 수정한 뒤에도 계속해서 디바이스가 정상적으로 동작하지 않았다. 로그 상으로는 디바이스 OPEN에 실패하는 것으로 나왔는데, 원인은 다른 곳에 있었다. ueventd.rc 파일을 수정하지 않아 관련된 디바이스 노드 파일에 대한 권한이 설정되지 않았던 것이 문제였다.\n본 포스팅에서는 uevent와 ueventd가 무엇인지 살펴보고 안드로이드 init 과정에서 어떻게 활용되는지 살펴보기로 한다.\n여담으로 안드로이드의 uevent는 리눅스의 udev 와 비슷한 역할을 하면서도 조금 다르다. 리눅스의 일반적인 환경 구성이 devfs + udev 로 디바이스 노드 파일들을 관리한다면, 안드로이드는 ueventd를 이용하여 노드 파일들을 관리한다.\nuevent \u0026amp; ueventd 리눅스에서는 디바이스 노드 파일을 생성할 수 있도록 mknod 유틸리티를 제공하지만 안드로이드에서는 보안 문제로 이를 제공하지 않는다. 때문에, 안드로이드의 init 프로세스는 아래의 두 가지 방식으로 디바이스 노드를 생성한다.\nhot plug: 시스템 동작 중 디바이스 장치가 삽입될 때 이에 대한 이벤트 처리로 ueventd를 거쳐 해당 장치의 디바이스 노드 파일을 동적으로 생성한다. cold plug: 미리 정의된 디바이스 정보를 바탕으로 init 프로세스가 실행될 때 일괄적으로 디바이스 노드 파일을 생성한다. 출처에 따르면 cold plug 방식에 대해서, ueventd가 실행되기 전에 디바이스 드라이버가 /sys 디렉토리 아래에 디바이스 노드를 생성하기 위한 정보들을 저장한 후, ueventd가 실행되면서 디바이스 노드를 생성하지 못한 디바이스 드라이버에 대해서 강제로 uevent 를 발생시켜 cold plug 처리를 한다고 설명하고 있다.\ninit process와 ueventd 안드로이드 init 과정에서 ueventd를 부른다. ueventd 에서는 내부적으로 아래의 ueventd_main 함수를 호출한다.\nint ueventd_main(int argc, char **argv) { struct pollfd ufd; int nr; char tmp[32]; /* Prevent fire-and-forget children from becoming zombies. * If we should need to wait() for some children in the future * (as opposed to none right now), double-forking here instead * of ignoring SIGCHLD may be the better solution. */ signal(SIGCHLD, SIG_IGN); open_devnull_stdio(); klog_init(); INFO(“starting ueventd\\n”); /* Respect hardware passed in through the kernel cmd line. Here we will look * for androidboot.hardware param in kernel cmdline, and save its value in * hardware[]. */ import_kernel_cmdline(0, import_kernel_nv); get_hardware_name(hardware, \u0026amp;revision); ueventd_parse_config_file(“/ueventd.rc”); snprintf(tmp, sizeof(tmp), “/ueventd.%s.rc”, hardware); ueventd_parse_config_file(tmp); device_init(); ufd.events = POLLIN; ufd.fd = get_device_fd(); while(1) { ufd.revents = 0; nr = poll(\u0026amp;ufd, 1, -1); if (nr \u0026lt;= 0) continue; if (ufd.revents == POLLIN) handle_device_fd(); } } 여기서 중요한 함수는 ueventd_parse_config_file와 device_init함수이다. ueventd_parse_config_file함수는 ueventd.rc파일과 ueventd.%hardware%.rc 파일을 읽어 디바이스 노드 파일을 만드는 정보를 얻는다. 이 파일에 저장되어 있는 정보는 device 이름, permission, gid, uid 이다. 아래는 업무에서 사용한 실제 ueventd.rc 파일로서 문제가 된 videosource에 대한 내용들이 추가되어야 했다.\n/dev/switch_gpio_reverse\t0666\tsystem\tsystem #/dev/videosource* 로 가능 /dev/videosource0\t0666\tsystem\tsystem /dev/videosource1\t0666\tsystem\tsystem 이 때, 별도로 저장되어 있지 않는 디바이스는 디폴트로 600, 0, 0이 세팅된다. device_init 함수는 uevent_socket을 열고 coldboot 함수를 실행한다.\n여기서 연 소켓은 uevent를 보낼때 쓰이는 것이 아니라 나중에 발생한 uevent를 받을때 쓰인다.\nvoid device_init(void) { suseconds_t t0, t1; struct stat info; int fd; /* is 64K enough? udev uses 16MB! */ device_fd = uevent_open_socket(64*1024, true); if(device_fd \u0026lt; 0) return; fcntl(device_fd, F_SETFD, FD_CLOEXEC); fcntl(device_fd, F_SETFL, O_NONBLOCK); if (stat(coldboot_done, \u0026amp;info) \u0026lt; 0) { t0 = get_usecs(); coldboot(“/sys/class”); coldboot(“/sys/block”); coldboot(“/sys/devices”); t1 = get_usecs(); fd = open(coldboot_done, O_WRONLY|O_CREAT, 0000); close(fd); log_event_print(“coldboot %ld uS\\n”, ((long) (t1 – t0))); } else { log_event_print(“skipping coldboot, already done\\n”); } } 호출되는 coldboot는 내부적으로 do_coldboot를 호출한다.\nstatic void do_coldboot(DIR *d) { struct dirent *de; int dfd, fd; dfd = dirfd(d); fd = openat(dfd, “uevent”, O_WRONLY); if(fd \u0026gt;= 0) { write(fd, “add\\n”, 4); close(fd); handle_device_fd(); } while((de = readdir(d))) { DIR *d2; if(de-\u0026gt;d_type != DT_DIR || de-\u0026gt;d_name[0] == ‘.’) continue; fd = openat(dfd, de-\u0026gt;d_name, O_RDONLY | O_DIRECTORY); if(fd \u0026lt; 0) continue; d2 = fdopendir(fd); if(d2 == 0) close(fd); else { do_coldboot(d2); closedir(d2); } } } 디바이스 노드를 생성하지 못한 디바이스가 저장한 /sys 밑의 각각의 해당 폴더를 들어가 uevent 파일에 “add” 메시지를 써넣어 강제로 uevent를 발생시킨다. 그 후 handle_device_fd 함수를 통해 uevent 를 파싱해 디바이스 노드를 만든다. 이 과정에서 ueventd_parse_config_file 에서 얻어온 정보를 사용한다.\n출처 https://kshokd.wordpress.com/2012/08/29/init-%EA%B3%BC%EC%A0%95%EC%97%90%EC%84%9C-uevent%EC%99%80-ueventd%EC%9D%98-%ED%99%9C%EC%9A%A9/ ", 
        "url": "\/\/localhost:1313\/posts\/%EC%95%88%EB%93%9C%EB%A1%9C%EC%9D%B4%EB%93%9C%EC%9D%98-uevent-ueventd\/"
    },
    
    "\/\/localhost:1313\/posts\/dev\/mem-vs.-\/dev\/kmem\/": {
        "title": "\/dev\/mem vs. \/dev\/kmem",
        "tags": ["mem","kmem",],
        "content": "개요 업무에서 사용하는 디버깅 툴은 특정 레지스터 정보를 보기 위해, 메모리 상에 매핑되어 있는 주소에 접근하여 해당 레지스터의 값을 읽어오는 방식을 이용한다. 이 때, /dev/mem 디바이스 노드가 반드시 있어야 한다고 들었기에 Kconfig에서 관련 설정 플래그를 찾던 중 kmem 이라는 것도 있다는 것을 알게 되었다. 문득 이 둘의 차이점과 공식적(?)인 디버깅 툴이 어떤 것이 있는지 알아보고자 한다.\n/dev/mem vs. /dev/kmem 이 둘의 차이점은 출처에 따르면 아래와 같이 나와있다.\n/dev/mem is a device file that directly represents physical memory, so an open(/dev/mem)/seek(1000)/read(10) system call combination ends up reading 10 bytes from RAM address 1000.\n/dev/kmem is a device file that directly represents kernel virtual memory, so an open(/dev/kmem)/seek(1000)/read(10) system call combination ends up reading 10 bytes from virtual address 1000, which is in turn mapped by your system\u0026rsquo;s memory management unit to some physical RAM address.\n쉽게 요약하면, /dev/mem : /dev/kmem = physical memory : virtual memory 의 관계인 것이다.\n출처 https://www.quora.com/What-is-the-difference-between-dev-mem-and-dev-kmem https://lwn.net/Articles/147901/ ", 
        "url": "\/\/localhost:1313\/posts\/dev\/mem-vs.-\/dev\/kmem\/"
    },
    
    "\/\/localhost:1313\/tags\/kmem\/": {
        "title": "Kmem",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/kmem\/"
    },
    
    "\/\/localhost:1313\/tags\/mem\/": {
        "title": "Mem",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/mem\/"
    },
    
    "\/\/localhost:1313\/tags\/kobject\/": {
        "title": "Kobject",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/kobject\/"
    },
    
    "\/\/localhost:1313\/posts\/kobject\/": {
        "title": "Kobject",
        "tags": ["kobject",],
        "content": "kobjects, ktypes, ksets 디바이스 트리를 살펴보다 kobject 에 대한 내용이 언급되기 시작했다. 단순한 객체가 아니라 특수한 목적으로 사용될 것이라 예상되어 관련 내용을 찾아보았다.\n61 struct kobject { 62 const char *name; 63 struct list_head entry; 64 struct kobject *parent; 65 struct kset *kset; 66 struct kobj_type *ktype; 67 struct kernfs_node *sd; 68 struct kref kref; 69 #ifdef CONFIG_DEBUG_KOBJECT_RELEASE 70 struct delayed_work release; 71 #endif 72 unsigned int state_initialized:1; 73 unsigned int state_in_sysfs:1; 74 unsigned int state_add_uevent_sent:1; 75 unsigned int state_remove_uevent_sent:1; 76 unsigned int uevent_suppress:1; 77 }; 커널 문서(https://www.kernel.org/doc/Documentation/kobject.txt )에 따르면 kobjects와 더불어 ksets, ktypes 에 대해 아래와 같이 기술하고 있다.\nA kobject is an object of type struct kobject. Kobjects have a name and a reference count. A kobject also has a parent pointer (allowing objects to be arranged into hierarchies), a specific type, and, usually, a representation in the sysfs virtual filesystem.\nKobjects are generally not interesting on their own; instead, they are usually embedded within some other structure which contains the stuff the code is really interested in\nA ktype is the type of object that embeds a kobject. Every structure that embeds a kobject needs a corresponding ktype. The ktype controls what happens to the kobject when it is created and destroyed.\nA kset is a group of kobjects. These kobjects can be of the same ktype or belong to different ktypes. The kset is the basic container type for collections of kobjects. Ksets contain their own kobjects, but you can safely ignore that implementation detail as the kset core code handles this kobject automatically.\nKobject는 이름과 참조횟수를 가지고 있는 객체로서 자기 자신에 대해 관심을 가지는 것이 아니라 다른 구조체에 embedded 되어 관련 코드에서 필요로 하는 내용들을 저장하는 용도로 사용한다. kobjects 자체는 계층 구조로 구성할 수 있도록 부모 포인터를 가지며 특정 유형이나 sysfs 등에도 사용된다.\nktype은 kobject에 대한 도메인(정의역)이라 생각하면 이해하기 쉽다. 서로 다른 값들로 구성할 수 있는 도메인은 그 값에 대한 타입 시스템으로 구성할 수 있으며 이러한 타입시스템에 대한 개념이 여기에도 그대로 묻어나는 것을 네이밍을 통해 단번에 알 수 있다.\nkset은 kobject, 즉 단순한 집합이다. 때문에 서로 다른 ktype들을 가지고 있을 수도 있고 모두 같은 ktype들로 구성할 수도 있다.\n정리하면, kobjects, ksets, ktypes 들을 이용하여 커널 내에서 객체를 이용한 작은 타입시스템이라 생각할 수 있다. Functional Programming 에서 얘기하는 타입시스템을 이해하고 있다면 개념적으로 단번에 이해가 가능할 것이라 생각된다. 개념적으로 타입시스템을 구성하고 이를 sysfs 와 같은 파일 시스템에 적용하여, 해당 서브시스템(?)이 가질 수 있는 데이터들에 대한 타입시스템을 추상화하여 일관성 있게 구현하기 위해 고민한 흔적들을 엿볼 수 있다.\n알면 알수록, 모르는 게 정말 많다.\n출처 https://sonseungha.tistory.com/236 ", 
        "url": "\/\/localhost:1313\/posts\/kobject\/"
    },
    
    "\/\/localhost:1313\/tags\/big-endian\/": {
        "title": "Big Endian",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/big-endian\/"
    },
    
    "\/\/localhost:1313\/tags\/endian\/": {
        "title": "Endian",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/endian\/"
    },
    
    "\/\/localhost:1313\/tags\/little-endian\/": {
        "title": "Little Endian",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/little-endian\/"
    },
    
    "\/\/localhost:1313\/posts\/little-endian-vs.-big-endian\/": {
        "title": "Little Endian vs. Big Endian",
        "tags": ["big endian","little endian","endian",],
        "content": "개요 빅 엔디안과 리틀 엔디안에 관해 업무에서 접할 수 있는 상황과 각각에 해당하는 포인터 연산 예제를 종합적으로 정리하도록 한다.\n엔디안(Endianness)은 컴퓨터의 메모리와 같은 1차원 공간에 여러 개의 연속된 대상을 배열하는 방법을 뜻하며, 바이트를 배열하는 방법을 바이트 순서(Byte-order)라고 한다. 엔디안은 보통 큰 단위가 앞에 나오는 빅 엔디안(Big-Endian)과 작은 단위가 앞에 나오는 리틀 엔디안(Little-Endian)으로 나눌 수 있으며, 두 경우에 속하지 않거나 둘 모두 지원하는 것을 미들 엔디안(Middle-Endian)이라 부른다.\nBig-Endian: 최상위 바이트(MSB)부터 차례로 저장하는 방식 (사람이 읽고 쓰는 방식과 비슷함) Little-Endian: 최하위 바이트(LSB)부터 차례로 저장하는 방식 예제 예를 들어, 메모리에 0x12345678을 대입한다고 했을 때, 빅 엔디안과 리틀 엔디안 각각 아래와 같이 저장된다.\n빅 엔디안은 사람이 숫자를 사용하는 것과 같이 큰 단위의 바이트가 앞에 오는 방법이고 리틀 엔디안은 반대로 작은 단위의 바이트가 앞에 오는 방법이다.\n#include \u0026lt;stdio.h\u0026gt; int main(void) { unsigned long value = 0x12345678; unsigned char* ptr = \u0026amp;value; int i; for (i = 0; i \u0026lt; 4; i++) { fprintf(stdout, \u0026quot;value[%d] = 0x%x\\n\u0026quot;, i, *ptr++); } unsigned long long value2 = 0x12345678abcdefab; ptr = \u0026amp;value2; for (i = 0; i \u0026lt; sizeof(value2); i++) { fprintf(stdout, \u0026quot;value2[%d] = 0x%x\\n\u0026quot;, i, *ptr++); } return 0; } 위의 코드를 컴파일하여 Mac OS 환경에서 실행하면 아래와 같은 결과를 얻을 수 있다.\n~/Workspaces/study/languages/modernc/endian $ ./endian value[0] = 0x78 value[1] = 0x56 value[2] = 0x34 value[3] = 0x12 value2[0] = 0xab value2[1] = 0xef value2[2] = 0xcd value2[3] = 0xab value2[4] = 0x78 value2[5] = 0x56 value2[6] = 0x34 value2[7] = 0x12 0x12345678에서 MSB는 0x12, LSB는 0x78이며, LSB가 처음 나오는 것으로 보아 리틀 엔디안 방식으로 Byte-ordering을 하고 있는 것을 알 수 있다. unsigned long long의 경우로 확인할 수 있듯이 4바이트나 8바이트 단위로 byte-ordering 되는 것이 아니라 해당 데이터 타입에 따라 달라지는 것을 알 수 있다.\n장/단점 가독성 Big-Endian은 소프트웨어의 디버그를 편하게 해주는 경향이 있다. 사람이 숫자를 읽고 쓰는 방법과 같기 때문에 디버깅 과정에서 메모리의 값을 보기 편하다. 예를 들어, 0x59654148을 Big-Endian으로 표현하면 0x59, 0x65, 0x41, 0x48 등으로 메모리에 순서대로 표현된다.\n반대로 Little-Endian은 메모리에 저장된 값의 하위 바이트들만 사용할 때 별도의 계산이 필요 없다는 장점이 있다. 예를 들어, 32비트 숫자인 0x2A는 리틀 엔디언으로 표현하면 2A 00 00 00이 되는데, 이 표현에서 앞의 두 바이트 또는 한 바이트만 떼어내면 하위 16비트 또는 8비트를 바로 얻을 수 있다. 반면 32비트 빅 엔디안 환경에서는 하위 16비트나 8비트 값을 얻기 위해 변수 주소에 2바이트 또는 3바이트를 더해야 한다.\n커널 내 인터페이스 커널은 byte order 에 대한 의존성을 해결하기 위해 Type Identifier, Conversion Macro 등을 제공하고 있다. include/uapi/linux/types.h 헤더 파일 내에서는 아래와 같이 엔디안 별로 타입들이 정의되어 있는 것을 알 수 있다. 여기서 uapi 디렉토리는 커널의 userspace API를 포함하고 있다. (참고. https://stackoverflow.com/questions/18858190/whats-in-include-uapi-of-kernel-source-project )\n타입 정의 /* * Below are truly Linux-specific types that should never collide with * any application/library that wants linux/types.h. */ #ifdef __CHECKER__ #define __bitwise__ __attribute__((bitwise)) #else #define __bitwise__ #endif #define __bitwise __bitwise__ typedef __u16 __bitwise __le16; typedef __u16 __bitwise __be16; typedef __u32 __bitwise __le32; typedef __u32 __bitwise __be32; typedef __u64 __bitwise __le64; typedef __u64 __bitwise __be64; typedef __u16 __bitwise __sum16; typedef __u32 __bitwise __wsum; bitwise 속성(단순히 정수로써 사용되는 것을 제한하는데 사용)으로 정의되어 있는 Type Identifiers 들이다. bitwise 속성은 sparse 유틸리티(static analyzer)가 변수에 대한 연산을 수행하기 전에 로컬 프로세서로 변환될 수 있도록 보장한다.\nByte Order 알아내기 아래와 같이 간단한 user-space 프로그램을 작성하여 현재 시스템의 바이트 오더를 알아낼 수 있다.\nunion { int i; char c[sizeof(int)]; } foo; main() { foo.i = 1; if (foo.c[0] == 1) printf(\u0026quot;Little endian\\n\u0026quot;); else printf(\u0026quot;Big endian\\n\u0026quot;); } 다음에 소개되는 매크로는 변환 후의 값들을 반환한다.\n#include \u0026lt;linux/kernel.h\u0026gt; __u16\tle16_to_cpu(const __le16); __u32\tle32_to_cpu(const __le32); __u64\tle64_to_cpu(const __le64); __le16\tcpu_to_le16(const __u16); __le32\tcpu_to_le32(const __u32); __le64\tcpu_to_le64(const __u64); __u16\tbe16_to_cpu(const __be16); __u32\tbe32_to_cpu(const __be32); __u64\tbe64_to_cpu(const __be64); __be16\tcpu_to_be16(const __u16); __be32\tcpu_to_be32(const __u32); __be64\tcpu_to_be64(const __u64); 포인터에 대한 변환은 p를 붙여서 아래와 같이 사용하며, 현재 사용 중인 프로세서 엔디안 환경에 맞게 변환해주는 매크로도 아래와 같이 제공하고 있다.\n#include \u0026lt;linux/kernel.h\u0026gt; void le16_to_cpus(__u16 *); void le32_to_cpus(__u32 *); void le64_to_cpus(__u64 *); void cpu_to_le16s(__u16 *); void cpu_to_le32s(__u32 *); void cpu_to_le64s(__u64 *); void be16_to_cpus(__u16 *); void be32_to_cpus(__u32 *); void be64_to_cpus(__u64 *); void cpu_to_be16s(__u16 *); void cpu_to_be32s(__u32 *); void cpu_to_be64s(__u64 *); __u16\tle16_to_cpup(const __le16 *); __u32\tle32_to_cpup(const __le32 *); __u64\tle64_to_cpup(const __le64 *); __le16\tcpu_to_le16p(const __u16 *); __le32\tcpu_to_le32p(const __u32 *); __le64\tcpu_to_le64p(const __u64 *); __u16\tbe16_to_cpup(const __be16 *); __u32\tbe32_to_cpup(const __be32 *); __u64\tbe64_to_cpup(const __be64 *); __be16\tcpu_to_be16p(const __u16 *); __be32\tcpu_to_be32p(const __u32 *); __be64\tcpu_to_be64p(const __u64 *); 출처 http://www.bruceblinn.com/linuxinfo/ByteOrder.html https://stackoverflow.com/questions/18858190/whats-in-include-uapi-of-kernel-source-project ", 
        "url": "\/\/localhost:1313\/posts\/little-endian-vs.-big-endian\/"
    },
    
    "\/\/localhost:1313\/tags\/bt.601\/": {
        "title": "Bt.601",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/bt.601\/"
    },
    
    "\/\/localhost:1313\/tags\/bt.656\/": {
        "title": "Bt.656",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/bt.656\/"
    },
    
    "\/\/localhost:1313\/posts\/%EC%98%81%EC%83%81-%EC%B6%9C%EB%A0%A5-%ED%8F%AC%EB%A7%B7\/": {
        "title": "영상 출력 포맷",
        "tags": ["bt.601","bt.656",],
        "content": "디지널 출력 포맷 아날로그 신호에 대한 디지털 포맷으로서 표준화된 포맷들을 기술한다. 대표적인 표준으로는 BT.601, BT.656 이 있으며 그 외에도 최근에는 BT.2020 등의 고화질 영상을 타겟으로 한 표준들이 나오고 있다. 디지털 텔레비전의 부호화 파라미터들을 정의한 권고안으로서 601은 모든 컴포넌트 디지털 영상 표준에 대한 기초가 되었다.\n규격 내용 각 표준에는 아래와 같은 내용들을 정의한다.\n화면비 휘도 및 색 신호에 대한 처리 영상 포맷 형식 기준 주파수 샘플링 주파수 샘플링 및 코딩 형식 BT.601 Rec. 601 또는 BT.601, CCIR 601로 알려져있는 표준으로서 컬러 인코딩 시스템은 YCbCr 4:2:2 를 따른다.\nBT.601 의 경우 위 그림과 같이 HSYNC, VSYNC, DATA, PCLK 등의 필수 신호들로 통신하는 것을 알 수 있다.\nBT.656 위의 BT.601에서 HSYNC, VSYNC, DATA 선을 필요로 하는 것과 달리, BT.656에서는 DATA 라인에 타이밍 정보를 함께 포함해서 보내주는 것이 특징이다. 때문에 VSYNC, HSYNC 를 위해 필요한 핀을 절약할 수 있다는 장점이 있다.\nBT.709 \u0026amp; BT.2020 BT.709 는 방송용 출력 포맷(예. HDTV)으로 사용되며, 컬러 표현 영역이 작다는 것이 큰 단점이다. 이를 보완하기 위해 새로 나온 것이 BT.2020 이며, UHDTV를 위한 방송용 색 표현을 위한 표준이다. 업무에서 사용하는 SDTV 해상도와는 달리 컬러 색에 대한 표현 자체의 스펙이 다르기 때문에 특수한 카메라를 사용해야 한다. (카메라 예시: https://www.adorama.com/cac500m2.html )\nUHDTV 방송의 컬러 표현 출처 https://en.wikipedia.org/wiki/Rec._601 ", 
        "url": "\/\/localhost:1313\/posts\/%EC%98%81%EC%83%81-%EC%B6%9C%EB%A0%A5-%ED%8F%AC%EB%A7%B7\/"
    },
    
    "\/\/localhost:1313\/tags\/colorspace\/": {
        "title": "Colorspace",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/colorspace\/"
    },
    
    "\/\/localhost:1313\/tags\/rgb\/": {
        "title": "Rgb",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/rgb\/"
    },
    
    "\/\/localhost:1313\/tags\/yuv\/": {
        "title": "Yuv",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/yuv\/"
    },
    
    "\/\/localhost:1313\/posts\/%EC%83%89-%EA%B3%B5%EA%B0%84color-space\/": {
        "title": "색 공간(Color Space)",
        "tags": ["rgb","yuv","colorspace",],
        "content": "색 공간 포맷 업무 상에 사용되는 일반적인 색 인코딩 시스템은 RGB 와 YUV 이다. 그 중에서도 SoC의 각 컴포넌트에서 이미지 처리를 위해 기본으로 요구하는 포맷은 YUV 이다. YUV에는 서브샘플링 방법에 따라 YUV444, YUV422, YUV411, YUV420 등으로 세분화할 수 있는데 각 특징에 따라 실제 표현되는 색이 달라지므로 주의해야 한다.\n색 포맷에 대한 비트 구성은 표준에 따르지 않고 각 플랫폼 환경에 따라 달라지므로 SoC 데이터시트를 참고하여 컴포넌트에서 어떻게 구성되는지 확인해야 한다.\nRGB RGB의 대표적인 포맷으로는 ARGB8888, RGB888 등이 있다. 각 채널 별로 2 bytes 씩이다. R/G/B 각각의 색상 정보에 대한 모든 정보를 가지고 있기 때문에 색 표현에 있어서 정확하지만 픽셀 한 개를 위해 필요한 비트가 최소 24비트나 차지하기 때문에 데이터 전송 면에 있어서 YUV 포맷보다 비효율적이다.\nRGB 8 BITS RGB 8비트는 모든 색상 중에서 256개만을 선택하여 사용하며 모든 색상을 한번에 다 표현하지 못하기 때문에 팔레트라는 개념을 사용한다. 8비트 색상 정보는 256개의 팔레트 정보를 가지고 있고, 1 바이트의 점을 표현하는데, 이 때 1 바이트에 해당하는 것을 인덱스 컬러라고 하며, 어떤 팔레트인지를 나타낸다.\nRGB 16 BITS 8비트 RGB와는 달리 팔레트 개념이 없으며 RGB 요소를 공평하게 갖기 위해 5 비트씩 구성하거나 G 색상에 6 비트를 할당하여 RGB555, RGB565 등으로 나뉜다.\nRGB 24 BITS 16비트 RGB에서 5비트씩 나눈 것을 8비트씩 나눈 포맷이다.\nRGB 32 BITS Alpha 채널이 추가된 RGB 형식으로 ARGB8888, RGBA8888 등의 포맷이 있다.\nYUV 색상을 나타내기 위해 삼원색을 표현하는 RGB 방식과 달리 빛의 밝기를 나타내는 휘도(Y)와 색상 신호(U, V)로 표현하는 방식이다. 티비나 비디오 카메라에서 많이 사용하는 방식이며, 사람의 눈이 색상 신호보다 밝기 신호에 더 둔하다는 점을 고려하여 만든 색 공간이기에 실제 RGB 신호와 YUV 신호의 차이를 잘 느끼지 못한다.\nYUV는 Packed format 과 Planar format 등으로 아래와 같이 나눌 수 있다.\nPacked format: Y와 UV가 섞여 macro pixel을 이루는 방식 (YUYV a.k.a. {YUY2, YUNV, V422}, UYVY a.k.a. {Y422, UYNV})\nPlanar format: Y, UV가 각각의 다른 영역에 나뉘어 픽셀 정보를 이루는 방식 (YUV422, YUV420(NV12, NV21))\nRGB-to-YUV 변환 YUV 데이터는 일반적으로 RGB 데이터를 변환하여 얻을 수 있으며, 일반적인 변환 공식은 아래와 같다. 변환 공식은 영상 출력에 사용하는 표준 또는 포맷에 따라 달라질 수 있다. BT.601의 경우 아래와 같다.\n$$ W_R = 0.299, \\\\ W_G = 1 - W_R - W_B = 0.587, \\\\ W_B = 0.114,\\\\ U_{max} = 0.436,\\\\ V_{max} = 0.615$$\n이 때, RGB 채널 각각에 대한 가중치를 이용하여 Y, U, V 채널 값을 아래와 같이 구할 수 있다.\n$$Y\u0026rsquo; = W_RR + W_GG + W_BB = 0.299R + 0.587G + 0.114B,\\\\U = U_{max}((B - Y\u0026rsquo;)/(1 - W_R)) =~ 0.492 (B - Y\u0026rsquo;),\\\\V=V_{max}(R-Y\u0026rsquo;)/(1-W_R) = 0.877(R-Y\u0026rsquo;)$$\nBT.809의 경우는 R과 B에 대한 가중치를 아래와 같이 갖는다.\n$$W_R=0.2126, W_B=0.0722$$\n일반적인 RGB-to-YUV 예제 출처에서 가져온 변환 예제로 사용하는 표준, 포맷, 환경에 따라 가중치를 다르게 하여 변환할 수 있다.\nSub-Sampling 에 따른 포맷 같은 YUV 포맷이라도 해도, 이를 다시 서브샘플링하여 압축함으로써 전송 효율을 높일 수 있다. 이 때, YUV 의 각 채널에 대한 기호로 아래와 같이 나타낸다.\n$$Y: Y, \\\\U: P_B(orC_B)\\\\V:P_R(orC_R)$$\nYUV444 원본 YUV 색상으로서 Y가 4 바이트 사용될 때 U와 V 채널에도 각각 4바이트를 사용한다. 왼쪽에서 오른쪽으로 픽셀 배열이 있다고 가정했을 때, 세로로 표현한 픽셀 한 개를 구성하는 각 채널은 Y\u0026rsquo;, Cb, Cr 모두 동일한 비율로 구성되어 있다. 메모리 상으로 살펴 보았을 때, YCbCr, YCbCr, ... 등이 된다.\nYUV422 YUV444와 달리 Cb, Cr 채널에 대해 픽셀 두 개에 대한 Cb, Cr 정보를 한 개로 하여 서브샘플링한 것을 나타낸다. 즉, Y 값이 두 개 올 때 나머지 채널은 1개씩 오게 되어 결국 4:2:2 의 비율을 갖게 된다. 메모리 상으로 살펴 보았을 때, YCbYCr, YCbYCr, ... 등이 된다.\nYUV411 이전까지 다뤘던 내용처럼 밝기 신호인 Y가 4바이트 올 때 U, V 신호가 각각 1바이트씩 위치한다.\n메모리 상으로 살펴보면 YYYCb, YYYCr, YYYCb, YYYCr, \u0026hellip; 등이 된다. 밝기에 비해 색상의 해상도가 1/4로 떨어진다.\nYUV420 안드로이드 카메라 클래스에서 제공하는 YUV 데이터 포맷이다. 12비트 데이터 포맷으로 4가지 종류 (YV12, NV12, IMC2, IMC4 등)가 존재하며, Y가 4개 오면 U와 V가 1바이트씩 위치하며 4개의 Y값이 U와 V값을 공유한다. YYYYCb, YYYYCr, YYYYCb , YYYYCr, \u0026hellip; 등의 메모리 구성을 갖는다.\n출처 https://m.blog.naver.com/PostView.nhn?blogId=wndrlf2003\u0026amp;logNo=220253497246\u0026amp;proxyReferer=https%3A%2F%2Fwww.google.com%2F https://en.wikipedia.org/wiki/YUV#Y′UV411_to_RGB888_conversion https://imagej.tistory.com/150 https://seoduckchan.tistory.com/entry/yuv-color ", 
        "url": "\/\/localhost:1313\/posts\/%EC%83%89-%EA%B3%B5%EA%B0%84color-space\/"
    },
    
    "\/\/localhost:1313\/about\/": {
        "title": "평범한 개발자",
        "tags": [],
        "content": " 이맥스를 사용합니다. Gentoo Linux를 사랑하지만 WSL를 사용합니다. FSF의 철학을 좋아합니다. 인성도 중요한 스킬이라 생각합니다. ", 
        "url": "\/\/localhost:1313\/about\/"
    },
    
    "\/\/localhost:1313\/tags\/bootloader\/": {
        "title": "Bootloader",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/bootloader\/"
    },
    
    "\/\/localhost:1313\/tags\/embedded_linux\/": {
        "title": "Embedded_linux",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/embedded_linux\/"
    },
    
    "\/\/localhost:1313\/posts\/%EB%B6%80%ED%8A%B8%EB%A1%9C%EB%8D%94bootloader\/": {
        "title": "부트로더(Bootloader)",
        "tags": ["bootloader","embedded_linux",],
        "content": " 이 포스팅의 내용은 임베디드 리눅스에서의 부트로더 역할에 대해 중점적으로 다룬다. 부트로더는 시스템을 초기화하고 임베디드 운영체제를 탑재하거나 실행하기 위해서 시스템 초기화 코드, 하드웨어 제어 프로그램, 네트워크, USB 등의 프로토콜과 일부 파일 시스템을 관리한다.\n브투로더는 개인용 컴퓨터의 BIOS(Basic Input Output System)와 유사하다고 생각하면 된다. BIOS는 처음 전원을 공급하면 메인보드의 CMOS에 저장된 설정값을 읽어서 하드웨어를 초기화하고 그래픽 카드 정보 및 바이오스 정보를 출력한 다음 POST(Power-On Self Test) 과정을 수행하고 하드디스크나 CD-ROM 또는 플로피디스크에서 운영체제를 찾아 부팅한다.\n부트로더의 역할 타깃 초기화 부트로더는 전원이 입력되면 타깃이 정상 동작하도록 하드웨어 및 소프트웨어 환경을 설정한다. 이 과정은 시스템 리셋 핸들러의 동작과 같다. 불필요한 하드웨어의 동작 중지, 시스템 클록 설정, 메모리 제어기 설정, 필요에 따라 MMU(Memory Management Unit)나 MPU(Micro-Processing Unit)를 설정한다.\n시스템 동작에 필요한 하드웨어 설정이 완료된 후에는 실제 프로그램 동작에 필요한 재배치와 스택 영역 설정 및 C에서 사용하는 변수 영역을 설정한 다음 C로 작성된 함수를 호출한다. 필요에 따라 IRQ와 같은 예외 처리 벡터 및 핸들러도 제공해야 한다.\n타깃 동작 환경 설정 타깃의 동작 환경 설정은 BIOS의 CMOS 설정과 유사하며, 부트 방법, 부트 디바이스, 네트워크를 이용한 부트를 지원하기 위한 네트워크 설정, IP 주소 설정 등 부트로더 동작에 필요한 정보를 설정한다.\n일반적으로 설정된 환경 변수값은 플래시 메모리 또는 EEPROM에 저장 관리된다.\nEEPROM vs. 플래시 메모리\n이 둘의 공통점은 임베디드 분야에서 저장장치로 사용되는 메모리라는 점이다. 흔히, PC에 비유해보자면 EEPROM을 BIOS로, 플래시 메모리는 하드디스크로 비유할 수 있겠다.\nEEPROM(Electrically Erasable PROM)은 On-Board 상태에서 사용자가 내용을 바이트 단위로 읽거나 쓸 수 있으므로 사실상 SRAM 처럼 사용할 수 있는 비휘발성 메모리이다. 하지만 읽기 동작은 다소 느릴지라도 SRAM과 유사하여 별 문제가 없지만 쓰기 동작을 수행하는 경우에 1 바이트를 write 할 때마다 수 ms 이상의 지연 시간이 필요하여 SRAM과 동일하게 사용될 수 없다. 따라서 EEPROM은 실시간으로 사용되는 변수를 저장하는 메모리로써는 사용 불가능하고 지워져서는 안되는 중요한 데이터를 백업해 둬야 하는 설정값 저장용 메모리로 주로 사용된다. 이 메모리 소자는 28Cxxx의 형태로 이름을 짓는 경우가 많다.\n플래시 메모리는 흔히 디지털 카메라, 스마트폰을 예로 들 수 있다. 일반적인 데이터를 저장하고 컴퓨터 사이에 데이터를 옮기는 용도로서 USB 드라이브를 사용하는데 이 때도 플래시 메모리가 사용된다. 임베디드용 하드디스크라고 생각하면 쉬우며, NOR 플래시 메모리와 달리 NAND 플래시 메모리의 경우는 주소가 없기 때문에 저장된 코드를 곧바로 실행할 수 없다는 단점을 가지고 있다.\n시스템 운영체제 부팅 일반적으로 임베디드 시스템의 운영체제는 플래시 메모리에 탑재되어 있으므로 부팅 과정에서 주메모리(일반적으로 DRAM 사용)에 탑재한 후 실행한다. 따라서 부트로더는 플래시 메모리에 있는 운영체제를 DRAM에 복사하고 제어권을 운영체제의 시작점으로 넘겨주는 기능을 제공한다.\n플래시 메모리 관리 임베디드 시스템에서 플래시 메모리는 다양한 용도로 사용되기 때문에 가장 효율적인 보조 기억장치라고 할 수 있다. 보통 플래시 메모리에 부트로더가 탑재된다.\n모니터 기능 모니터 기능이란 시스템의 동작 상태를 감시하고 하드웨어 정상 동작 여부 검사, 메모리 검사를 비롯한 POST(Power-On Self Test) 등의 기능을 말한다. 부트로더는 필요에 따라 이와 같은 모니터 기능을 가진다.\n부트로더 특징 부트로더는 하드웨어 의존성이 강하다. 일반적으로 클록, 메모리 제어기 설정 등의 하드웨어 설정을 먼저 수행한다. 따라서 부트로더를 작성하려면 프로세서 구조와 특징 및 사용법을 알아야 한다. 특히 부트로더의 시작 부분은 어셈블리어로 작성되기 때문에 명령어 사용법을 알고 있어야 한다.\n또한, 부트로더 콘솔을 구현하기 위해서는 UART 제어기의 구조와 제어 방법을 알고 있어야 하며 네트워크, USB를 이용하려면 이더넷 제어기 및 USB 디바이스 제어기에 대해서도 알아야 한다.\n하지만 직접 구현은 힘들기 때문에 LILO, GRUB, Loadlin, EtherBoot, Blob, PMON, RedBoot, U-Boot과 같은 부트로더를 이용한다.\n출처 https://treeroad.tistory.com/entry/Flash-Memory%EC%99%80-EEPROM-%EC%B0%A8%EC%9D%B4%EC%A0%90 ", 
        "url": "\/\/localhost:1313\/posts\/%EB%B6%80%ED%8A%B8%EB%A1%9C%EB%8D%94bootloader\/"
    },
    
    "\/\/localhost:1313\/posts\/command-posix_spawn-failed-resource-temporarily-unavailable\/": {
        "title": "command: posix_spawn failed: Resource temporarily unavailable",
        "tags": ["posix-spawn",],
        "content": "QEMU를 맥에서 빌드하던 중 아래와 같은 에러가 출력되었다.\n이를 위한 해결 방법은 링크 에서 쉽게 찾을 수 있었는데 한번에 실행할 수 있는 프로세스의 개수 제한이 너무 낮아 발생하는 문제라고 한다. 이 때 아래와 같이\nsudo sysctl -w kern.maxproc=2500 sudo sysctl -w kern.maxprocperuid=2500\n명령어를 통해 제한값을 높여주면 해결된다. 맥은 설치는 쉬운데 왜이리도 설정해줘야 많은지 모르겠다.\n", 
        "url": "\/\/localhost:1313\/posts\/command-posix_spawn-failed-resource-temporarily-unavailable\/"
    },
    
    "\/\/localhost:1313\/tags\/posix-spawn\/": {
        "title": "Posix-Spawn",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/posix-spawn\/"
    },
    
    "\/\/localhost:1313\/tags\/arm-versatile\/": {
        "title": "Arm-Versatile",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/arm-versatile\/"
    },
    
    "\/\/localhost:1313\/tags\/qemu\/": {
        "title": "Qemu",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/qemu\/"
    },
    
    "\/\/localhost:1313\/posts\/qemu-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EC%9E%84%EB%B2%A0%EB%94%94%EB%93%9C-%EA%B0%9C%EB%B0%9C-%ED%99%98%EA%B2%BD-%EA%B5%AC%EC%B6%95%ED%95%98%EA%B8%B0\/": {
        "title": "QEMU 이용한 임베디드 개발 환경 구축하기",
        "tags": ["arm-versatile","qemu",],
        "content": "임베디드 분야로 공부를 하기 위해 보드를 구매하였지만, 보드 없이도 가상으로 개발하는 방법은 없을까 고민하던 중 QEMU를 이용하는 방법을 찾게 되었다. 특정 프로세서에 대한 에뮬레티어 기능을 이용해 단순 커널 분석을 위해 사용할 수 있을 뿐만 아니라, machine 자체도 에뮬레이팅이 가능하기 때문에 임베디드 쪽으로도 많이 사용된다고 한다.\n개발 환경 구축을 위해서는 리눅스가 필수인데 필자는 여건상 어쩔 수 없이 맥에서 리눅스 도커 이미지를 사용하는 방식으로 하여 크로스 빌드는 컨테이너에서, 빌드된 이미지는 호스트인 맥에서 실행하는 방식으로 진행하였다.\n이번 포스팅은 아래와 같은 순서로 주관적이기는 하지만 가능한 자세하게(?) 기술하고자 한다.\n\u0026lsquo;Buildroot\u0026rsquo; 이용하여 qemu용 arm versatile 이미지 빌드 가상 디바이스 추가하기 추가한 디바이스 드라이버 구현 및 동작 확인 ", 
        "url": "\/\/localhost:1313\/posts\/qemu-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EC%9E%84%EB%B2%A0%EB%94%94%EB%93%9C-%EA%B0%9C%EB%B0%9C-%ED%99%98%EA%B2%BD-%EA%B5%AC%EC%B6%95%ED%95%98%EA%B8%B0\/"
    },
    
    "\/\/localhost:1313\/tags\/rcu\/": {
        "title": "RCU",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/rcu\/"
    },
    
    "\/\/localhost:1313\/posts\/rcu-read-copy-update\/": {
        "title": "RCU (Read, Copy, Update)",
        "tags": ["RCU",],
        "content": "1. 들어가기 전에 \u0026hellip; 앞으로 기술하는 내용은 http://jake.dothome.co.kr/rcu/#comment-214230 에서 발췌하여 정리하는 내용이므로 원본 내용 확인을 위해서는 링크에서 직접 보길 권한다. RCU 내용 뿐만 아니라, 리눅스 커널 전반적인 내용에 대해 정말 자세하게 정리가 잘 되어있다.\nRCU(Read, Copy, Update)란 리눅스 커널 내에서 주로 읽기 연산만 일어나고 쓰기 연산의 비중은 매우 작은 객체에 주로 쓰이는 동기화 기법이다. Reader-Writer 락과 비슷한 동기화 기법인데, RW 락에 대해 RCU가 가지는 상대적인 강점으로는 읽기 연산이 wait-free(읽기 연산에 대해 Block이 일어나지 않음)이며 그 오버헤드가 극도로 작다는 점 등이 있다. 대신 쓰기 연산의 오버헤드가 상당하고 쓰기에 필요한 동기화의 시간 복잡도가 최소한 RCU로 보호되는 객체의 크기에 비례한다.\n2. RCU란? 불변 객체(Immutable Object)에 대한 쓰레드는 안전하다. 즉, 어떤 코드 영역을 수행하는 동안 공유되는 메모리 구간이 불변인 경우라면, 코드로 인한 상태의 변화는 항상 예측 가능하므로 코드 수행의 결과가 항상 결정적(Deterministic)이다.\n메모리 배리어 등을 통해 가시성이 확보된 경우 단일 워드에 대한 쓰기 연산은 대개 원자적이다.\n어떤 객체의 상태를 변경하는 코드를 수행할 때 그 변경이 다른 쓰레드의 입장에서 한 순간에 원자적으로 이루어지는 것처럼 보인다면, 연관된 코드들의 수행 내역은 단일 쓰레드로 재현 가능하다. 쓰레드 간 코드 수행 순서에 대한 구체적인 합의가 필요한 경우는 드물기 때문에 대부분 이 정도만 보장되어도 동기화로서 충분한 역할을 할 수 있다.\nGC(Garbage Collector)를 지원하는 요즘 언어들은 객체를 직접 변경하는 대신 불변 객체를 새로 만들어서 사용한다. 예를 들어, 자바의 경우는 문자열을 변경할 경우, 변경된 결과의 불변 문자열을 가져와 새로운 인스턴스를 할당하여 사용한다(String Internalization). 즉, 기존의 객체를 변경하는 대신 새로운 불변 객체를 만들어 쓰레드의 안전성을 쉽게 확보하는 것이 RCU의 기본 개념이다.\nRCU로 보호되는 객체에 대한 읽기를 하려고 하는 경우, 평소처럼 포인터를 가져와 읽기 전용으로 해당 객체를 다루면 된다. 하지만 RCU로 보호되는 객체를 변경하려는 경우, 객체를 직접 변경하는 대신 새로 할당한 메모리 영역에 객체를 복사한 뒤 새로 만든 객체를 변경한다. 기존의 객체는 불변이므로 이 과정은 안전하다.\n하지만 실제로 GC와 달리 직접 관련된 자원을 해제해 관리해줘야 하는 경우 Reader 입장에서 더 이상 접근할 방도가 없는 객체인 경우만 해제해주면 되지만 이를 위해서는 레퍼런스 카운팅이 필요하다. 즉, 읽기 영역에 진입/이탈 시에 원자적인 정수 연산이 적어도 한 번씩은 일어나는데 리눅스 커널에서는 이를 간단하게 해결한다.\nRCU는 커널 코드 내에서만 사용되며 커널 모드에서 수행이 끝나면 RCU의 읽기 구간 역시 끝나게 된다. 선점형 커널을 사용하고 있는 경우 문맥 교환이 한번씩 이루어지고 비선점형 커널인 경우 읽기가 끝나면 모든 동작들이 종료된 것이므로 메모리를 안전하게 해제할 수 있다.\n3. RCU의 장/단점 RCU는 read-side overhead를 최소화하는데 목적이 있기 때문에 동기화 로직이 읽기 동작에 더 많은 비율로 사용되는 경우만 사용한다. 수정 동작이 10% 이상인 경우에는 오히려 성능이 떨어지므로 RCU 대신 다른 동기화 기법을 선택해야 한다.\n3-1. 장점 read-side overhead가 거의 없다. (zero wait, zero overhead) Deadlock 이슈 없음 Priority Inversion 이슈 없음 Unbounded Latency 이슈 없음 Memory Leak Hazard 이슈 없음 3-2. 단점 사용이 복잡하다. 쓰기 동작에서 다른 동기화 기법보다 느리다. 4. 기본 요소 및 개념들 커널 문서를 읽기에 앞서 기본 요소에 대해서 미리 알고 읽는 것이 훨씬 낫다. 이름이 낯설기 때문에 관련 용어들에 대해 미리 알아야 한다. RCU는 아래와 같이 3가지 기본 요소와 특징이 있다.\n4-1. Reader Reader는 rcu_read_lock()과 rcu_read_unlock() 코드 범위의 Read-side critical section이다.\nRCU에서 보호해야 하는 데이터에 접근할 때 항상 이 영역 내에서만 접근해야 한다는 점이다. 이 영역을 벗어난 사용은 RCU 규칙을 벗어난 것이며 존재하지 않는 데이터 또는 잘못된 데이터로의 접근이 될 수 있다. 그리고 반드시 보호해야 할 데이터에 접근하기 전, 메모리 배리어를 사용한 후에 참조해야 한다.\n4-2. Updater 기존의 여러가지 락 중 하나를 사용하여 데이터를 보호하고 수정한다.\n4-3. Reclaimer Updater에서 최종 수정한 객체가 아닌 사용이 완료된 객체들에 대해 메모리 해제하는 작업을 한다. 이 때, 안전하게 메모리 해제가 되도록 삭제할 데이터에 접근하는 Reader들이 없음을 보장하기 위해 반드시 Grace Period가 지난 후에 Reclaimer가 동작하도록 한다.\n4-4. Grace Period (GP) Grace Period를 사전에서 찾아보니 유예기간이라는 의미로 해석되었다. 한 마디로 정리하면, 동기화를 위해 업데이트 해야 할 내용을 곧바로 반영하지 않고 기존 Reader들이 읽기를 모두 완료할 때까지 기다리는 시간이다.\n사용자는 특정 자료의 동기화를 위해 두 가지의 관점에서 처리한다. 읽기 처리만 수행하는 read-side critical section이 있고 이를 Reader라고 부른다. 변경 처리가 포함된 write-side critical section이 있고 이를 Writer 또는 Updater라고 부른다. 이 때, RCU에서는 업데이트가 발생하는 시점부터 각 처리 과정을 아래와 같이 세 가지 단계(Removal, Grace Period, Reclamation)로 구분한다.\n1단계. Removal write-side critical section 구간에서는 데이터를 읽은 뒤 사본을 만들고 해당 사본을 변경한다. Removal(Read-Copy-Update)이 진행되는 동안 해당 데이터에 접근하고 있는 Reader들을 보호하기 위해 기존에 접근하고 있는 자료는 곧바로 수정하지 않고 이를 Copy한 후 Update하여 사용한다. 즉, Reader들을 원본 자료에만 접근하므로 데이터에 안전하게 접근할 수 있다.\n2단계. Grace Period Removal 단계의 데이터에 관련된 Reader가 완료됨을 보장할 수 있도록 대기하는 단계이다. 앞서 설명한 1단계(Removal)에서 생성된 사본을 원본에 반영해야 하는데, 곧바로 반영하면 각 cpu가 동시에 처리하고 있는 Reader에 문제가 생긴다. 따라서, Removal 기간 동안 같은 자료에 접근한 Reader의 처리가 모두 완료될 때까지 기다려야 하는데 이 구간을 GP(Grace Period)라고 한다.\n3단계. Reclamation 사본을 원본에 적용하는 구간이다. 2단계 GP의 완료를 인지하면 사본을 원본에 반영하고 필요 없어진 기존 데이터는 폐기하는 Reclamation 구간이 시작된다. 이러한 처리들은 RCU의 후 CB(Call-Back) 함수를 리스트에 보관해 두었다가 Reclamation 구간에서 호출하여 처리한다.\n그렇다면, Reader의 모든 처리가 완료되는 시점을 어떻게 알 수 있을까? 출처에 따르면, Grace Period 자체는 개념적인 의미이고 실제로는 Min Grace Period라고 불린다고 한다. Removal 기간 중 관련된 Reader 들의 끝 부분이 곧바로 인식되지 않으므로 Grace Period가 끝나는 시점을 감지하는 방법을 사용한다는데, 사실 직감적으로는 이해가 되지 않는다.\n4-5. QS(Quiescent State) QS(Quiescent State)의 quiescent는 \u0026lsquo;진행이 중단된, 조용한, 잠잠한\u0026rsquo;이라는 뜻을 가지고 있다. 이 상태는 Reader 간의 사이 시간 상태로 GP 구간 전에 이미 진행되고 있었던 Reader 의 구간이 끝난 것을 인지할 목적으로 사용된다. QS 상태가 지났는지 여부는 아래와 같은 방법들을 사용한다.\nrcu_sched: 각 CPU에서 태스트 간 호출되는 스케쥴 틱을 이용하여 인터럽트 구간을 이용하는 방법\nContext Switch 유저 모드 실행 후 스케쥴 틱 진입 Dynticks or Idle rcu_bh: softirq를 사용하는 방법\n커널에서 실행되는 Reader가 Idle 태스크를 수행한다는 것은 이미 문맥교환(Context Switch)를 완료했다는 의미이므로 해당 CPU에서 Reader의 완료를 보장한다. 개념 수준에서 QS(Quiescent State)는 Reader 간의 시간으로 기존 Reader의 처리가 완료되었음을 보장하는 기간이다.\n출처 https://www.kernel.org/doc/Documentation/RCU/whatisRCU.txt https://summerlight.tistory.com/entry/%EB%8F%99%EA%B8%B0%ED%99%94-%EA%B8%B0%EB%B2%95-Read-Copy-Update http://jake.dothome.co.kr/rcu/#comment-214230 ", 
        "url": "\/\/localhost:1313\/posts\/rcu-read-copy-update\/"
    },
    
    "\/\/localhost:1313\/tags\/bkl\/": {
        "title": "BKL",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/bkl\/"
    },
    
    "\/\/localhost:1313\/posts\/%EB%A6%AC%EB%88%85%EC%8A%A4-%EC%BB%A4%EB%84%90-%EB%9D%BD-%EC%A2%85%EB%A5%98-4\/5\/": {
        "title": "리눅스 커널 락 종류 (4\/5)",
        "tags": ["BKL",],
        "content": "BKL(Big Kernel Lock)은 커널 2.0에서 SMP와 함께 소개된 락으로서 Giant-Lock, Big-Lock 또는 Kernel-Lock 으로 알려졌었다. 2.0 버전의 커널에서는 한 번에 하나의 스레드만이 커널 모드에서 동작하기 위해 락(Lock)을 획득해야 커널 모드로 진입되었고 나머지 프로세서들은 락을 획득하기 위해 대기한다. 하지만 이 후, 성능, 실시간 애플리케이션에 대한 latency 이슈로 BKL(Big Kernel Lock)은 스핀락과 뮤텍스, RCU 등으로 대체되면서 현재는 거의 관련 코드가 제거되어 있는 상태이다.\nlock_kernel(): Acquires the BKL unlock_kernel(): Releases the BKL kernel_locked(): Returns nonzero if the lock is held and zero otherwise (UP always returns nonzero) BKL은 프로세서들이 동시에 커널에 진입하는 것을 막아 동기화 문제를 해결한다.\n출처 http://jake.dothome.co.kr/bkl/ https://www.kernel.org/doc/Documentation/RCU/whatisRCU.txt ", 
        "url": "\/\/localhost:1313\/posts\/%EB%A6%AC%EB%88%85%EC%8A%A4-%EC%BB%A4%EB%84%90-%EB%9D%BD-%EC%A2%85%EB%A5%98-4\/5\/"
    },
    
    "\/\/localhost:1313\/tags\/semaphore\/": {
        "title": "Semaphore",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/semaphore\/"
    },
    
    "\/\/localhost:1313\/posts\/%EB%A6%AC%EB%88%85%EC%8A%A4-%EC%BB%A4%EB%84%90-%EB%9D%BD-%EC%A2%85%EB%A5%98-3\/5\/": {
        "title": "리눅스 커널 락 종류 (3\/5)",
        "tags": ["semaphore",],
        "content": "이번 포스팅에서는 리눅스 커널 락의 세 번째인 세마포어에 대해 기술한다. 리눅스의 세마포어(semaphore)는 휴면하는 락이라고 생각하면 된다. 태스크가 이미 사용 중인 세마포어를 얻으려고 하면, 세마포어는 해당 태스크를 대기큐에 넣고 휴면 상태로 만든다. 그 다음 프로세서는 자유롭게 다른 코드를 실행한다. 세마포어가 다시 사용 가능해지면 대기큐의 태스크 하나를 깨우고 이 태스크가 세마포어를 사용하게 된다.\n세마포어는 아래와 같은 경우에 적합하다.\n태스크, 즉 프로세스의 상태는 TASK_RUNNING, TASK_INTERRUPTIBLE, TASK_UNINTERRUPTIBLE 로 나눌 수 있으며, 이 중 TASK_INTERRUPTIBLE 상태가 프로세스가 휴면에 진입한 상태로 프로세스를 깨우면 다시 TASK_RUNNING(실행 대기) 상태로 변경된다. 실행 대기 중인 프로세스는 CPU에 attach 되면 실행되며 이 때의 상태값 또한 TASK_RUNNING이라는 점에 유의하자.\n세마포어 특징 락을 기다리는 태스크가 휴면 상태(TASK_INTERRUPTIBLE)로 전환되므로, 세마포어는 오랫동안 락을 사용하는 경우에 적합하다. 반대로 락 사용 시간이 짧은 경우에는 휴면 상태 전환 및 대기큐 관리, 태스크 깨우기 등의 부가 작업을 처리하는 시간이 락 사용을 넘어설 수 있기 때문에 세마포어 사용이 적절하지 않다. 락이 사용 중이면 실행 스레드가 휴면 상태로 전환되기 때문에 스케쥴링이 불가능한 인터럽트 컨텍스트가 아닌 프로세스 컨텍스트에서만 세마포어를 사용할 수 있다. 다른 프로세스가 같은 세마포어를 얻으려 하는 경우라도 데드락에 빠지는 일이 발생하지 않으므로 락을 잡으려는 다른 프로세스도 휴면 상태로 전환되므로 결국은 실행이 계속된다. 세마포어를 얻기 위해서는 휴면 상태로 전환될 수 있는데, 스핀락이 걸린 상태에서는 휴면 상태가 될 수 없으므로 세마포어를 얻으려고 할 때에는 스핀락이 걸려 있으면 안된다. 즉, 간단하게 정리하면, 세마포어를 사용하는 대부분의 경우는 다른 락을 사용할 수 없는 상황이다. 동기화를 사용하는 사용자 공간 코드와 마찬가지로 휴면이 필요한 상황이라면, 사용할 수 있는 방법은 세마포어 뿐이다. 만일, 세마포어와 스핀락 사이에서 선택을 해야하는 상황이라면 락을 쥐고 있는 시간으로 결정해야 하며, 스핀락과 달리 세마포어는 커널 선점을 비활성화시키지 않기 때문에 세마포어를 잡고 있는 코드 또한 선점될 수 있다.\n세마포어 종류 세마포어는 아래와 같은 종류가 있다.\n카운팅 세마포어 바이너리 세마포어: 카운트 값이 최대 1이 되는 카운팅 세마포어 리더-라이터 세마포어 리더-라이터 세마포어 #include \u0026lt;linux/module.h\u0026gt; #include \u0026lt;linux/kernel.h\u0026gt; #include \u0026lt;linux/fs.h\u0026gt; #include \u0026lt;linux/cdev.h\u0026gt; #include \u0026lt;linux/device.h\u0026gt; #include \u0026lt;asm/uaccess.h\u0026gt; #include \u0026lt;linux/semaphore.h\u0026gt; #include \u0026lt;linux/sched.h\u0026gt; #include \u0026lt;linux/delay.h\u0026gt;\n#define FIRST_MINOR 0 #define MINOR_CNT 1\nstatic dev_t dev; static struct cdev c_dev; static struct class *cl; static struct task_struct *task; static struct rw_semaphore rwsem;\nint open(struct inode *inode, struct file *filp) { printk(KERN_INFO \u0026ldquo;Inside open\\n\u0026rdquo;); task = current; return 0; }\nint release(struct inode *inode, struct file *filp) { printk(KERN_INFO \u0026ldquo;Inside close\\n\u0026rdquo;); return 0; }\nssize_t read(struct file *filp, char *buff, size_t count, loff_t *offp) { printk(\u0026ldquo;Inside read\\n\u0026rdquo;); down_read(\u0026amp;rwsem); printk(KERN_INFO \u0026ldquo;Got the Semaphore in Read\\n\u0026rdquo;); printk(\u0026ldquo;Going to Sleep\\n\u0026rdquo;); ssleep(30); up_read(\u0026amp;rwsem); return 0; }\nssize_t write(struct file *filp, const char *buff, size_t count, loff_t *offp) { printk(KERN_INFO \u0026ldquo;Inside write. Waiting for Semaphore\u0026hellip;\\n\u0026rdquo;); down_write(\u0026amp;rwsem); printk(KERN_INFO \u0026ldquo;Got the Semaphore in Write\\n\u0026rdquo;); up_write(\u0026amp;rwsem); return count; }\nstruct file_operations fops = { read: read, write: write, open: open, release: release };\nint rw_sem_init(void) { int ret; struct device *dev_ret;\nif ((ret = alloc_chrdev_region(\u0026amp;dev, FIRST_MINOR, MINOR_CNT, \u0026quot;rws\u0026quot;)) \u0026lt; 0) { return ret; } printk(\u0026quot;Major Nr: %d\\n\u0026quot;, MAJOR(dev)); cdev_init(\u0026amp;c_dev, \u0026amp;fops); if ((ret = cdev_add(\u0026amp;c_dev, dev, MINOR_CNT)) \u0026lt; 0) { unregister_chrdev_region(dev, MINOR_CNT); return ret; } if (IS_ERR(cl = class_create(THIS_MODULE, \u0026quot;chardrv\u0026quot;))) { cdev_del(\u0026amp;c_dev); unregister_chrdev_region(dev, MINOR_CNT); return PTR_ERR(cl); } if (IS_ERR(dev_ret = device_create(cl, NULL, dev, NULL, \u0026quot;mychar%d\u0026quot;, 0))) { class_destroy(cl); cdev_del(\u0026amp;c_dev); unregister_chrdev_region(dev, MINOR_CNT); return PTR_ERR(dev_ret); } init_rwsem(\u0026amp;rwsem); return 0; }\nvoid rw_sem_cleanup(void) { printk(\u0026ldquo;Clean up semaphores\u0026hellip;\\n\u0026rdquo;);\ndevice_destroy(cl, dev); class_destroy(cl); cdev_del(\u0026amp;c_dev); unregister_chrdev_region(dev, MINOR_CNT); }\nmodule_init(rw_sem_init); module_exit(rw_sem_cleanup);\nMODULE_LICENSE(\u0026ldquo;GPL\u0026rdquo;); MODULE_AUTHOR(\u0026ldquo;Sukbeom Kim - code from SysPlay Workshops\u0026rdquo;); MODULE_DESCRIPTION(\u0026ldquo;Reader Writer Semaphore Demo\u0026rdquo;);\n", 
        "url": "\/\/localhost:1313\/posts\/%EB%A6%AC%EB%88%85%EC%8A%A4-%EC%BB%A4%EB%84%90-%EB%9D%BD-%EC%A2%85%EB%A5%98-3\/5\/"
    },
    
    "\/\/localhost:1313\/tags\/major-number\/": {
        "title": "Major Number",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/major-number\/"
    },
    
    "\/\/localhost:1313\/posts\/major%EC%99%80-minor-numbers\/": {
        "title": "Major와 Minor Numbers",
        "tags": ["minor number","major number",],
        "content": "세마포어를 이용한 모듈 프로그래밍을 하던 중 Major, Minor 라는 개념이 등장하였다. 인터넷으로 찾은 커널 모듈 소스가 구버전 커널을 기준으로 한 까닭에, 커널 코드가 어떻게 변경되어 갔는지 히스토리를 삽질해 볼 수 있는 아주 좋은 기회다.\n캐릭터 디바이스는 /dev 디렉토리에서 쉽게 확인할 수 있는데 파일의 속성에서 각 장치에 대한 속성은 맨 앞 문자를 통해 판단할 수 있다. 예를 들어, \u0026lsquo;c\u0026rsquo;를 포함하고 있다면 캐릭터 디바이스(character devices)를 위한 특수 파일로, \u0026lsquo;b\u0026rsquo;를 포함하고 있다면 블록 디바이스(block devices)로 식별할 수 있다. 아래와 같이 ls 명령어를 사용하면 각 디바이스 파일에 번호가 할당되어 있는 것을 알 수 있다.\ndrwxr-xr-x 2 root root 60 May 31 23:18 vfio crw------- 1 root root 10, 63 May 31 23:18 vga_arbiter crw------- 1 root root 10, 137 May 31 23:18 vhci crw-rw----+ 1 root kvm 10, 238 May 31 23:18 vhost-net crw------- 1 root root 10, 241 May 31 23:18 vhost-vsock crw-rw----+ 1 root video 81, 0 May 31 23:18 video0 crw-rw----+ 1 root video 81, 1 May 31 23:18 video1 crw------- 1 root root 10, 130 May 31 23:18 watchdog crw------- 1 root root 246, 0 May 31 23:18 watchdog0 crw-rw-rw- 1 root root 1, 5 May 31 23:18 zero 이 때, major number는 특정 디바이스에 할당된 드라이버를 식별한다. 예를 들어, /dev/zero는 드라이버 1이 관리하고 /dev/watchdog0은 드라이버 246이 관리한다. minor number는 드라이버가 맡고 있는 장치들을 분류하기 위한 것으로 아래와 같이 같은 major number를 가지고 있는 장치들을 분류할 때 사용한다.\n➜ ~ ls -l /dev | egrep \u0026#39;^c.*.(\\s)1,\u0026#39; crw-rw-rw- 1 root root 1, 7 May 31 23:18 full crw-r--r-- 1 root root 1, 11 May 31 23:18 kmsg crw-r----- 1 root kmem 1, 1 May 31 23:18 mem crw-rw-rw- 1 root root 1, 3 May 31 23:18 null crw-r----- 1 root kmem 1, 4 May 31 23:18 port crw-rw-rw- 1 root root 1, 8 May 31 23:18 random crw-rw-rw- 1 root root 1, 9 May 31 23:18 urandom crw-rw-rw- 1 root root 1, 5 May 31 23:18 zero 버전 2.4 커널에서 devfs(device file system)라는 새 기능이 추가되었다. 만약 이 파일시스템 사용되면 디바이스 파일들을 그 전보다 훨씬 간단하게 관리할 수 있지만 호환성에 문제가 생긴다. 이에 대해서 자세히 알아보자.\ndevfs를 사용하지 않을 경우, 시스템에 드라이버를 새로 추가한다는 의미는 새로운 major number를 할당한다는 의미와 같다. 그래서 아래와 같은 코드를 이용해 직접 그 숫자를 할당해줘야 한다.\n// return: success or failure(\u0026lt;0) // major: major number being requested // name: name of the device (which will appear in /proc/devices) // fops: driver\u0026#39;s entry point int register_chrdev(unsigned int major, const char *name, struct file_operations *fops); Major Number는 small integer 형태로서 캐릭터 드라이버 배열의 인덱스를 담당한다. 2.0 커널에서는 128개 디바이스에 대해, 2.2와 2.4에서는 256개 디바이스에 대한 인덱스를 가질 수 있으며 Minor Number의 경우 8비트를 가져 마찬가지로 256개 디바이스에 대한 인덱스를 가질 수 있다. 하지만 Minor Number는 위 함수에서 특별히 인자로 넘기지 않는데 이는 드라이버에서만 제한적으로 사용되는 숫자이기 때문에 그렇다.\n드라이버를 커널 테이블에 등록하면 주어진 major number를 할당한다. 이후부터는 캐릭터 디바이스에 대한 파일 연산을 할 때마다 등록 시에 정의했던 file_operations 구조체의 각 함수들을 이용하게 된다. 하지만 코드가 아닌 실제 명령어를 통해 이러한 등록 과정을 아주 간단히 할 수가 있는데 그것이 바로 mknod 명령어이다.\n$ mknod /dev/scull0 c 254 0 $ rm /dev/scull0 위처럼 major number가 254, minor number가 0인 캐릭터 디바이스(c)를 생성하고 해당 디바이스를 삭제할 수 있다. 하지만 이렇게 정적으로 디바이스를 관리하는 인덱스 번호를 할당할 필요가 있을까?\nDynamic Allocation of Major Numbers 몇몇 주요 장치들에 대한 인덱스 숫자는 정적으로 할당된다. 이러한 장치들에 대한 정보는 Documentation/admin-guide/devices.txt에서 찾을 수 있다. (책에는 Documentation/devices.txt라고 되어 있으나 커널 버전이 업되면서 경로가 바뀌었다.)\n정적으로 Major Number를 할당하면 공식 커널 트리에 포함되어 유용하게 사용되는 경우에만 할당해야 하며, 그렇지 않으면 반드시 동적으로 할당하는 방법을 사용해야 한다. 하지만 동적으로 Major Number를 할당하는 방법의 단점은 디바이스 노드를 생성할 수 없다는 것이다. 항상 같은 번호를 할당받을 수 없기 때문인데 이 말은 즉슨, loading-on-demand 방식을 사용할 수 없다는 말과 같다. 하지만 이러한 특징은 일반적인 사용에 있어서 크게 문제가 되지는 않는다. 앞서 설명했던 것처럼 /proc/devices의 정보를 사용하면 되기 때문이다.\n동적으로 생성하는 방법은 아래와 같은 코드를 이용하면 된다. 이 때, scull_major 값을 0으로 주어지면 동적 할당을 사용한다는 의미이다.\nresult = register_chrdev(scull_major, \u0026#34;scull\u0026#34;, \u0026amp;scull_fops); if (result \u0026lt; 0) { printk(KERN_WARNING \u0026#34;scull: can\u0026#39;t get major %d\\n\u0026#34;, scull_major); return resul;t } if (scull_major == 0) scull_major = result; /* dynamic */ unregister_chrdev(scull_major, \u0026#34;scull\u0026#34;); 이 때, 코드 마지막에 위치하는 unregister_chrdev 사용에 있어서 실패했을 경우를 염두에 두어야 한다. 등록 해제(unregister_chrdev)가 실패했을 때는 그 영향에 대해 주의해야 한다./proc/devices 자체가 실패할 수 있는데 그 이유는 이미 해제된 장치에 대해 이름을 가리키는 포인터가 잘못될 수 있기 때문이다.\nkdev_t and dev_t 본래 유닉스에서는 16비트 정수 형태로 dev_t안에 디바이스 번호를 담고 있었는데 오늘날에는 이것이 minor number의 최대치인 256보다 더 많은 인덱스 숫자를 한번에 요구하는 경우가 생기게 되었다. 하지만 하위 호환성을 위해서 dev_t자체의 구조를 변경하지는 못하고 있다.\n리눅스에서는 이와 달리 kdev_t라는 약간 다른 타입을 사용한다. 블랙박스 형태로 설계되었기 때문에 사용자 애플리케이션은 kdev_t에 대해 완전히 알지 못하고 커널 함수들 또한 해당 자료구조의 내부를 정확히 알지 못한다. 때문에 커널 버전 변경에 따라 자료구조가 변경되더라도 디바이스 드라이버에서 해당 변경에 대해 별다른 변경 작업을 할 필요가 없도록 설계되었다. kdev_t를 이용하기 위해서는 직접 사용할 필요가 없고 아래와 같이 제공되는 함수 또는 매크로를 이용한다.\n// Extract the major number from a kdev_t structure. #define MAJOR(dev)\t((unsigned int) ((dev) \u0026gt;\u0026gt; MINORBITS)) // Extract the minor number. #define MINOR(dev)\t((unsigned int) ((dev) \u0026amp; MINORMASK)) // Create a kdev_t build from major and minor numbers #define MKDEV(ma,mi)\t(((ma) \u0026lt;\u0026lt; MINORBITS) | (mi)) 출처 https://www.oreilly.com/library/view/linux-device-drivers/0596000081/ch03s02.html ", 
        "url": "\/\/localhost:1313\/posts\/major%EC%99%80-minor-numbers\/"
    },
    
    "\/\/localhost:1313\/tags\/minor-number\/": {
        "title": "Minor Number",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/minor-number\/"
    },
    
    "\/\/localhost:1313\/tags\/global-kernel-lock\/": {
        "title": "Global Kernel Lock",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/global-kernel-lock\/"
    },
    
    "\/\/localhost:1313\/tags\/mutex\/": {
        "title": "Mutex",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/mutex\/"
    },
    
    "\/\/localhost:1313\/tags\/spinlock\/": {
        "title": "Spinlock",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/spinlock\/"
    },
    
    "\/\/localhost:1313\/posts\/%EB%A6%AC%EB%88%85%EC%8A%A4-%EC%BB%A4%EB%84%90-%EB%9D%BD-%EC%A2%85%EB%A5%98-2\/5\/": {
        "title": "리눅스 커널 락 종류 (2\/5)",
        "tags": ["mutex","semaphore","spinlock","global kernel lock",],
        "content": "지난 번 포스팅에서는 리눅스 커널 락 중 하나인 스핀락(Spinlock)에 대해 기술하였다. 이번 포스팅에서는 뮤텍스(Mutex)에 대해서 기술하고자 한다. 많은 곳에서 뮤텍스는 세마포어의 카운트 값이 단순하게 1로 설정되었을 때를 말한다고 기술한다. 하지만 이것이 맞는 설명일까?\n참고 자료 에 따르면 뮤텍스는 공유 자원으로의 접근(Access)에 대한 상호 배제(Mutual Exclusion)을 위한 수단이 Mutex라고 정의하고 있다. 이에 반해 세마포어(Semaphore)는 시그널 매커니즘으로서 스레드(또는 프로세스) 간 동기화가 주 목적으로, wait 함수를 호출한 스레드만이 뮤텍스를 신호를 보낼 수 있다는 점이 기능적인 특징이다. 쉽게 말해, 세마포어(Semaphore)는 임계 구역(Critical Section)을 실행하려하는 여러 개의 프로세스들을 제한하며 동기화하고, 뮤텍스(Mutex)는 오직 한 번에 한 개 프로세스에서 자원에 대한 접근을 제한하는 것이 mutex이자 mutual exclusion이라는 목적이다.\n뮤텍스는 카운트 1인 세마포어처럼 상호 배제성을 가진 휴면 가능한 락이지만 세마포어보다 부가적인 제약 사항을 가지고 있다.\n한 태스크는 한 번에 하나의 뮤텍스만 얻을 수 있다. 즉, 뮤텍스의 사용 카운트 값은 항상 1이다.\n뮤텍스를 얻은 곳에서만 뮤텍스를 해제할 수 있다. 이 때문에 한 컨텍스트에서 얻은 뮤텍스를 다른 컨텍스트에서 해제할 수 없으므로, 커널과 사용자 공간 사이의 복잡한 동기화에 사용하기에는 부적절하다.\n재귀적으로 락을 얻고 해제할 수 없다. 같은 뮤텍스를 재귀적으로 여러 번 얻을 수 없으며 해제된 뮤텍스를 한번 더 해제할 수도 없다.\n뮤텍스를 가지고 있는 동안에는 프로세스 종료가 불가능하다.\n인터럽트 핸들러나 후반부 처리 작업 내에서는 뮤텍스를 얻을 수 없으며, mutex_trylock() 함수도 사용할 수 없다.\n뮤텍스는 공식 API를 통해서만 관리할 수 있다. 또한 뮤텍스를 복사하거나 초기화 상태 전달, 재 초기화하는 작업은 불가능하다.\n그렇다면 어떤 경우에 뮤텍스와 스핀락, 세마포어 중에서 뮤텍스를 선택해야 하는가? 뮤텍스와 세마포어의 선택에는 동기화 여부를, 뮤텍스와 스핀락의 선택에서는 인터럽트 컨텍스트 여부와 락 사용시간을 비교하여 선택할 수 있다.\n요구사항 추천 Lock 락 부담이 적어야 하는 경우 스핀락 락의 사용시간이 짧아야 할 때 스핀락 락 사용시간이 길 때 뮤텍스 인터럽트 컨텍스트에서 락을 사용할 때 반드시 스핀락 락을 얻은 상태에서 휴면할 필요가 있을 때 반드시 뮤텍스 뮤텍스에 관련된 예제는 간단하다. 애초에 뮤텍스에 관련된 API가 간단할 뿐더러 락을 얻은 컨텍스트에서 해당 락을 해제해야 하는 제약사항이 있기 때문이다.\n이제 뮤텍스를 사용하는 코드 예제를 간단하게 살펴보자. 먼저 유저 레벨에서 동작하는 애플리케이션 코드이다.\n#include \u0026lt;stdio.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;pthread.h\u0026gt; #include \u0026lt;string.h\u0026gt;\npthread_t tid[2]; pthread_mutex_t lock; int counter;\nvoid* trythis (void *arg) { pthread_mutex_lock(\u0026amp;lock);\nunsigned long i = 0; counter += 1; printf(\u0026quot;\\n Job %d has started\\n\u0026quot;, counter); for (i=0; i\u0026lt;(0x9999); i++); printf(\u0026quot;\\n Job %d has finished\\n\u0026quot;, counter); pthread_mutex_unlock(\u0026amp;lock); return NULL; }\nint main(int argc, char* argv[]) { int i = 0; int error;\nif (pthread_mutex_init(\u0026amp;lock, NULL) != 0) { printf(\u0026quot;\\nMutex init has failed\\n\u0026quot;); return 1; } while (i \u0026lt; 2) { error = pthread_create(\u0026amp;(tid[i]), NULL, \u0026amp;trythis, NULL); if (error != 0) { printf(\u0026quot;\\nThread can't be created: %s\u0026quot;, strerror(error)); } i++; } pthread_join(tid[0], NULL); pthread_join(tid[1], NULL); pthread_mutex_destroy(\u0026amp;lock); return 0; }\nvoid* trythis(void* arg) 함수 내 뮤텍스로 인해 스레드가 락을 얻은 순서(Thread 1 -\u0026gt; Thread 2)대로 실행되는 것을 확인할 수 있다.\n그렇다면, 유저 레벨 애플리케이션이 아닌 커널 모듈에서는 뮤텍스를 어떤 식으로 사용할 수 있을까? 여기에 대한 예제는 커널 문서 에서 쉽게 찾을 수 있었다.\n#include \u0026lt;linux/list.h\u0026gt; #include \u0026lt;linux/slab.h\u0026gt; #include \u0026lt;linux/string.h\u0026gt; #include \u0026lt;linux/mutex.h\u0026gt; #include \u0026lt;asm/errno.h\u0026gt;\nstruct object { struct list_head list; int id; char name[32]; int popularity; };\n/* Protects the cache, cache_num, and the objects within it */ static DEFINE_MUTEX(cache_lock); static LIST_HEAD(cache); static unsigned int cache_num = 0; #define MAX_CACHE_SIZE 10\n/* Must be holding cache_lock */ static struct object *__cache_find(int id) { struct object *i;\nlist_for_each_entry(i, \u0026amp;cache, list) if (i-\u0026gt;id == id) { i-\u0026gt;popularity++; return i; } return NULL; }\n/* Must be holding cache_lock */ static void __cache_delete(struct object *obj) { BUG_ON(!obj); list_del(\u0026amp;obj-\u0026gt;list); kfree(obj); cache_num\u0026ndash;; }\n/* Must be holding cache_lock */ static void __cache_add(struct object *obj) { list_add(\u0026amp;obj-\u0026gt;list, \u0026amp;cache); if (++cache_num \u0026gt; MAX_CACHE_SIZE) { struct object *i, *outcast = NULL; list_for_each_entry(i, \u0026amp;cache, list) { if (!outcast || i-\u0026gt;popularity \u0026lt; outcast-\u0026gt;popularity) outcast = i; } __cache_delete(outcast); } }\nint cache_add(int id, const char *name) { struct object *obj;\nif ((obj = kmalloc(sizeof(*obj), GFP_KERNEL)) == NULL) return -ENOMEM; strlcpy(obj-\u0026gt;name, name, sizeof(obj-\u0026gt;name)); obj-\u0026gt;id = id; obj-\u0026gt;popularity = 0; mutex_lock(\u0026amp;cache_lock); __cache_add(obj); mutex_unlock(\u0026amp;cache_lock); return 0; }\nvoid cache_delete(int id) { mutex_lock(\u0026amp;cache_lock); __cache_delete(__cache_find(id)); mutex_unlock(\u0026amp;cache_lock); }\nint cache_find(int id, char *name) { struct object *obj; int ret = -ENOENT;\nmutex_lock(\u0026amp;cache_lock); obj = __cache_find(id); if (obj) { ret = 0; strcpy(name, obj-\u0026gt;name); } mutex_unlock(\u0026amp;cache_lock); return ret; }\n출처 Mutex vs. Semaphore Using Semaphore in Linux mutex lock for linux thread synchronization An example of Kernel Locking ", 
        "url": "\/\/localhost:1313\/posts\/%EB%A6%AC%EB%88%85%EC%8A%A4-%EC%BB%A4%EB%84%90-%EB%9D%BD-%EC%A2%85%EB%A5%98-2\/5\/"
    },
    
    "\/\/localhost:1313\/posts\/%EB%A6%AC%EB%88%85%EC%8A%A4-%EC%BB%A4%EB%84%90-%EB%9D%BD-%EC%A2%85%EB%A5%98-1\/5\/": {
        "title": "리눅스 커널 락 종류 (1\/5)",
        "tags": ["mutex","semaphore","spinlock","global kernel lock",],
        "content": "리눅스 커널에서 사용되는 락의 종류는 다양하다. 학부 시절, \u0026lsquo;뮤텍스는 세마 포어의 카운트가 1인 락\u0026rsquo;이라는 말도 안되는 내용으로 학교 선배로부터 배웠던 것이 기억난다. 락의 종류별로 쓰임새가 있고 장단점이 있는 것인데, 이 문서에는 커널에서 사용하는 락(lock)의 종류와 각각에 대한 사용 예를 기술하고자 한다.\n커널에서 사용 가능한 락은 다음과 같다.\n스핀락(Spinlock) 뮤텍스(Mutex) 세마포어(Semaphore) Read/Write Lock(rwlock) Big Kernel Lock 스핀락(Spinlock) 커널 락의 주요 타입에는 스핀락(spinlock)과 뮤텍스(mutex)가 있다. 스핀락은 이름 그대로 임계구역(critical section)에 진입이 불가능할 때, 진입기 가능할 때까지 루프를 돌면서 재시도를 하는 방식으로 구현된 락을 가리킨다. 즉, 락을 획득할 때까지 해당 스레드가 계속 돌고 루핑하고 있다는 것을 의미하며, Busy Waiting의 한 종류이다.\nBusy Waiting이란, Spinning이라고도 하며, 특정 공유 자원에 대해 두 개 이상의 프로세스나 스레드가 그 이용 권한을 획득하고자 하는 동기화 상황에서 권한 획득을 위한 과정에서 일어나는 현상이다. 대부분 스핀락과 동일한 개념으로 사용되지만 엄밀하게 말하자면, 스핀락이 Busy Waiting 개념을 이용하여 구현된 것이다.\n다른 락과 비교되는 스핀락의 가장 특징적인 차이점은 운영 체제의 스케쥴링 지원을 받지 않는다는 점이다. 즉, 락을 사용하는 스레드에 대한 문맥 교환(Context Switching)이 일어나지 않는다. 따라서 스핀락은 임계 구역에 짧은 시간 안에 진입할 수 있는 경우, 문맥 교환을 제거할 수 있어 효율적이다. 하지만 스핀락이 오랜 시간을 소요한다면 다른 스레드를 실행하지 못하고 대기하게 되어 오히려 비효율적인 결과를 가져온다.\n스핀락은 아키텍처별로 어셈블리어로 구현된다. \u0026lt;asm/spinlock.h\u0026gt; 파일에 아키텍처별 코드가 정의돼 있으며, 실제 사용하는 인터페이스는 \u0026lt;linux/spinlock.h\u0026gt;에 들어있다. 이제 실제 코드를 살펴보자.\n스핀락 커널 코드 // 스핀락 선언 #define DEFINE_SPINLOCK(x)\tspinlock_t x = __SPIN_LOCK_UNLOCKED(x) // 스핀락을 얻는 함수 spin_lock static __always_inline void spin_lock(spinlock_t *lock) { raw_spin_lock(\u0026amp;lock-\u0026gt;rlock); } // ... #ifndef CONFIG_INLINE_SPIN_LOCK void __lockfunc _raw_spin_lock(raw_spinlock_t *lock) { __raw_spin_lock(lock); } EXPORT_SYMBOL(_raw_spin_lock); #endif // 아래에서도 알 수 있듯이 스핀락을 얻을 때 선점을 비활성화하고 스핀락을 거는 것을 알 수 있다. static inline void __raw_spin_lock(raw_spinlock_t *lock) { preempt_disable(); spin_acquire(\u0026amp;lock-\u0026gt;dep_map, 0, 0, _RET_IP_); LOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock); } static __always_inline void spin_unlock(spinlock_t *lock) { raw_spin_unlock(\u0026amp;lock-\u0026gt;rlock); } // 스핀락 초기화 매크로 #define spin_lock_init(_lock)\t\\ do {\t\\ spinlock_check(_lock);\t\\ raw_spin_lock_init(\u0026amp;(_lock)-\u0026gt;rlock);\t\\ } while (0) 마지막으로 raw_spin_lock_init을 gtags으로 계속 찾아 들어가면 _raw_spin_lock에 대해서 아래와 같이 spinlock_api_up.h과 spinlock_api_smp.h로 각각 구분되어 정의되어 있는 것을 알 수 있다.\n/Volumes/KernelHacking/Workspaces/linux-4.20.9/include/linux/spinlock_api_up.h 58: #define _raw_spin_lock( /Volumes/KernelHacking/Workspaces/linux-4.20.9/include/linux/spinlock_api_smp.h 47: #define _raw_spin_lock( UP는 Uni-Processor를, SMP는 Symmetric Multi-Processor를 의미한다. 각각의 경우에서 스핀락을 사용할 때 생기는 문제점은 다음과 같다. 프로세서가 하나인 UP(Uni-Processor)인 경우, 스핀락으로 인한 성능 오버헤드는 상당하다. 임계영역(Critical Section) 내에서 락을 잡고 있는 스레드가 선정됐다는 상황을 생각해보자. 이런 상황에서 스케쥴러는 스핀락을 잡고 있는 스레드 외의 다른 모든 스레드를 실행하려할 것이고 스케쥴링된 스레드는 첫 번째 스레드(임계영역을 실행하고 있던)가 쥐고 있는 락을 얻으려 하면서 불필요하게 CPU 사이클을 낭비하게 되는 문제가 발생한다.\n하지만, SMP(Symmetric Multi-Processor) 환경인 경우 스핀락은 잘 동작한다. UP(Uni-Processor) 환경과 달리 한 스레드가 임계영역 내에서 락을 잡고 있는 상태라도 프로세스가 여러 개이므로 앞서 설명한 상황과 같이, 다른 스레드들이 스케쥴링 되더라도 스핀락 자체가 매우 짧은 시간동안 언락된다면 스케쥴링된 새로운 스레드가 해당 락을 잡게 되어 \u0026lsquo;Uni-Processor\u0026rsquo;처럼 클럭 낭비를 줄일 수 있다.\n스핀락과 후반부 처리 스핀락은 인터럽트 핸들러에서 종종 사용된다고 언급했다. 특히, \u0026ldquo;후반부 처리와 지연된 작업\u0026quot;에서 후반부 처리를 진행할 때는 락에 대해 특별한 주의를 기울여야 한다. spin_lock_bh() 또는 spin_unlock_bh() 함수와 같은 경우 지정한 락을 걸고 모든 후반부 처리 작업을 비활성화 시킨다. 이렇게 후반부 처리를 비활성화하는 이유는 프로세스 컨텍스트 코드를 선점할 수 있기 때문이다. 만약 후반부 처리와 프로세스 컨텍스트 간에 공유하는 데이터가 있다면 반드시 데이터를 보호하기 위해서 락을 걸고 후반부 처리를 비활성화 시켜야 한다.\n출처 스핀락 \u0026mdash; 위키피디아 Different kernel flavors (UP, SMP, ENTERPRISE, ENTNOSPLIT) ", 
        "url": "\/\/localhost:1313\/posts\/%EB%A6%AC%EB%88%85%EC%8A%A4-%EC%BB%A4%EB%84%90-%EB%9D%BD-%EC%A2%85%EB%A5%98-1\/5\/"
    },
    
    "\/\/localhost:1313\/tags\/jiffies\/": {
        "title": "Jiffies",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/jiffies\/"
    },
    
    "\/\/localhost:1313\/posts\/%EC%A7%80%ED%94%BCjiffies\/": {
        "title": "지피(Jiffies)",
        "tags": ["jiffies",],
        "content": " 오랜만에 지피에 대한 포스팅을 다시 작성한다(대학생 때 커널 공부를 한 뒤로 이렇게 별도로 문서를 작성하는 것은 처음인 것 같다). 전역 변수인 jiffies에는 시스템 시작 이후 발생한 진동 횟수(tick)이 저장된다. 시스템 시작 시 커널은 이 값을 0으로 설정하고 타이머 인터럽트가 발생할 때마다 1씩 증가시킨다. 타이머 인터럽트는 초당 HZ회 발생하므로, 초당 지피 수는 HZ가 되며, 이에 따라 시스템 가동 시간은 jiffies / HZ(초)가 된다.\n커널은 버그 식별을 위해 jiffies 변수의 오버플로우 현상이 자주 일어나기 위해 jiffies 변수를 0이 아닌 특별한 값으로 초기화하며 실제 jiffies값이 필요한 경우에는 이 차이(offset)값을 빼야 한다.\n지피 비교를 위한 매크로 jiffies는 unsigned long 타입으로 언젠가는 오버플로우가 발생하게 된다. 오버플로우에 대한 내용을 찾던 중 재미있는 함수를 발견했는데 이와 관련해 잠깐 설명하고자 한다.\n#define time_after(a,b)\t\\ (typecheck(unsigned long, a) \u0026amp;\u0026amp; \\ typecheck(unsigned long, b) \u0026amp;\u0026amp; \\ ((long)((b) - (a)) \u0026lt; 0)) #define time_before(a,b)\ttime_after(b,a) #define time_after_eq(a,b)\t\\ (typecheck(unsigned long, a) \u0026amp;\u0026amp; \\ typecheck(unsigned long, b) \u0026amp;\u0026amp; \\ ((long)((a) - (b)) \u0026gt;= 0)) #define time_before_eq(a,b)\ttime_after_eq(b,a) 예전에 지피(jiffies) 비교 매크로를 보면서 이게 어떻게 작동할 수 있는지 확실하게 이해하지 못했었는데 이는 signed, unsigned와 실제 바이트와의 관계를 내가 정확하게 이해하지 못하고 있었기 때문이었다.\n맘에 들지는 않지만, 스택 오버플로우에 매우 친절하게 설명되어 있는 답변이 있었다. unsigned와 signed의 관계를 자세하게 나타냈는데 아래와 같이 표현해주었다. 아래 그림(?)은 편의상 unsigned long 대신 unsigned int와 signed int의 관계를 나타낸 그림이다.\n[0x0 - - - 0xFFFF] [0x0 0x7FFF][0x8000 0xFFFF] [0 32,767][-32,768 -1] integer unsigned 형태는 0x0부터 0xFFFF(4bytes, 2^32)까지의 값 범위를 가지고 있고 signed는 그 절반을 갖고 있다. 이 때 중요한 점은 signed의 양수 범위를 초과하는 순간 -1로 되는 것이 아니라 음수 최소값으로 돌아간다는 점이다. 그러므로 0xFFFF는 -1이 된다.\n지피를 비교하는 경우로 다시 돌아가보자. 비교할 지피 t1, t2에 대해 1) 모두 양수인 경우, 2) 모두 음수인 경우, 3) 한쪽은 양수, 한쪽은 음수인 경우를 생각해 볼 수 있다.\n이를 비교하기 위해 아래와 같이 간단하게 코드를 짜본뒤 확인해보자.\n/* unsigned long 오버플로우 검사 */ unsigned long t1 = 0; // 0x0 unsigned long t2 = 1; unsigned long t3 = (t1 - 1); // 0xffffffff unsigned long t4 = (t1 - 1) / 2; // 중간 값(signed 최대) printf(\u0026#34;(unsigned)t1 = %lu, t2 = %lu, t3 = %lu, t4 = %lu\\n\u0026#34;, t1, t2, t3, t4); printf(\u0026#34;(signed)t1 = %ld, t2 = %ld, t3 = %ld, t4 = %ld, t4+1 = %ld\\n\u0026#34;, t1, t2, t3, t4, t4+1); /* case 1. 비교 대상이 모두 양수일 때 */ printf(\u0026#34;case 1: t1 - t2 as long: %ld\\n\u0026#34;, (long)t1 - (long)t2); /* case 2. 비교 대상이 모두 음수일 때 */ printf(\u0026#34;case 2: (t4 + 1) - (t4 + 2): %ld\\n\u0026#34;, (t4+1) - (t4+2)); /* case 3-1. 비교 대상이 서로 다른 부호일 때 */ printf(\u0026#34;case 3-1: t3 - t1 as long: %ld\\n\u0026#34;, (long)((t3) - (t1))); /* case 3-2. 비교 대상이 서로 다른 부호일 때 */ printf(\u0026#34;case 3-2: t4 - (t4 + 1): %ld\\n\u0026#34;, (long)t4 - (long)(t4 + 1)); 이에 대한 출력 결과는 다음과 같다.\n(unsigned)t1 = 0, t2 = 1, t3 = 18446744073709551615, t4 = 9223372036854775807 (signed)t1 = 0, t2 = 1, t3 = -1, t4 = 9223372036854775807, t4+1 = -9223372036854775808 case 1: t1 - t2 as long: -1 case 2: (t4 + 1) - (t4 + 2): -1 case 3-1: t3 - t1 as long: -1 case 3-2: t4 - (t4 + 1): -1 즉, 앞서가는 unsigned, signed의 특성 상 앞서가는 포인트에 대해 signed 범위 안에서 값을 비교하면 그 결과 범위가 양수 또는 음수로 나오게 된다.\n매크로 속 typecheck 여기서 사용된 typecheck 함수를 잠깐 살펴보자. typecheck는 무조건 참(1) 값을 반환하는 매크로 함수이다. 그럼에도 사용하는 이유는 컴파일 타임에서 인자로 전달한 타입과 변수의 타입을 서로 비교하여 같은지 여부를 나타낼 수 있는 일종의 트릭을 사용하고 있기 때문이다. (커널 해킹을 시작한지 얼마되지 않은 시점에서 오랜만에 느껴보는 즐거움이다.) 아래는 typecheck를 구현한 코드이다.\n#define typecheck(type,x) \\ ({ type __dummy; \\ typeof(x) __dummy2; \\ (void)(\u0026amp;__dummy == \u0026amp;__dummy2); \\ 1; \\ }) 재미있는 것은 반환값은 항상 1이지만 반환 전에 (void)(\u0026amp;__dummy == \u0026amp;__dummy2); 부분을 통해 각 변수의 타입으로 만든 포인터가 일치하는지를 경고 또는 에러 메세지로 출력한다. 런타임 시의 결과를 기대하는 코드가 아니라, 컴파일 시의 결과를 기대하며 짠 코드인만큼 컨셉 자체가 정말 신선하게 다가왔다.\n이러한 코드는 아래와 같이 컴파일을 통해 출력 값을 기대할 수 있다. unsigned long 타입은 j1의 변수를 int과 비교하는 경우이다. 이러한 예제를 아래 스크린샷처럼 -Werror 옵션과 함께 컴파일하게 되면 타입 에러를 사전에 감지해낼 수 있다.\nint a = 1; unsigned long j1 = 12345678; unsigned long j2 = 23456789; if (typecheck(int,a) \u0026amp;\u0026amp; typecheck(int, j1) \u0026amp;\u0026amp; typecheck(unsigned long, j2)) { printf(\u0026#34;a value is int and (j1, j2) is unsigned long\\n\u0026#34;); } else { printf(\u0026#34;a value is not int\\n\u0026#34;); } 지피(jiffies)는 누가 증가시키는가? 앞서, \u0026quot;지피(jiffies)에는 시스템 시작 이후 발생한 진동 횟수(tick)가 저장된다.\u0026quot;라고 말했다. 그렇다면 그러한 tick은 누가 발생시킬까? 이를 위한 것이 바로 시스템 타이머이다.\n시간 기록을 위해서 리눅스에서는 RTC(Real Time Clock), 시스템 타이머를 이용한다.\n\u0026lsquo;시간\u0026rsquo;에 관련된 하드웨어 RTC는 보통 시스템 기판에 붙어 있는 원형 배터리를 통해 시스템이 꺼져 있는 동안에도 시간을 기록하며 일반적인 PC 아키텍처인 경우 RTC와 CMOS가 통합되어 있는 것을 확인할 수 있다.\n커널은 시스템 시작 시 RTC를 읽고 xtime변수에 저장되는 현재 시간을 초기화한다. 보통 커널은 최초 init에만 RTC를 읽으며 x86 시스템을 제외하고는 RTC를 다시 읽지 않는다.\n시스템 타이머는 커널의 시간 기록에 있어 매우 중요한 역할을 한다. 현재 시간을 초기화할 때 사용되는 RTC와는 달리 시스템 타이머는 주기적으로 인터럽트를 발생시킨다. 그리고 커널은 이러한 타이머 인터럽트에 대한 인터럽트 핸들러를 내부에 가지고 있어 아래와 같은 작업들을 처리한다.\njiffies_64 및 현재 시간을 저장하는 xtime 변수에 안전하게 접근하기 위해 xtime_lock을 얻는다. 필요에 따라 시스템 타이머를 확인하고 재설정한다. 갱신된 현재 시간을 주기적으로 실시간 시계에 반영한다. 아키텍처 종속적 타이머 함수인 tick_periodic() 함수를 호출한다. 커널 코드 속 시스템 타이머 인터럽트 핸들러 그렇다면, 직접 타이머 인터럽트 핸들러를 따라가보자. 커널 분석 책에는 아키텍처 종속적인 부분은 시스템 타이머의 인터럽트 핸들러 형태로 되어 있으며 타이머 인터럽트가 발생했을 때 실행된다고 되어 있다. 하지만 실제 커널(v4.20.x)에는 tick-common.h 안에 하나로 통합되어 있었다. tick-internal.h 헤더파일에는 tick_set_periodic_handler라는 함수가 정의되어 있다. 타이머 인터럽트에 대한 핸들러를 등록하는 함수로서 clock_event_device(시스템 타이머 장치) 디바이스의 이벤트 핸들러로 등록하는 부분이다.\n/* Set the periodic handler in non broadcast mode */ static inline void tick_set_periodic_handler(struct clock_event_device *dev, int broadcast) { dev-\u0026gt;event_handler = tick_handle_periodic; } 이제, tick_handle_periodic() 인터럽트 핸들러를 살펴보자.\n/* * Event handler for periodic ticks */ void tick_handle_periodic(struct clock_event_device *dev) { int cpu = smp_processor_id(); ktime_t next = dev-\u0026gt;next_event; tick_periodic(cpu); #if defined(CONFIG_HIGH_RES_TIMERS) || defined(CONFIG_NO_HZ_COMMON) /* * The cpu might have transitioned to HIGHRES or NOHZ mode via * update_process_times() -\u0026gt; run_local_timers() -\u0026gt; * hrtimer_run_queues(). */ if (dev-\u0026gt;event_handler != tick_handle_periodic) return; #endif if (!clockevent_state_oneshot(dev)) return; for (;;) { /* * Setup the next period for devices, which do not have * periodic mode: */ next = ktime_add(next, tick_period); if (!clockevents_program_event(dev, next, false)) return; /* * Have to be careful here. If we\u0026#39;re in oneshot mode, * before we call tick_periodic() in a loop, we need * to be sure we\u0026#39;re using a real hardware clocksource. * Otherwise we could get trapped in an infinite * loop, as the tick_periodic() increments jiffies, * which then will increment time, possibly causing * the loop to trigger again and again. */ if (timekeeping_valid_for_hres()) tick_periodic(cpu); } } 책에서는 \u0026ldquo;아키텍처 종속적인 부분은 시스템 타이머의 인터럽트 핸들러 형태로 되어 있으며, 타이머 인터럽트가 발생했을 때 실행된다.\u0026rdquo; 라고 되어 있으나, 실제 tick_periodic 자체는 아키텍처 종속 코드가 발견되지 않았다. 대신 cpu 아이디를 얻어오는 부분에 대해 아래와 같은 코드를 발견할 수 있었다.\n#ifdef CONFIG_DEBUG_PREEMPT extern unsigned int debug_smp_processor_id(void); # define smp_processor_id() debug_smp_processor_id() #else # define smp_processor_id() raw_smp_processor_id() #endif 그리고 raw_smp_processor_id()에 대해서는 아키텍처 별로 종속적인 코드가 들어가 있는 것을 확인할 수 있다. 한 예로, x86 코드를 살펴보면 아래와 같이 정의된다.\n#define raw_smp_processor_id() (this_cpu_read(cpu_number)) 여기서 더 깊게 들어가는 것은 그만두고 다시 원점으로 돌아가, 시스템 타이머 인터럽트 핸들러에서 SMP(Symmetric Multiprocessing)에 관련, 아키텍처에 종속된 코드가 실행된다는 것을 파악하였다. 이제 tick_periodic()을 살펴보자.\nstatic void tick_periodic(int cpu) { if (tick_do_timer_cpu == cpu) { write_seqlock(\u0026amp;jiffies_lock); /* Keep track of the next tick event */ tick_next_period = ktime_add(tick_next_period, tick_period); do_timer(1); write_sequnlock(\u0026amp;jiffies_lock); update_wall_time(); // 진동수 경과에 맞춰 현재 시간을 갱신한다. } update_process_times(user_mode(get_irq_regs())); profile_tick(CPU_PROFILING); } 이 함수에서 눈여겨볼 부분은 do_timer()와 update_process_times() 부분이다. do_timer() 함수는 실제 지피값을 증가시키는 작업을 담당하며 해당 코드는 아래와 같다. 커널은 전자 함수를 통해 지비를 발생한 틱만큼 증가시키고 후자를 통해 시스템의 평균 로드 통계를 갱신한다.\nvoid do_timer(unsigned long ticks) { jiffies_64 += ticks; calc_global_load(ticks); } void update_process_times(int user_tick) { struct task_struct *p = current; /* Note: this timer irq context must be accounted for as well. */ account_process_tick(p, user_tick); run_local_timers(); rcu_check_callbacks(user_tick); #ifdef CONFIG_IRQ_WORK if (in_irq()) irq_work_tick(); #endif scheduler_tick(); if (IS_ENABLED(CONFIG_POSIX_TIMERS)) run_posix_cpu_timers(p); } update_process_times는 run_local_timers()를 통해 로컬 타이머, 즉 softirq를 발생시켜 시간이 만료된 타이머를 실행한다.\n출처 how does linux handle overflow in jiffies ", 
        "url": "\/\/localhost:1313\/posts\/%EC%A7%80%ED%94%BCjiffies\/"
    },
    
    "\/\/localhost:1313\/posts\/%ED%83%9C%EC%8A%A4%ED%81%AC%EB%A6%BFtasklet\/": {
        "title": "태스크릿(Tasklet)",
        "tags": ["tasklet",],
        "content": "커널 모듈 프로그래밍을 연습하던 도중, 태스크릿 예제를 접하게 되었다. 한참 전에 태스크릿이 어떤 것인지 이론으로 접하기는 했으나 직접 사용해본 적은 없었기 때문에 이 문서를 통해 정리하고자 한다.\n간단히 말해 tasklet은 스택이나 자체 컨텍스트가 없는 스레드와 같은 것으로 설명하고 있다.\n태스크릿(Tasklet)의 특성 태스크릿(tasklet)은 원자성을 가지고 있기 때문에 mutex, semaphore와 같은 동기화 수단을 사용하거나 sleep() 을 사용할 수 없다. 단, spinlock은 가능하다. ISR보다 유연한 컨텍스트(softer context)로 불린다. 때문에 태스크릿의 컨텍스트 도중 하드웨어 인터럽트가 발생하는 것을 허용한다. 한번에 한 프로세서에 할당되어 실행되며 여러 개에 병렬적으로 실행될 수 없다. 태스크릿은 비선점 스케쥴링 방식으로 하나씩 순서대로 스케쥴링된다. 원자성(atomic)은 (물리학에서의 의미가 아닌 문자 그대로의 의미로) \u0026lsquo;더이상 나눌 수 없는 것\u0026rsquo;이다. 즉, 어떤 연산에 대해 더이상 나눌 수 없는 것 또는 한 번에 처리되어야 하는 것을 말하는데, 흔히 데이터베이스의 트랜잭션을 떠올리면 된다. 인터럽트와 선점 스케쥴러를 사용하는 리눅스 커널에서는 어떤 연산(operation)을 수행하는데, 해당 연산이 다른 것에 의해 선점되거나 방해받을 수 있다. 이럴 때, 해당 연산이 \u0026lsquo;Atomic\u0026rsquo;인 경우, 해당 연산이 다른 것에 의해 방해될 수 없으며 연산의 실패 또는 롤백의 단위가 오로지 연산 자체임을 보장하게 된다.\n태스크릿 구조 살펴보기 struct tasklet_struct { struct tasklet_struct next; / The next tasklet in line for scheduling / unsigned long state; / TASKLET_STATE_SCHED or TASKLET_STATE_RUN */ atomic_t count; // Responsible for the tasklet being activated or nothing void (*func)(unsigned long); // The main function of the tasklet unsigned long data; // The parameter func is started with };\n태스크릿은 아래의 매크로를 이용하거나 init 함수를 사용하여 초기화한다.\nextern void tasklet_init(struct tasklet_struct *t, void (*func)(unsigned long), unsigned long data);\n#define DECLARE_TASKLET(name, func, data) struct tasklet_struct name = { NULL, 0, ATOMIC_INIT(0), func, data }\n#define DECLARE_TASKLET_DISABLED(name, func, data) struct tasklet_struct name = { NULL, 0, ATOMIC_INIT(1), func, data }\n태스크릿은 우선순위에 따라 단방향 링크드 리스트로 구성된 큐에 삽입되며 해당 큐는 각 CPU별로 할당된다.\n태스크릿 상태 태스크릿이 스케쥴링되면 상태는 TASKLET_STATE_SCHED로 설정되고 큐에 삽입된다. 일단 태스크릿이 한번 이 상태에 있게 되면, 다시 스케쥴 될 수 없다. 태스크릿이 TASKLET_STATE_RUN 상태에 있게 되면 스케쥴이었던 상태가 제거되며 이 상태에 있게 되면 해당 스케쥴링을 스케쥴할 수 있게 된다.\n출처 Multitasking in the linux kernel interrupts and tasklets ", 
        "url": "\/\/localhost:1313\/posts\/%ED%83%9C%EC%8A%A4%ED%81%AC%EB%A6%BFtasklet\/"
    },
    
    "\/\/localhost:1313\/tags\/buildroot\/": {
        "title": "Buildroot",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/buildroot\/"
    },
    
    "\/\/localhost:1313\/tags\/busybox\/": {
        "title": "Busybox",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/busybox\/"
    },
    
    "\/\/localhost:1313\/posts\/linux-%EC%BB%A4%EB%84%90-busybox-%EB%B9%8C%EB%93%9C-%ED%9B%84-qemu%EC%97%90%EC%84%9C-%EC%8B%A4%ED%96%89%ED%95%98%EA%B8%B02\/2\/": {
        "title": "Linux 커널, Busybox 빌드 후 QEMU에서 실행하기(2\/2)",
        "tags": ["kernel","buildroot","busybox",],
        "content": "지난 번 포스팅에서는 단순하기 initramfs 램디스크를 만들어 busybox에 올리는 방법을 기술했었다. 그러면서 램디스크가 무엇인지 busybox는 무엇인지, 그리고 대략적인 빌드 디렉토리 구조를 파악할 수 있었던 기회였다.\n두 번째 포스팅에서는 직접 busybox를 빌드하지 않고 crosstool-ng라는 크로스 컴파일러 생성 스크립트 도구와 buildroot라는 Makefile 패키지를 이용하여 램디스크를 만드는 방법에 대해 기술하겠다.\nCrosstool-ng 이용하여 크로스 컴파일러 만들기 crosstool-ng는 미리 설정된 config를 이용해 손쉽게 크로스 컴파일러를 만들 수 있는 패키지이다. 이를 어떻게 이용하는지 자세히 설명하겠다.\n먼저 crosstool-ng를 받는다. 이 때, 깃허브 에 있는 경로를 이용해 직접 받아쓰는 경우가 있을텐데 만약, 받아쓰는 경우라면 반드시 릴리즈 버전으로 checkout해서 사용해야 한다. master로 그냥 받아서 사용하면 이상한데서 고생하게 된다. git을 사용하지 않는 경우는 여기 를 통해 홈페이지에서 직접 받아서 사용할 수 있다.\n$ git clone https://github.com/crosstool-ng/crosstool-ng $ git fetch \u0026ndash;all $ git checkout tags/crosstool-ng-1.24.0-rc3\n압축을 푼 후 내부에서 아래와 갈이 설정해준다.\n$ ./configure \u0026ndash;local $ make $ make install\n이제 크로스컴파일러를 만들어보자. 컴파일러 생성을 위해 필요한 패키지(binutls, glibc, gcc, mpc, flex..등등)들을 자동으로 다운로드하고 빌드한다. 예전 LFS(Linux from scratch)에서 이 방법을 썼다면 정말로 편하게 작업할 수 있었을텐데 하는 아쉬움이 남는 순간이었다.\n아래와 같이 list-samples 옵션을 주어 실행하면 사용 가능한 샘플 목록들이 출력된다. 여기에 없다면 앞으로 수행할 menuconfig에서 필요한 설정들을 직접 해주어야 한다.\nroot@19893213a218:~/Workspaces/crosstool-ng# ./ct-ng list-samples Status Sample name (생략) \u0026hellip; [L\u0026hellip;] x86_64-multilib-linux-gnu [L..X] x86_64-multilib-linux-musl [L\u0026hellip;] x86_64-multilib-linux-uclibc [L..X] x86_64-w64-mingw32,x86_64-pc-linux-gnu [L\u0026hellip;] x86_64-ubuntu12.04-linux-gnu [L\u0026hellip;] x86_64-ubuntu14.04-linux-gnu [L\u0026hellip;] x86_64-ubuntu16.04-linux-gnu [L\u0026hellip;] x86_64-unknown-linux-gnu [L\u0026hellip;] x86_64-unknown-linux-uclibc [L..X] x86_64-w64-mingw32 [L..X] xtensa-fsf-elf [L\u0026hellip;] xtensa-fsf-linux-uclibc\nL (Local) : sample was found in current directory G (Global) : sample was installed with crosstool-NG X (EXPERIMENTAL): sample may use EXPERIMENTAL features B (BROKEN) : sample is currently broken O (OBSOLETE) : sample needs to be upgraded\n커널 해킹을 위한 것이지만 임베디드용 커널을 살펴볼 것은 아니기 때문에 필자는 x86_64-unknown-linux-gnu를 선택하였다.\n$ ./ct-ng x86_64-unknown-linux-gnu $ ./ct-ng menuconfig # 옵션을 추가로 선택할 경우 $ ./ct-ng build\nBuildRoot 이용하여 rootfs 만들기 앞서 빌드한 크로스 컴파일러들을 /opt/crosstool/x86_64-unknown-linux-gnu 경로에 설치했다고 가정하고 buildroot를 이용하여 이미지 파일을 생성한다. buildroot 를 이용할 경우 크로스컴파일러를 이용해 컴파일한 응용 프로그램과 커널 모듈을 함께 빌드하여 추가할 수 있기 때문에 용이하다.\n$ export BUILDROOT=$OPT/buildroot $ export BUILDROOT_BUILD=$BUILDS/buildroot $ mkdir -p $BUILDROOT_BUILD $ cd $BUILDROOT_BUILD $ touch Config.in external.mk $ echo \u0026rsquo;name: mini_linux\u0026rsquo; \u0026gt; external.desc $ echo \u0026lsquo;desc: minimal linux system with buildroot\u0026rsquo; \u0026raquo; external.desc $ mkdir configs overlay $ cd $BUILDROOT $ make O=$BUILDROOT_BUILD BR2_EXTERNAL=$BUILDROOT_BUILD qemu_x86_64_defconfig $ cd $BUILDROOT_BUILD $ make menuconfig\n이 후, 아래와 같이 설정해준다. 이 때 중요한 것은 System configuration ---\u0026gt; Network interface to configure through DHCP 부분을 빈칸으로 해줘야한다는 점이다. 기본값이 eth0으로 되어있을텐데, init 스크립트에서 해당 인터페이스가 로드될 때까지 기다리며 없을 경우에는 셸이 실행되지 않게된다.\nBuild options ---\u0026gt; Location to save buildroot config ---\u0026gt; $(BR2_EXTERNAL)/configs/mini_linux_defconfig Build options ---\u0026gt; Download dir ---\u0026gt; /some/where/buildroot_dl Build options ---\u0026gt; Number of jobs to run simultaneously (0 for auto) ---\u0026gt; 8 Build options ---\u0026gt; Enable compiler cache ---\u0026gt; yes Build options ---\u0026gt; Compiler cache location ---\u0026gt; /some/where/buildroot_ccache Toolchain ---\u0026gt; Toolchain type ---\u0026gt; External toolchain Toolchain ---\u0026gt; Toolchain ---\u0026gt; Custom toolchain Toolchain ---\u0026gt; Toolchain origin ---\u0026gt; Pre-installed toolchain Toolchain ---\u0026gt; Toolchain path ---\u0026gt; /opt/toolchains/x86_64-unknown-linux-gnu Toolchain ---\u0026gt; Toolchain prefix ---\u0026gt; x86_64-unknown-linux-gnu Toolchain ---\u0026gt; External toolchain gcc version ---\u0026gt; 5.x Toolchain ---\u0026gt; External toolchain kernel headers series ---\u0026gt; 4.3.x Toolchain ---\u0026gt; External toolchain C library ---\u0026gt; glibc/eglibc Toolchain ---\u0026gt; Toolchain has C++ support? ---\u0026gt; yes System configuration ---\u0026gt; System hostname ---\u0026gt; mini_linux System configuration ---\u0026gt; System banner ---\u0026gt; Welcome to mini_linux System configuration ---\u0026gt; Run a getty (login prompt) after boot ---\u0026gt; TTY port ---\u0026gt; ttyS0 System configuration ---\u0026gt; Network interface to configure through DHCP ---\u0026gt; System configuration ---\u0026gt; Root filesystem overlay directories ---\u0026gt; $(BR2_EXTERNAL)/overlay Kernel ---\u0026gt; Linux Kernel ---\u0026gt; no Filesystem images ---\u0026gt; cpio the root filesystem (for use as an initial RAM filesystem) ---\u0026gt; yes Filesystem images ---\u0026gt; Compression method ---\u0026gt; gzip 아래와 같이 설정 저장 후 init 스크립트를 추가해준뒤 빌드한다.\n$ make savedefconfig $ vim $BUILDROOT_BUILD/overlay/init #!/bin/sh /bin/mount -t devtmpfs devtmpfs /dev /bin/mount -t proc none /proc /bin/mount -t sysfs none /sys exec 0\u0026lt;/dev/console exec 1\u0026gt;/dev/console exec 2\u0026gt;/dev/console cat \u0026lt;\u0026lt;! Boot took $(cut -d\u0026#39; \u0026#39; -f1 /proc/uptime) seconds ! exec /bin/sh # vim 종료 후 스크립트에 권한 부여 $ chmod +x overlay/init $ make 이제 qemu를 통해 실행시켜보면 정상적으로 실행되는 것을 확인할 수 있다.\nqemu-system-x86_64 -kernel $LINUX_BUILD/arch/x86_64/boot/bzImage \\ -initrd $BUILDROOT_BUILD/images/rootfs.cpio.gz -nographic \\ -append \u0026#34;console=ttyS0\u0026#34; 출처 Build and run minimal linux Building embedded ARM systems with Crosstool-NG ", 
        "url": "\/\/localhost:1313\/posts\/linux-%EC%BB%A4%EB%84%90-busybox-%EB%B9%8C%EB%93%9C-%ED%9B%84-qemu%EC%97%90%EC%84%9C-%EC%8B%A4%ED%96%89%ED%95%98%EA%B8%B02\/2\/"
    },
    
    "\/\/localhost:1313\/posts\/linux-%EC%BB%A4%EB%84%90-busybox-%EB%B9%8C%EB%93%9C-%ED%9B%84-qemu%EC%97%90%EC%84%9C-%EC%8B%A4%ED%96%89%ED%95%98%EA%B8%B01\/2\/": {
        "title": "Linux 커널, Busybox 빌드 후 QEMU에서 실행하기(1\/2)",
        "tags": ["busybox","qemu",],
        "content": " 취업은 언제하나\u0026hellip; 커널 공부를 할 게 아니라 취업을 위한 알고리즘을 공부해야 하는데 커널 해킹을 위한 환경 구축만 하고 공부하자는 것이 꼬리에 꼬리를 물게 되었다. 커널 분석을 공부하고 궁금했던 사항들을 직접 확인해보기 위해 QEMU를 이용한 환경을 구축하기로 결정했다. 다행히도 나와 같은 생각을 한 사람이 있었고 매우 자세하게 설명을 해놓았기에 금방 해결할 수 있었다. 다만, 부팅 후 램디스크만을 이용하고 루트파티션은 마운트하지 않는다는 제한은 있다.\n이 문서는 참고한 페이지를 토대로 필요한 정보들을 중간에 좀 더 추가한 형태로 정리하였다. 향후 커널 분석과 토이 프로젝트들을 진행하기 위해 필요한 환경으로 실행 환경은 아래와 같다.\n출처 페이지와는 다르게 필자는 맥 환경에서 커널 컴파일을 진행해야 했기 때문에 빌드를 시작하기에 앞서 docker 컨테이너를 준비하는 과정이 있었다. 맥에서 직접 크로스 컴파일러를 직접 만들어 사용하는 방법도 있지만, 시간도 오래 걸리고 빌드에 필요한 헤더 환경이 달라 도커를 사용하였다.\n맥에서 작업하기 위한 환경 구축하기 현재 주로 사용하는 운영체제는 macOS Mojave 10.14.5이다. 이 환경에서 리눅스 커널을 빌드하기 위해서는 GNU GCC, GLIBC 환경이 리눅스와 일치해야 하는데 커널 컴파일을 위해 필요한 헤더파일 경로부터 맞지 않은 부분이 있어 크로스 컴파일러를 준비하는 방법은 포기하고 대신, docker를 사용하기로 했다. 가상머신을 이용하는 방법도 있지만, 도커에 비해 무겁고 GUI 환경이 불필요했기에 도커를 이용해 커널 컴파일을 하는 편이 훨씬 유리하다고 생각했다.\n우분투 이미지로 컨테이너 생성하기 docker를 설치했다는 가정하에, 아래와 같이 컨테이너를 만들었다. 호스트 볼륨을 컨테이너에 맵핑해주었는데 이렇게 해야 커널 해킹한 소스를 바로 빌드하여 맥에서 QEMU를 통해 확인할 수 있기 때문이다.\ndocker run --name kernel_builder -ti -v /Users/sukbeom/Workspaces:/kernel/ ubuntu /bin/bash # 아래 부터는 개발 환경을 위한 패키지 설치이다. apt update \u0026amp;\u0026amp; apt upgrade apt install vim \u0026amp;\u0026amp; vim /etc/apt/sources.list # 편집기 설치 및 미러저장소 경로 설정 apt install git curl libncurses-dev wget gcc make flex build-essential bison linux-headers-generic libelf-dev openssl bc libssl-dev cpio 1. BusyBox 준비 이제 컴파일을 위한 빌드 환경이 준비되었다. 커널 컴파일을 위한 맥에서의 경로는 $HOME/Workspaces/kernel 로 지정하고 컨테이너 내에서는 /kernel 로 접근하도록 설정하였다. 커널은 여기 에서, busybox는 여기 에서 받을 수 있다. 필자가 사용한 버전은 아래와 같다.\nbusybox: 1.30.1 kernel: 4.20.9 busybox란?\nSingle executable 파일 형태로 여러 가지 유닉스 유틸리티를 제공하는 Software suite 이다. 안드로이드 상에서 리눅스와 같은 터미널 환경을 제공하는 termux 애플리케이션을 살펴보면 최초 실행 시 busybox를 설치하는 것을 알 수 있다. 임베디드 환경과 같이 아주 제한된 리소스를 가진 시스템 상에서 필요한 (셸 환경을 위한)최소한의 유틸리티만을 사용하고자 할 때 사용하는 소프트웨어라고 생각하면 된다.\n원래는 커널 분석을 하던 버전(v2.6.39)으로 진행을 하고자 했으나 소스 자체가 오래되었고 GCC 7 버전을 지원하지 않아 플래그와 perl 소스에서 생기는 에러를 고쳐도 컴파일 에러가 나 버전을 바꾸었다. 추후 GCC 버전을 낮춰 다시 한번 빌드 해봐야겠다.\n1-1. BusyBox 빌드 설정 이제 컨테이너 환경으로 돌아가 아래와 같이 (shell)환경변수를 임시로 설정해준 뒤 차례대로 빌드해준다.\nSTAGE=~/Workspaces TOP=$STAGE/custom-kernel mkdir -p $STAGE 다운로드 한 busybox, kernel 소스의 압축을 풀어주고 아래와 같이 busybox를 설정해준다. Busybox settings -\u0026gt; Build BusyBox as a static binary (no shared libs) 항목에 체크해준다. (출처 링크에 가면 친절한 스크린샷과 함께 각 과정들을 자세하게 확인할 수 있다.)\nmake O=$TOP/obj/busybox-x86 menuconfig 1-2. BusyBox 빌드 및 initramfs 디텍토리 구조 만들기 cd $TOP/obj/busybox-x86 make -j3 make install make install을 완료하고 나면 _install 이름으로 디렉토리가 생성된 것을 확인할 수 있다. 이를 이용하여 아래와 같이 initramfs를 생성한다.\nmkdir -pv $TOP/initramfs/x86-busybox cd $TOP/initramfs/x86-busybox mkdir -pv {bin,dev,sbin,etc,proc,sys/kernel/debug,usr/{bin,sbin},lib,lib64,mnt/root,root} cp -av $TOP/obj/busybox-x86/_install/* $TOP/initramfs/x86-busybox sudo cp -av /dev/{null,console,tty,sda1} $TOP/initramfs/x86-busybox/dev/ 1-3. Init 파일 만들기 $TOP/initramfs/x86-busybox/init 파일을 생성한 뒤 아래와 같이 내용을 작성한다.\nmount -t proc none /proc mount -t sysfs none /sys mount -t debugfs none /sys/kernel/debug echo -e \u0026#34;\\nBoot took $(cut -d\u0026#39; \u0026#39; -f1 /proc/uptime) seconds\\n\u0026#34; exec /bin/sh 작성 후에는 실행 권한을 조정한다:\nchmod +x $TOP/initramfs/x86-busybox/init 1-4. initramfs 생성 initramfs는 메모리 기반 디스크 구조, 즉 램디스크이다. 주요 목적은 root 파일 시스템을 마운트 하기 위한 것이며, 일반적인 root 파일 시스템에서 찾아볼 수 있는 디렉토리 구조를 갖고 있다.\n일반적으로, 루트 파일시스템의 init 프로그램으로 제어권을 넘기기 전에, 필요한 파일 시스템을 마운트하는 필수 도구와 스크립트를 포함하고 있는 램디스크로서 initramfs 루트디스크에서 시스템을 준비하는 설정 스크립트를 준비하고 실제 파일 시스템으로 전환한 뒤 init을 실행한다.\n요약하면, 실제 루트파일시스템을 마운트 하고 그 안의 init 프로그램을 실행하기까지의 준비단계를 위한 램디스크라 생각하면 된다.\n그렇다면 그러한 램디스크는 왜 필요한 것일까? 예전 젠투 리눅스를 이용해 리눅스를 설치했던 경험으로 ramdisk 없이도 리눅스를 충분히 사용할 수 있었다. 다만, 루트파티션이 암호화된 경우에는 반드시 램디스크를 통해 로드해야 했고 최근에는 (데스크탑용 리눅스에서) 이러한 램디스크가 필수적이 되어가고 있는 추세인 듯하다.\ncd $TOP/initramfs/x86-busybox find . | cpio -H newc -o \u0026gt; ../initramfs.cpio cd .. cat initramfs.cpio | gzip \u0026gt; $TOP/obj/initramfs.igz 2. 커널 빌드하기 이제 busybox 준비가 끝났으니 리눅스 커널을 빌드해보자.\n2-1. Minimal config로 리눅스 커널 설정하기 cd $STAGE/linux-4.20.9 mkdir -pv $TOP/obj/linux-x86-allnoconfig make O=$TOP/obj/linux-x86-allnoconfig allnoconfig make O=$TOP/obj/linux-x86-allnoconfig -j3 빌드한 커널, busybox 함께 실행하기 qemu-system-x86_64 \\ -kernel $TOP/obj/linux-x86-allnoconfig/arch/x86/boot/bzImage \\ -initrd $TOP/obj/initramfs.igz \\ -nographic -append \u0026#34;earlyprintk=serial,ttyS0 console=ttyS0\u0026#34; 정상적으로 커널이 로드되고 셸이 실행된 것을 확인하면 Control-a x를 입력하여 QEMU를 종료한다.\n출처 Build the linux kernel and busybox and run them on qemu LFS - About initramfs Gentoo Linux - initramfs Build \u0026lsquo;Mini Linux\u0026rsquo; Instruction - forked ", 
        "url": "\/\/localhost:1313\/posts\/linux-%EC%BB%A4%EB%84%90-busybox-%EB%B9%8C%EB%93%9C-%ED%9B%84-qemu%EC%97%90%EC%84%9C-%EC%8B%A4%ED%96%89%ED%95%98%EA%B8%B01\/2\/"
    },
    
    "\/\/localhost:1313\/tags\/osi-7-layers\/": {
        "title": "OSI 7 Layers",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/osi-7-layers\/"
    },
    
    "\/\/localhost:1313\/posts\/osi-7-layers\/": {
        "title": "OSI 7-Layers",
        "tags": ["OSI 7 Layers",],
        "content": "네트워크의 기본 개념인 OSI 7-Layers에 대해서 정리하고자 한다. 학부 시절 달달 외우기만 했던 까닭에 제대로 공부하지 못해 아쉬웠던 7계층 개념인데 외우기보다 이해하는 것이 훨씬 간단하므로, 필자의 경험을 이용해 (나중에 기억하기 쉽도록) 정리하고자 한다.\nOSI 7계층에 대해 설명하기 위해 이 역시 방산업체에서의 경험을 이용하고자 한다. 아래는 OSI 7계층을 나타낸 그림이다. 이 때, 각 레이어들을 데이터들이 송수신될 때 전달되는 경로로 생각하며 그림을 이해해야 한다.\nL1. 물리 계층, Physical Layer 물리 계층은 말 그대로 네트워크 장비들 간에 데이터를 송수신하기 위해 물리적으로 신호를 주고받는 계층이다. 무인 드론 개발 프로젝트에서 각 장비들은 스위치와 광케이블을 이용해 연결되었는데 이러한 1차적인 물리적 인프라들, 광케이블 따위들을 물리 계층의 하나로 이해하면 된다. 광케이블의 경우 선이 길어지면 신호가 약해져 수신측에서 신호 입력을 제대로 받지 못하는 경우가 있기에 리피터를 별도로 달아 사용하기도 한다. 이처럼 물리적인 신호를 주고받는 인프라들을 물리 계층이라 하며, Coax, Fiber, Wireless, Hubs, Repeaters 등이 여기에 포함된다.\n허브(Hubs)와 스위치(Switch) 둘이 헷갈리는 경우가 종종 있는데 출처 를 확인해보면 이해하기 쉬울 것이다. 간략하게 말하면, 허브는 물리적인 신호 전달 경로만 제공하고 스위치는 각 장비들을 구분할 수 있다는 점이 차이점이다.\nL2. 데이터 링크 계층, Data Link Layer 데이터 링크 계층의 주 목적은 잡음이 있는 인접 노드 간의 물리적인 회선을 상위 네트워크 계층이 사용할 수 있도록 전송 에러가 없는 통신 채널로 변환시키는 계층이다. 때문에 데이터 링크 계층의 역할 중 가장 중요한 역할은 **에러 제어(Error Control)**이다.\n드론 개발 프로젝트를 다시 예로 들어보자. 각 장비들은 광 케이블로 연결되고 이러한 광 케이블은 다시 스위치 한 개에 집중되어 연결된다. 마침내 각 장비들은 서로를 MAC 주소를 이용해 식별하고 raw 데이터를 신뢰성 있게 송수신 할 수 있게 되었다. 이처럼 데이터 링크 계층의 역할은 1계층(Physical Layer)에서 raw 데이터들의 흐름을 제어하고 에러를 제어하는 역할을 한다.\n이 계층에는 Ethernet, PPP, Switch, Bridge 등이 포함된다.\n데이터링크 계층 역할 프레이밍(Framing)\n데이터 배열에 Data, Header, Trailer 등을 넣어서 캡슐화를 하는 작업을 말한다. 각각의 노드(송신자, 수신자, 중간 중간의 각종 라우터, 스위치 등의 하드웨어)에서 다음 노드로 데이터를 보낼 때, L3(Network Layer)에서 받은 데이터그램(Datagram)을 캡슐화하며 캡슐화된 틀(데이터 배열)을 프레임(Frame) 또는 패킷(packet)이라 한다.\n흐름제어(Flow Control)\n송신 측이 수신 측의 데이터 처리 속도보다 훨씬 빠른 속도로 데이터를 보낸다면 어떻게 될까?\n수신 측의 버퍼(Buffer)가 점점 더 길어지게 된다. 버퍼의 길이는 제한되어 있기 때문에 버퍼가 가득차면 데이터를 더이상 저장할 수 없어 데이터가 손실된다. 이러한 문제를 보완하기 위해 수신 측에서는 송신 측에게 그만 보내거나 천천히 보내달라는 피드백을 보낼 수 있는데 이러한 것을 흐름제어(Flow Control)라 한다.\n에러제어(Error Control)\n송신 측에서는 프레이밍을 통해 0과 1로 구성된 비트로 변환한 뒤 송신한다. 수신 측에서는 당연히 이 비트 배열들을 받을 텐데, 0과 1을 이루는 전기적 신호들은 매우 취약하기 때문에 수신 측의 L2 계층이 이 전기적 신호를 보고 프레이밍을 해야 한다. 이 과정에서 에러를 검출할 수 있으며, 검출된 에러는 직접 수정하거나 송신 측에 재송신을 요청하여 해결한다.\n데이터링크의 부계층, LLC와 MAC 데이터 링크 계층을 더 세부적으로 나누면 LLC(Logical Link Control) 부계층과 MAC(Media Access Control) 부계층으로 나눌 수 있다. LLC는 Flow Control과 Error Control에 대한 책임을 갖고 있고 MAC은 네트워크 노드 간의 접근과 권한을 관리한다.\n주소 종류 Unicast address: 1 대 1 통신\n모든 기기(송신자, 수신자, 라우터 등)에는 Unicast 주소가 할당되어 있다. Unicast는 1 대 1 통신을 의미하며 L2 계층에서 프레이밍을 할 때 Unicast address를 넣으면 해당 MAC 주소를 가진 곳으로만 간다.\nMulticast address: 1 대 다 통신\n특정 L2 계층의 프로토콜에는 Multicast 주소를 정의하고 있다. Multicast란 1대 다 통신을 의미하며 아무데서나 쓸 수 없고 특정 범위에서만 사용 가능하다.\nBroadcast address: 1 대 전체 통신\nBroadcast는 1 대 전체 통신을 의미한다. Broadcast 주소를 담은 프레임은 모든 기기에 전달되며 일반적인 브로드캐스트 주소는 FF:FF:FF:FF:FF:FF 이다.\nL3. 네트워크 계층, Network Layer 네트워크 계층은 여러 개의 네트워크 간 데이터를 구성하고 전송하는 계층으로 라우팅, 주소체계를 관리하는 계층이라고 이해하면 된다. 네트워크 계층에 대한 간단한 예로, 보스턴에 있는 컴퓨터가 캘리포니아에 있는 서버에 연결하려고 할 때 그 경로는 수백 만 가지다. 이 계층의 라우터가 이러한 작업을 효율적으로 처리한다.\n네트워크 계층에는 라우터와 브릿지 라우터 등이 포함되며 IPv4, IPv6와 같은 인터넷 프로토콜(Internet Protocol)과 ARP가 여기에 포함된다.\n계층 역할 경로제어(라우팅)\n단말기 혹은 컴퓨터가 어떤 망에 접속되어 있을 때 송신하고자 하는 정보가 망을 통해서 어떻게 상대방 수신측까지 전달되는가를 제어하는 일을 담당한다. 네트워크 계층의 패킷이 하위 계층인 데이터 링크 계층으로 내려오면 데이터 링크 계층은 패킷을 프레임 단위로 만들고 바로 인접 노드의 물리 주소를 헤더에 첨부하여 전송하게 된다. 이 때, 목적지 호스트(논리주소)까지 각 노드를 거칠 때마다 인접 노드의 물리 주소를 프레임에 다시 첨부하여 전송하게 되는데, 이처럼 경로를 지정하거나 교환 기능을 제공하는 것을 라우팅이라고 한다.\n투명성\n상위의 전송계층(Transport Layer)이 경로 선택과 중계 등 망 계층 고유의 기능에 대하여 의식하지 않도록 투명하게 하는 역할이다.\n논리적 주소체계 부여\n라우팅과 관련된 주소 지정 등 주소 체계를 정립한다. (IP 주소, 공인 IP, 사설 IP 등) 상위 전송계층에서 받은 데이터에 목적지 호스트의 논리 주소를 헤더에 추가하여 전송한다.\nL4. 전송 계층, Transport Layer 전송 계층은 양단 간 어떤 종류의 망이 사용되었는지 의식하지 않고 쌍방 응용프로세스 간 투명하고 신뢰성 있게 논리적인 통신을 이루는 계층이다. 이 계층은 Error Control, Flow Control, Congestion Control 등의 매커니즘을 제공하며 일반적인 프로토콜은 TCP(Transmission Control Protocol), UDP(User Datagram Protocol) 등이 있다.\n계층 역할 전송 계층의 하위 계층인 네트워크 계층(L2)는 목적지까지 개별적인 패킷(프레임)을 전달하지만, 이들 패킷 간의 관계는 알지 못한다. 다시 말해, 올바른 순서대로 도착하여 정상적인 메시지로 조립되었는지 알지 못하기 때문에 이를 전송 계층에서 담당해야 한다.\n서비스 지점 주소 지정\n컴퓨터는 여러 개의 프로그램을 동시에 실행하는 경우가 많은데 각 프로그램을 서비스 단위 혹은 프로세스로 보고, 전송 계층은 상위 계층에서 내려온 메시지를 세그먼트로 나누어 헤더에 목적지 호스트의 해당 서비스 지점 주소, 즉 포트번호를 붙여 전송한다.\n분할과 재조립\n상위 계층에서 내려온 세그먼트를 전송할 수 있는 세그먼트 단위로 나누면 각 세그먼트는 순서 번호를 갖고 목적지에 도착하면 수신자 측에서 이를 재조립한다.\n흐름 제어\n데이터 링크 계층의 흐름 제어는 노드 대 노드의 흐름제어지만 전송 계층의 흐름제어는 단일 링크가 아닌 종단 간 흐름(end-to-end)을 제어한다.\n오류 제어\n흐름 제어와 마찬가지로 데이터링크 계층에서는 프레임에 대한 오류를 제어하나, 전송 계층은 전체 세그먼트 오류(손상, 유실 등) 없이 수신측 전송 계층에 도착하도록 오류 제어 기능을 수행하며, 오류 교정은 통상 재전송을 통해 이루어진다.\nL5. 세션 계층, Session Layer 여기서부터는 애플리케이션에 관련된 계층이라고 할 수 있다. (물리계층(L1)부터 전송계층(L4)까지의 주된 기능은 단순히 데이터를 전달하는 것이다.) 세션 계층은 응용 프로세스가 통신을 관리하기 위한 방법을 정의하는 계층으로, 세션을 만들고 없애는 역할을 한다. 흔한 예로 HTTP의 세션을 생각하면 이해하기 쉽다.\nL6. 표현 계층, Presentation Layer 네트워크 상의 여러 이기종 시스템들은 저마다 다른 데이터 표현 방식을 사용하는데, 이를 하나의 통일된 구문으로 변환시키는 기능을 수행하는 계층을 말한다. 즉, 네트워크와 애플리케이션 간 데이터 형식을 변환하는 역할을 수행한다.\n표현 계층의 주요 기능은 아래와 같이 정리할 수 있다.\n암호화 압축 코드 변환(ASCII, EBCDIC, Binary 등), 파일 변환 및 문장 축소화 등의 기능 수행 L7. 응용 계층, Application Layer 여러 하위 통신 프로토콜 개체에 대하여 사용 관점의 사용자 인터페이스를 제공하며 응용계층 프로세스들 사이의 통신은 표현계층이 제공하는 서비스를 이용한다. (데이터의 형식을 정하고 부호화하고 암호화, 압축하는 서비스를 통해 이루어진다.)\n응용계층에는 FTP, SMTP, HTTP, HTTPS, TELNET 등이 해당된다.\n계층별 전송 단위 OSI-7 Layers의 각 계층에서 전송 단위는 아래와 같이 정리할 수 있다.\n1계층: 비트 2계층: 프레임 3계층: 패킷 4계층: 세그먼트 5계층: 메시지 계층별 네트워킹 장비 1계층: 리피터, 허브 2계층: 브리지, 스위치 3계층: 라우터 4 - 7계층: 게이트웨이 출처 허브와 스위치의 차이점 데이터링크 계층 OSI 7 Layer OSI 7계층 각 계층별 역할 비유 ", 
        "url": "\/\/localhost:1313\/posts\/osi-7-layers\/"
    },
    
    "\/\/localhost:1313\/tags\/tcp\/": {
        "title": "Tcp",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/tcp\/"
    },
    
    "\/\/localhost:1313\/posts\/tcp%EC%99%80-udp%EC%9D%98-%EC%B0%A8%EC%9D%B4%EC%A0%90\/": {
        "title": "TCP와 UDP의 차이점",
        "tags": ["udp","tcp",],
        "content": "예전에 TCP, UDP에 대해서 정리해놓은 것들을 찾을 수 없어 블로그 페이지로 정리하고자 포스팅을 새로 만들었다. 앞으로 두고두고 찾아보게 될 페이지므로 향후 커널 소스 분석을 통해 TCP, UDP와 관련된 부분이 나올 경우 포스팅을 업데이트 할 예정이다.\n방산 업체에서 무인 정찰기를 개발할 당시에도 중요 데이터는 모두 TCP를 이용해 송수신하고 영상과 같은 정보는 UDP를 이용해 전달받았다. 당시에 구현된 코드를 보면서 이전에 프로토콜 사용에 있어서 어디부터 어디까지를 UDP 혹은 TCP로 해야 하는가에 대해 논란이 있었을거라 예상했지만 코드를 변경할 수 없어 아쉬웠던 적이 있었다.\n각설하고 이 포스팅에서는 리눅스 커널 소스에서 TCP와 UDP, 즉 L4 전송계층에 대한 구현이 어떤 식으로 되어있는지 살펴볼 것이다.\nTCP (Transmission Control Protocol) 먼저, TCP(Transmission Control Protocol)는 connection-oriented 프로토콜이다. 연결이 수립되기 전까지는 데이터를 송수신하지 않으며, 데이터 송수신이 종료되면 반드시 연결을 닫아야 한다. 이러한 특성 덕분에 TCP는 아래와 같은 특징을 갖는다.\n데이터 송수신에 신뢰성이 보장된다. 에러 체크 기능을 제공한다. 수신자에게 패킷이 순서대로 전달된다. UDP보다 상대적으로 느리다. 패킷 손실로 인한 패킷 재전송이 가능하다. 헤더 크기가 20bytes이다. HTTP, HTTPS, FTP, SMTP, Telnet 등에 사용된다. 먼저 앞서 언급한 것들에 대해서 커널 소스를 통해 자세히 살펴보자. tcp에 관련된 코드는 /net/ipv4/tcp.c 하위 경로 내에 위치한다. (분석으로는 IPv4를 먼저 진행하고 충분히 이해한 뒤 추후에 IPv6에 대해서 분석한 뒤 업데이트 하고자 한다.)\n코드를 살펴보면 EXPORT_SYMBOL 코드를 많이 볼 수 있을 것이다. 로드 가능한 모듈이 삽입되면 해당 모듈이 가지고 있는 커널 함수들과 데이터 구조들은 반드시 현재 실행 중인 커널에 로드되어 reference 될 수 있어야 한다. 하지만 모듈을 로드하는 Module Loader는 모듈에서 명시적으로 export 하지 않는 한 모듈이 가지고 있는 모든 심볼들을 export 하지 않는다.\n또 한가지 재밌는 점은 라이센스에 따라 해당 심볼로의 접근을 제한할 수 있다는 것이다. cscope를 통해 탐색해보면 알겠지만, __EXPORT_SYMBOL 함수에는 심볼 이름 외에 라이센스 그룹에 대한 정보를 전달하게 되어 있어 같은 라이센스 구릅인지 아닌지를 구분하여 액세스 여부를 허용/거부 하도록 되어 있다.\n초기화 코드 void __init tcp_init(void) { struct sk_buff *skb = NULL; unsigned long nr_pages, limit; int i, max_share, cnt; unsigned long jiffy = jiffies; BUILD_BUG_ON(sizeof(struct tcp_skb_cb) \u0026gt; sizeof(skb-\u0026gt;cb)); percpu_counter_init(\u0026amp;tcp_sockets_allocated, 0); percpu_counter_init(\u0026amp;tcp_orphan_count, 0); tcp_hashinfo.bind_bucket_cachep = ... TCP 초기화 코드는 __init 이라는 키워드와 함께 심볼이 정의되어 있다.\n__init에 관한 설명은 $KERNEL_SOURCE/arch/um/include/shared/init.h 에서 찾아볼 수 있다. 이 키워드는 해당 함수를 초기화 함수로 표시하는 역할을 한다. 커널은 초기화 프로세스 도중에 초기화에 관련된 것으로 간주하고 메모리 리소스를 해제하거나 초기화 프로세스 과정에서 표시된 함수나 데이터들을 이용한다.\n링크 를 살펴보면 재미있는 예시가 하나 있다. \u0026lsquo;Freeing unused kernel memory\u0026rsquo;와 같은 메시지를 나타내는 것을 그 예로 들 수 있다.\n출처 https://www.geeksforgeeks.org/differences-between-tcp-and-udp/ LWN - On the value of EXPORT_SYMBOL_GPL Naver D2 - TCP/IP 스택 이해하기 ", 
        "url": "\/\/localhost:1313\/posts\/tcp%EC%99%80-udp%EC%9D%98-%EC%B0%A8%EC%9D%B4%EC%A0%90\/"
    },
    
    "\/\/localhost:1313\/tags\/udp\/": {
        "title": "Udp",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/udp\/"
    },
    
    "\/\/localhost:1313\/categories\/web\/": {
        "title": "Web",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/categories\/web\/"
    },
    
    "\/\/localhost:1313\/tags\/process\/": {
        "title": "Process",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/process\/"
    },
    
    "\/\/localhost:1313\/posts\/%EC%BB%A4%EB%84%90-%ED%95%B4%ED%82%B9-%ED%94%84%EB%A1%9C%EC%84%B8%EC%8A%A4\/": {
        "title": "커널 해킹: 프로세스",
        "tags": ["process",],
        "content": "앞으로 리눅스 커널 공부를 해가면서 포스팅으로 정리해둘 계획이다. 디바이스 드라이버 개발 이전에 리눅스라는 운영체제에 대해 개인적으로 정리해야할 필요성을 느꼈다. 그리고 최신 커널을 사용하기 보다 상대적으로 오래된 커널을 시작으로 공부하고자 한다. 기본 틀은 크게 변하지 않았을거라 생각하고 충분히 이해한 뒤에 최근 버전을 받아 개발 흐름을 이해하는 것이 옳은 방법이라 생각하였다.\n이 문서에서는 리눅스/유닉스 운영체제의 기본 추상화 개념 중 하나인 프로세스에 대해 정리하고자 한다. 정리에 필요한 정보 수집을 위해서 Linux kernel development(3rd edition) 책과 LWN 등의 사이트들을 참고한다.\n문서는 계속해서 업데이트할 예정이며, 내용이 추가되면 문서를 분리하거나 링크를 통해 업데이트할 예정이다.\n프로세스(Process)란? 프로세스는 실행 중인 프로그램으로 다음과 같은 리소스들을 포함한다.\n프로세스 디스크립터, PCB 프로세스 디스크립트(Process Descriptor)는 Task Descriptor 또는 Process Control Block이라고도 불린다. 프로세스의 전반적인 정보들을 담고 있으며 대표적인 것들은 아래와 같다.\n프로세스가 사용 중인 파일\nstruct files_struct *files; 파일 디스크립터 테이블을 포함한다. 이 정보는 태스크(프로세스를 커널 내부에서는 Task라 일컫는다.) 간에 공유할 수 있으며 CLONE_FILES를 이용해 특정할 수 있다.\n파일시스템 정보\nstruct fs_struct *fs; 처음 소스를 보자마자 이해하기 어려운 부분이었다. 어째서 프로세스가 파일시스템 정보까지 가지고 있어야 하는가? 출처에서 이 부분은 아래와 같이 설명하고 있다.\nroot directory\u0026rsquo;s dentry and mountpoint. alternate root directory\u0026rsquo;s dentry and mountpoint. current working directory\u0026rsquo;s dentry and mountpoint. 즉, ext4, xfs 와 같은 파일시스템의 정보가 아니라 프로세스 실행 환경을 위한 루트 디렉토리의 엔트리 정보와 마운트 포인트 정보를 가지고 있는 것이다. 이 부분에 대해서는 나중에 좀 더 알아봐야겠다.\n대기 중인 시그널과 시그널 핸들러\nstruct signal_struct *signal; struct sighand_struct *sighand; 파일시스템 정보와 마찬가지로 clone 된 태스크들과 공유할 수 있는 정보이며 CLONE_SIGHAND를 통해서 특정할 수 있다.\n프로세서 상태\n/* -1 unrunnable, 0 runnable, \u0026gt;0 stopped */ volatile long state; 프로세스의 상태를 volatile이라는 키워드와 함께한 변수로 담고 있으며 단순하게 unrunnable, runnable, stopped 등으로 구분하고 있다.\n여기서 volatile은 왜 사용된 걸까? TLDP 출처에는 \u0026ldquo;The volatile in p-\u0026gt;state declaration means it can be modified asynchronously (from interrupt handler) 라고 설명되어 있다. 그런데 커널 문서 를 살펴보면 \u0026ldquo;volatile\u0026rdquo; 타입 클래스를 사용해서는 안되는지에 대한 설명이 나와있는데 실제로 state 변수를 제외하고는 sched.h 나머지 어디에도 사용되고 있지 않다. 그 이유는 다음과 같다:\nvolatile의 목적은 최적화를 막는 것이다. 커널은 데이터 구조들을 원치 않은 동시 접근(concurrent access)로부터 철저하게 보호해야 하는데 그러한 보호 과정으로 최적화에 관련된 문제들을 더 효과적인 방법으로 피해갈 수 있다.\nvolatile과 같이 커널에는 동시 접근으로부터 데이터를 보호하기 위해 spinlocks, mutexes, memory barriers 등으로 원치 않은 최적화를 막기 위해 설계했다. 문서에는 \u0026ldquo;그러한 설계된 도구들을 충분히 잘 활용할 수 있다면 volatile을 사용할 이유가 없고, volatile이 여전히 필요하다면 대부분 코드 어딘가에 버그가 내재된 것이다\u0026rdquo; 라고 설명하고 있다.\n하지만, 프로세스 상태와 같이 멀티 프로세스 환경에서 반드시 여러 개의 쓰레드에 공유되어야 하는 변수들은 volatile을 사용하여 컴파일러가 최적화하는 것을 막는다.\n프로세스의 상태는 매크로 형태로 정의되어 있다. 커널 버전(5.x)에서는 값들이 Hex 값으로 정의되어 있는 반면, 2.6.x 버전에서는 단순히 정수형으로 정의되어 있다. 이전 커널과의 주요 차이점은 Task의 상태를 TASK_RUNNING, TASK_INTERRUPTIBLE, TASK_UNINTERRUPTIBLE, TASK_DEAD, TASK_WAKEKILL, TASK_WAKING 등으로 구분하고 종료 시점의 상태를 별도로 구분하여 EXIT_ZOMBIE, EXIT_DEAD 등으로 정의해 놓았다는 점이다.\n하나 이상의 물리적 메모리 영역이 할당된 메모리 주소 공간\n메모리 관리용 데이터 구조로서(객체라는 말은 사용하지 않겠다.) task_struct 안에는 해당 정보들을 받아올 수 있는 mm_struct를 포함하고 있다.\nstruct mm_struct *mm, *active_mm; 실행 중인 하나 이상의 스레드 정보\n/* CPU-specific state of this task */ struct thread_struct thread; 역시 다른 정보들과 마찬가지로 thread에 대한 정보를 가지고 있다. 리눅스에서는 프로세스와 스레드를 구분하지 않고 모두 스레드로 관리하고 스케쥴링한다. 둘의 차이점은 공유 자원을 다른 스레드와 공유하느냐 공유하지 않느냐에 따라 구분하며, 프로세스가 생성되면 커널 내부에서는 프로세스가 아닌 스레드 한 개가 생성된 것과 같다.\n출처 https://www.tldp.org/LDP/tlk/tlk.html https://www.tldp.org/LDP/lki/lki-2.html Linux Kernel Development 3rd edition https://www.kernel.org/doc/html/latest/process/volatile-considered-harmful.html ", 
        "url": "\/\/localhost:1313\/posts\/%EC%BB%A4%EB%84%90-%ED%95%B4%ED%82%B9-%ED%94%84%EB%A1%9C%EC%84%B8%EC%8A%A4\/"
    },
    
    "\/\/localhost:1313\/posts\/copy-on-write-cow-or-cow\/": {
        "title": "Copy on Write (CoW or COW)",
        "tags": ["COW","shadowing","implicit sharing",],
        "content": "Copy-on-write은 리눅스 커널의 fork() 함수에서 사용하는 기법이다.\n전통적인 fork()는 부모 프로세스의 모든 자원을 복사해 자식 프로세스에게 넘겨준다. 하지만 이러한 방식은 공유가 가능한 많은 데이터를 복사하므로 단순하고 비효율적이다. 게다가 새로 만든 프로세스가 곧바로 다른 프로그램을 실행한다면 복사 작업이 모두 헛수고가 되고 만다.\n이러한 문제를 해결하기 위해 리눅스에서는 Copy-on-write, COW 기법을 이용하는데 기록사항 발생 시에 복사하는 기능으로 즉각적인 데이터의 복사를 지연하거나 방지하는 기법이다. 때문에 fork()를 사용하게 되면 프로세스의 주소 공간을 모두 복사하는 대신, 부모와 자식 프로세스가 같은 공간을 공유하고 있다가 기록 사항이 발생했을 때 사본을 만든다. (그 전까지는 읽기전용 상태로서 복사를 지연한다.)\nCopy-on-write는 때때로 implicit sharing, shadowing이라고도 불리며 컴퓨터 프로그래밍에서 자원 관리 기법 중 하나로서 사용되는 기법이다. write라는 연산이 발생할 가능성이 있는 상태에서는 write가 전혀 발생하지 않을 가능성도 있는 법이다. 이러한 경우 Copy-on-write 기법은 자원 관리 측면에서 좋은 최적화 기법이 될 수 있다.\n예제 아래는 위키 페이지에 나와있는 간단한 예제이다. 변수를 위한 메모리 할당에 있어서 아래 상황에서 COW를 사용할 수 있다.\nstd::string x(\u0026#34;Hello\u0026#34;); std::string y = x; // x and y use the same buffer y += \u0026#34;, World!\u0026#34;; // now y uses a different buffer, x still uses the same old buffer 출처 https://en.wikipedia.org/wiki/Copy-on-write Chapter 3. Process in \u0026ldquo;Linux Kernel Development 3rd Edition\u0026rdquo; ", 
        "url": "\/\/localhost:1313\/posts\/copy-on-write-cow-or-cow\/"
    },
    
    "\/\/localhost:1313\/tags\/cow\/": {
        "title": "COW",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/cow\/"
    },
    
    "\/\/localhost:1313\/tags\/implicit-sharing\/": {
        "title": "Implicit Sharing",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/implicit-sharing\/"
    },
    
    "\/\/localhost:1313\/tags\/shadowing\/": {
        "title": "Shadowing",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/shadowing\/"
    },
    
    "\/\/localhost:1313\/tags\/drivers\/": {
        "title": "Drivers",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/drivers\/"
    },
    
    "\/\/localhost:1313\/tags\/modules\/": {
        "title": "Modules",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/modules\/"
    },
    
    "\/\/localhost:1313\/posts\/%EC%BB%A4%EB%84%90-%EB%AA%A8%EB%93%88%EA%B3%BC-%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B2%84%EC%9D%98-%EC%B0%A8%EC%9D%B4\/": {
        "title": "커널 모듈과 드라이버의 차이",
        "tags": ["modules","drivers",],
        "content": "오랜만에 커널 공부를 다시 시작하면서 소스 트리를 다시 살펴보게 되었다. 분명히 예전에도 같은 질문을 가졌겠거니 생각하면서 트리를 보고 난 후의 첫 질문을 정리하고자 한다.\n커널 디렉토리 구조는 대략 다음과 같이 구성되며, 그 중 drivers와 modules의 차이점이 이해하기가 어려웠다.\narch/ - 특정 아키텍처에 국한된 코드 include/ - 커널 빌드를 위해 포함하는 include 파일들 init/ - 커널 초기화 코드 mm/ - 메모리 관리 코드 drivers/ - 드라이버 ipc/ - IPC (Inter Process Communication) modules/ - 커널 모듈 fs/ - 파일시스템 kernel/ - 커널 코드 net/ - 네트워킹 코드 lib/ - 커널에서 사용하는 라이브러리 scripts/ - awk, tk와 같은 스크립트들(커널 configure 시에 사용) 구글링을 해보니 역시나 같은 생각을 한 사람이 있었다. 링크 를 참고하면 커널 모듈은 윈도우즈의 DLL과 같이 커널 런타임에서 로드될 수 있는 컴파일된 코드로 설명되고 드라이버는 하드웨어를 운용하는 코드로 설명하고 있다. 단, 하드웨어 드라이버 중에서도 모듈 형태로 배포되는 것이 간혹 있기 때문에 모든 하드웨어 드라이버가 반드시 drivers 디렉토리에 포함되는 것처럼 설명할 수는 없다고 한다.\n", 
        "url": "\/\/localhost:1313\/posts\/%EC%BB%A4%EB%84%90-%EB%AA%A8%EB%93%88%EA%B3%BC-%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B2%84%EC%9D%98-%EC%B0%A8%EC%9D%B4\/"
    },
    
    "\/\/localhost:1313\/tags\/nonpreemptive\/": {
        "title": "Nonpreemptive",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/nonpreemptive\/"
    },
    
    "\/\/localhost:1313\/tags\/scheduling\/": {
        "title": "Scheduling",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/scheduling\/"
    },
    
    "\/\/localhost:1313\/posts\/%EB%B9%84%EC%84%A0%EC%A0%90-%EC%8A%A4%EC%BC%80%EC%A5%B4%EB%A7%81nonpreemptive-scheduling\/": {
        "title": "비선점 스케쥴링(Nonpreemptive Scheduling)",
        "tags": ["nonpreemptive","scheduling",],
        "content": " 비선점 스케쥴링(Non-preemptive Scheduling) 비선점 스케쥴링(Non-preemptive Scheduling)은 프로세스가 자원을 할당받았을 경우 자원을 스스로 반납할 때까지 계속 그 자원을 사용하도록 허용하는 정책이다. 적용할 때는 현재 프로세서를 사용하는 프로세스가 생성되거나 현재 프로세스 작업을 종료할 때 입출력하기 위해 스스로 프로세서를 반납할 때까지 우선 순위가 높은 프로세스의 비선점 정책을 적용하면 프로세스의 종료 시간을 비교적 정확하게 예측할 수 있다.\n설명하기 전에 프로세스가 입출력 중심의 대화형 프로세스인지, 프로세서 실행 중심의 프로세스인지를 먼저 구분하는 것이 스케쥴링 정책을 적용하는 단계의 첫 번째이다. 물론 두 가지 특성은 상호 배타적이지 않기 때문에 입출력 중심이면서 프로세서를 많이 사용하는 프로세스일 수 있다. 가령, X Window Server의 경우 프로세서와 입출력 모두를 많이 사용한다. 워드프레스의 경우도 마찬가지이다. 보통은 키 입력을 기다리고 있지만 철자법 검사나 매크로 계산 같은 복잡한 기능을 사용하는 순간에는 프로세서를 많이 사용한다.\n스케쥴링의 목적 상충되는 두 가지 목적을 달성하는 것이 스케쥴링의 목적이다. 프로세스 응답시간을 빠르게 하는 것과 시스템 사용률을 최대화하는 것이 그것이다. 이런 모순된 요구사항을 만족시키기 위해 스케쥴러는 복잡한 알고리즘을 사용해 우선순위가 낮은 프로세스에게 공정함을 보장하면서도 순간순간 실행 가치가 가장 높은 프로세스를 선택한다.\n비선점 스케쥴링 정책 종류 정책(Policy)은 스케쥴러가 무엇을 언제 실행할 것인지를 정하는 동작을 말한다. 스케쥴러의 정책을 통해 시스템의 전체적인 느낌이 정해지는 경우가 많으며 프로세서 사용 시간을 최적화하는 책임이 있기 때문에 매우 중요하다.\n아래부터는 비선점 스케쥴링 정책 각각에 대해 좀 더 자세히 기술한다. 처음 문서 작성 시에는 스케쥴링의 알고리즘이라고 기술했지만 정책(policy)이라는 용어가 더 적합하여 수정하였다.\n1. 우선순위 스케쥴링(Priority Scheduling) 우선순위 스케쥴링은 간단하다. 각각의 프로세스에 우선순위가 있고 이 우선순위를 판별하여 우선순위가 더 높다고 판단되는 프로세스가 가장 먼저 프로세서를 할당받는다. 프로세스 우선순위는 커널이 결정하거나 커널 외부에서 결정하기도 하며, 우선순위를 나타내는 값이 작은 수를 지향하는지 큰 수를 지향하는 지는 운영체제에 따라 다르다.\n이런 우선순위 스케쥴링의 가장 큰 문제점은 기아 현상이다. 나중에 소개할 SJF(Shortest Job First)에도 공통적인 문제로서 우선순위가 상대적으로 낮은 프로세스가 계속해서 실행되지 못하는 현상이다.\n예를 들어, 아래와 같이 각 프로세스에 우선순위가 할당되어 있다고 가정해보자. 이를 해결하는 기법은 에이징(Aging) 기법으로, 일정 시간이 지나면 기아 상태에 빠질 것으로 예상되는 프로세스의 우선순위를 높이는 기법이다.\n2. SJF(Shortest Job First), SRTF(Shortest Remaining Time First) 출처 의 내용을 정리한 것이다. 원래 SJF만을 다루려 했지만 SRTF라는 정책이 추가로 정리되어 있기에 포스팅에 추가하였다.\nSJF(Shortest Job First)는 CPU의 Burst Time이 짧은 프로세스에게 프로세서를 우선 할당하는 정책이다. 중요한 것은 Burst Time을 이용한다는 점으로, 프로세스의 전반적인 실행시간이 아니라 실제로 프로세서를 이용하는 시간이 가장 짧은 프로세스부터 실행하여 효율을 높인 정책이다. 만약 스케쥴링 대상의 프로세스들이 모두 같은 Burst Time을 갖고 있다면 FCFS(First-Come-First-Served) 정책을 따른다. FCFS에 대한 설명은 바로 다음에서 기술하겠다.\nSRTF(Shortest Remaining Time First)는 SJF에 선점 정책을 도입한 것이라 이해한다. 아래 다이어그램은 SJF(Shortest Job First)를 이용했을 때의 결과이다. 우선순위와 마찬가지로 기아 현상이 나타날 수 있기 때문에 Aging 기법을 이용하여 해결 가능하다.\n아래는 SRTF 스케쥴링을 같은 프로세스에 적용할 때를 나타낸 것이다. SJF(Shortest Job First)와는 다르게 P1 실행 도중 P2, P3, P4가 선점하여 실행되는 것을 확인할 수 있다. 하지만 여기서 예상할 수 있듯이 잦은 프로세스 잔여 실행시간을 계산해야 하고 이에 따른 컨텍스트 전환이 발생하면 그로 인한 오버헤드가 증가할 수 밖에 없다. 따라서 현실적으로 구현 및 사용이 어려운 정책이다.\n3. FCFS(First-Come-First-Served), FIFO Scheduling 가장 간단한 스케쥴링 정책이다. 프로세서를 먼저 요청한 프로세스에게 할당하는 방식이다. 비선점인 데다 실행 순서에 대한 정책이 없어 스케쥴러 큐에 들어온 순서대로 프로세스를 실행한다. 너무 간단하기 때문에 여기에 대한 별도의 다이어그램은 추가하지 않겠다.\n4. 기한부 스케쥴링(Deadline Scheduling) Deadline 스케쥴링 정책은 실시간을 위한 비선점형 스케쥴링 정책이다.\n실시간 시스템에서 \u0026ldquo;Correct behavior\u0026quot;란 논리적인 행동 뿐만 아니라 사용자 또는 시스템이 원하는 deadline 이내에 결과를 도출하는 것이다. 만약 설정한 deadline 이내에 결과를 도출하여 전달하지 못한다면 시스템은 결함이 있다고 보여질 것이다.\n특히 리눅스와 같은 멀티태스킹 운영체제에서 realtime schedular는 모든 실시간 작업들이 deadline 안에 끝나는 것을 보장해야 한다. 그리고 이러한 타이밍에 대한 요구사항을 만족시키기 위해서 리눅스에서는 두 가지 스케쥴러를 제공하는데 바로 POSIX Realtime Schedular와 Deadline Schedular가 그것이다.\nDeadline 스케쥴러는 EDF(Earliest Deadline First) + CBS(Constant Bandwidth Server) 알고리즘 기반으로 동작한다. 처음 UP 시스템용으로 구현되었다가 SMP 시스템에서는 이를 더 발전시켜 각 CPU의 earliest deadline을 관리하고 다른 CPU로 전달하여 더욱 효과적으로 스케쥴링한다.\n출처 https://haun25ne.tistory.com/53 http://jake.dothome.co.kr/dl-scheduler/ ", 
        "url": "\/\/localhost:1313\/posts\/%EB%B9%84%EC%84%A0%EC%A0%90-%EC%8A%A4%EC%BC%80%EC%A5%B4%EB%A7%81nonpreemptive-scheduling\/"
    },
    
    "\/\/localhost:1313\/tags\/deadlock\/": {
        "title": "Deadlock",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/deadlock\/"
    },
    
    "\/\/localhost:1313\/posts\/%EA%B5%90%EC%B0%A9%EC%83%81%ED%83%9Cdeadlock\/": {
        "title": "교착상태(Deadlock)",
        "tags": ["deadlock",],
        "content": " 교착 상태(Deadlock) 두 개 이상의 작업이 서로 상대방의 작업이 끝나기만을 기다리고 있기 때문에 아무 것도 완료되지 않는 상태를 가리킨다. 이 문제를 해결하는 일반적인 방법은 아직 없는 상태다.\n교착 상태는 하나의 사다리에 두 명이 사람이 매달려 있는 상황을 예상할 수 있다. 위쪽의 사람은 아래로 내려오려고 하고 아래에 있는 사람은 위로 올라가려고 한다면 서로 상대방이 사다리에서 비켜줄 때까지 계속 기다리고 있을 것이고 결과적으로 아무도 사다리에서 내려오거나 올라가지 못하듯이 교착 상태란 다중 프로그래밍 환경에서 흔히 발생할 수 있는 문제이다.\n교착 상태의 관리 교착 상태의 예방 1. 상호 배제 조건 제거 교착 상태는 두 개 이상의 프로세스가 공유자원을 사용할 때 발생하므로 공유 불가능한 조건을 제거하면 교착상태를 해결할 수 있다. (애초에 다른 프로세스가 대기상태로 빠지는 것을 방지)\n2. 점유와 대기 조건 제거 한 프로세스에 수행되기 전에 모든 자원을 할당시키고 나서 점유하지 않을 때에는 다른 프로세스가 자원을 요구하도록 하는 방법이다. 자원 과다 사용으로 인한 비효율성, 프로세스가 요구하는 자원을 파악하는 데 드는 비용, 자원에 대한 내용을 저장 및 복원하는 비용, 기아 상태, 무한 대기 등의 문제점이 있다.\n3. 비선점 조건 제거 비선점 프로세스에 대해 선점 가능한 프로토콜을 만들어 준다.\n4. 환형 대기 조건 제거 자원 유형에 따라 순서를 할당한다.\n출처 https://ko.wikipedia.org/wiki/%EA%B5%90%EC%B0%A9_%EC%83%81%ED%83%9C ", 
        "url": "\/\/localhost:1313\/posts\/%EA%B5%90%EC%B0%A9%EC%83%81%ED%83%9Cdeadlock\/"
    },
    
    "\/\/localhost:1313\/tags\/default-export\/": {
        "title": "Default Export",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/default-export\/"
    },
    
    "\/\/localhost:1313\/tags\/named-export\/": {
        "title": "Named Export",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/named-export\/"
    },
    
    "\/\/localhost:1313\/posts\/named-export%EC%99%80-default-export\/": {
        "title": "Named export와 Default export",
        "tags": ["named export","default export",],
        "content": " 개요 모듈 프로그래밍 기반인 자바스크립트는 모듈 방식은 처음 접했을 때 이해가 되지 않았다. 특히 default export와 named export 라는 export 방식과 자바스크립트 버전에 따른 문법 호환 때문에 모듈 export와 import, require를 사용하는 코드를 이해하기 힘들었다.\n이 문서에서는 타입스크립트를 이용하여 default, named export 각각을 구현한 뒤 import, require 키워드 각각을 이용하였을 때 레퍼런스 변수가 어떤 값을 가지고 있는지 확인한다.\n직접 확인해보자 첫 번째 테스트 먼저 export할 테스트 클래스를 간단하게 구현한다.\n// Named Export를 위한 클래스 export class NamedExportClass { test() { console.log(\u0026#34;Named Export Class\u0026#34;) } } // Default Export를 위한 클래스: 키워드 default를 갖는다. export default class DefaultExportClass { test () { console.log(\u0026#34;Default Export Class\u0026#34;) } } 이제 import하는 소스 코드를 구현해준뒤 tsc 명령어를 통해 타입스크립트 파일들을 자바스크립트 파일로 Trans-compile 한다.\nimport DefaultClass, {NamedExportClass} from \u0026#39;./exportObj\u0026#39; const defaultObj = new DefaultClass() const namedObj = new NamedExportClass() defaultObj.test() namedObj.test() 이 때, 눈여겨 봐야할 점은 export default class ...로 구현한 클래스의 이름을 DefaultExportClass가 아닌 DefaultClass(임의 이름)으로 정하여 import했다는 것이다. 소스 내의 이름 DefaultClass 이름 대신 다른 이름을 사용하여도 테스트 결과는 같다.\n➜ jsImport git:(master) ✗ tsc *.ts \u0026amp;\u0026amp; node importTest Default Export Class Named Export Class 두 번째 테스트 이번에는 import 시에 require 키워드를 이용해서 그 결과를 확인해보자.\nimport constObj = require(\u0026#39;./exportObj\u0026#39;) console.log(constObj) 테스트 결과는 아래와 같다.\n{ __esModule: true, NamedExportClass: [Function: NamedExportClass], default: [Function: DefaultExportClass] } 즉, export 객체 자체를 반환하게 된다.\n세 번째 테스트 그렇다면, 클래스가 아닌 객체를 defaul export 로 정의하면 어떻게 될까? 기존 코드에서 new DefaultClass()는 실행이 불가능한 코드가 되고 DefaultClass 자체가 객체를 가리키게 된다. 하지만 require를 이용해 가져온 constObj는 export를 가리키므로 아래와 같은 결과를 가지게 된다.\nDefault Export Class Named Export Class { __esModule: true, NamedExportClass: [Function: NamedExportClass], DefaultExportClass: [Function: DefaultExportClass], default: DefaultExportClass {} } 정리 간단히 정리하면, require를 이용해 import를 할 경우에는 export 객체 자체를 가져오며, default export와 named export를 구분해서 제대로 사용하고자 한다면 import를 이용해 대괄호({ })를 이용해 사용한다.\n출처 https://medium.com/@etherealm/named-export-vs-default-export-in-es6-affb483a0910 ", 
        "url": "\/\/localhost:1313\/posts\/named-export%EC%99%80-default-export\/"
    },
    
    "\/\/localhost:1313\/categories\/typescript\/": {
        "title": "Typescript",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/categories\/typescript\/"
    },
    
    "\/\/localhost:1313\/categories\/machine-learning\/": {
        "title": "Machine Learning",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/categories\/machine-learning\/"
    },
    
    "\/\/localhost:1313\/tags\/scikit-learn\/": {
        "title": "Scikit Learn",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/scikit-learn\/"
    },
    
    "\/\/localhost:1313\/tags\/tensorflow\/": {
        "title": "Tensorflow",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/tensorflow\/"
    },
    
    "\/\/localhost:1313\/posts\/%ED%85%90%EC%84%9C%ED%94%8C%EB%A1%9C%EC%9A%B0tensorflow%EC%99%80-%EC%82%AC%EC%9D%B4%ED%82%B7%EB%9F%B0scikit-learn%EC%9D%98-%EC%B0%A8%EC%9D%B4\/": {
        "title": "텐서플로우(Tensorflow)와 사이킷런(Scikit-learn)의 차이",
        "tags": ["tensorflow","scikit learn",],
        "content": "책장 속에서 잊혀져 갔던 머신러닝 책을 꺼내 읽기 시작했다. 책을 받았을 당시에는 회사 출장으로 읽을 시간이 없었는데 이제서야 몇 개월이 지나서야 마음이 안정되어 이 책을 꺼내보게 되었다.\n각설하고, 책의 모든 내용이 사이킷런(Scikit-learn)을 이용하는데 문득 텐서플로우(Tensorflow)와의 차이점이 무엇인지 궁금해졌다. 통상적으로 머신러닝이라 하면 텐서플로우를 많이 쓰는데, 굳이 사이킷런을 사용하는 이유가 있을까 궁금해졌다.\n왜 라이브러리가 아닌 프레임워크라 부르는지 모르겠지만, 이들은 분류, 회귀, 클러스터링, 비정상행위 탐지, 데이터 준비를 위한 다양한 학습 방법을 다루며 인공 신경망 메서드를 포함할 수도, 포함하지 않을 수도 있다.\n차이점 출처에 따르면, 텐서플로우는 상대적으로 로우레벨 라이브러리에 가깝고 사이킷런은 하이레벨 라이브러리에 가깝다. 텐서플로우는 신경망이나 딥러닝을 위해 사용되는 데이터 계산, 연산을 위한 라이브러리며 신경망 네트워크 레이어 정의를 위한 메서드도 제공한다. 하지만 결정 트리, 논리 회귀, K-Means, PCA와 같은 머신러닝 메서드는 제공하지 않는다.\n이에 비해, 사이킷런(Scikit-learn)은 데이터 마이닝과 머신러닝을 위한 라이브러리다. 딥러닝이나 강화 학습을 다루지 않지만 지도 학습, 비지도 학습에 관련된 다양한 메서드를 제공하기 때문에 간단하게 학습 알고리즘을 사용하고자 한다면 사이킷런이 사용하기 쉽다는 장점이 있다.\n출처 https://m.blog.naver.com/PostView.nhn?blogId=kimkanu\u0026logNo=221116429423\u0026proxyReferer=https%3A%2F%2Fwww.google.com%2F https://www.quora.com/What-are-the-main-differences-between-TensorFlow-and-SciKit-Learn ", 
        "url": "\/\/localhost:1313\/posts\/%ED%85%90%EC%84%9C%ED%94%8C%EB%A1%9C%EC%9A%B0tensorflow%EC%99%80-%EC%82%AC%EC%9D%B4%ED%82%B7%EB%9F%B0scikit-learn%EC%9D%98-%EC%B0%A8%EC%9D%B4\/"
    },
    
    "\/\/localhost:1313\/tags\/%EB%B9%84%EC%A7%80%EB%8F%84-%ED%95%99%EC%8A%B5\/": {
        "title": "비지도 학습",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/%EB%B9%84%EC%A7%80%EB%8F%84-%ED%95%99%EC%8A%B5\/"
    },
    
    "\/\/localhost:1313\/posts\/%EB%B9%84%EC%A7%80%EB%8F%84-%ED%95%99%EC%8A%B5-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98%EA%B3%BC-%EC%A7%80%EB%8F%84-%ED%95%99%EC%8A%B5\/": {
        "title": "비지도 학습 알고리즘과 지도 학습",
        "tags": ["비지도 학습","지도 학습",],
        "content": "개요 프로젝트 진행을 위해 필요한 머신러닝을 배우기 위해 책을 펼쳤다. 제일 먼저 나오는 개념이 지도 학습(Supervised Learning)과 비지도(Unsupervised Learning)이었는데 어디에도 학습이 정확이 무엇인지에 대한 내용이 없었다.\n학습이란 무엇인가? 질문에 대한 답은 쉽게 찾을 수 있었다.(해당 링크 )\nA model with a set of parameters transforms the input into an output, this generates a signal from which the model updates the parameters to produce a new output. In the supervised learning scenario, the signal is the error between the model\u0026#39;s estimate and the ground truth (the correct answer). In an unsupervised sceneario it can be a constraint (a measure of sparsity, maintaining low entropy within the parameters, maximizing the separation between cluster centroids). In reinforcement learning, the model receives a signal and can differentiate how positive of a reward it is (e.g. you\u0026#39;re doing better, keep going, or colder colder colder). The learning is basically what happens after the model has produced an estimate given a new observation and a signal directing it towards minimizing a cost (= maximizing an objective). An additional requirement for the model is to perform similarly for multiple and different operations. 머신 러닝에서 학습이란, 기본적으로 새로운 데이터가 주어졌을 때 설계한 모델이 최대한 객관성을 가지도록 하기 위해 일어나는 것이다. 그러한 학습이 일어나는 이벤트를 출처(위)에서는 generating a signal라고 표현했다.\n지도 학습 (Supervised Learning) 지도 학습은 이미 알려진 사례를 바탕으로 일반화된 모델을 만들어 의사 결정 프로세스를 자동화하는 것들이다. 다시 말해, 알고리즘에 입력과 기대하는 출력을 제공하고 알고리즘은 주어진 입력에서 원하는 출력을 찾는다. 지도 학습의 예는 아래와 같다.\n편지 봉투에 손으로 쓴 우편번호 숫자 판별\nIN: 스캔한 편지 봉투 이미지 OUT: 우편 번호 (눈으로 확인해서 원하는 출력 값을 기록해놔야 한다.) 의료 영상 이미지에 기반한 종양 판단\nIN: 이미지 OUT: YES or NO 비지도 학습 비지도 학습은 알고리즘에 입력은 주어지지만 기대하는 출력값은 주어지지 않는다. 블로그 글의 주제 구분, 고객들을 취향이 비슷한 그룹으로 묶기, 비정상적인 웹사이트 접근 탐지 등을 예로 들 수 있다.\n출처 https://www.quora.com/What-does-learning-mean-in-machine-learning ", 
        "url": "\/\/localhost:1313\/posts\/%EB%B9%84%EC%A7%80%EB%8F%84-%ED%95%99%EC%8A%B5-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98%EA%B3%BC-%EC%A7%80%EB%8F%84-%ED%95%99%EC%8A%B5\/"
    },
    
    "\/\/localhost:1313\/tags\/%EC%A7%80%EB%8F%84-%ED%95%99%EC%8A%B5\/": {
        "title": "지도 학습",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/%EC%A7%80%EB%8F%84-%ED%95%99%EC%8A%B5\/"
    },
    
    "\/\/localhost:1313\/categories\/java\/": {
        "title": "Java",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/categories\/java\/"
    },
    
    "\/\/localhost:1313\/tags\/nested-class\/": {
        "title": "Nested Class",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/nested-class\/"
    },
    
    "\/\/localhost:1313\/posts\/%EC%9E%90%EB%B0%94%EC%9D%98-%EC%A4%91%EC%B2%A9-%ED%81%B4%EB%9E%98%EC%8A%A4nested-class\/": {
        "title": "자바의 중첩 클래스(Nested Class)",
        "tags": ["nested class",],
        "content": " 개요 중첩 클래스(Nested Class)에 대해서 여러 사이트에서 설명해놓은 것이 있지만 잘못 설명되어 있는 부분도 있었다. \u0026lsquo;Nested Class는 Inner Class와 같다.\u0026rsquo; 라는 식으로 기술해놓은 페이지가 많아 이번 기회에 확실하게 정리해놓고자 한다.\n중첩 클래스(Nested Class)는 내부 클래스(Inner Class)와 같은 개념이 아니라 포함 관계이다. 중첩 클래스는 static 사용 여부에 따라, 정적 중첩 클래스(Static nested class)와 비정적 중첩 클래스(Non-static nested class)로 구분하며, 통상적으로 각각을 정적 중첩 클래스(Static Nested Class), 내부 클래스(Inner Class)라고 한다. 중첩 클래스와 내부 클래스를 같은 개념으로 혼용하는 용례가 많다는데 static 여부에 따라 구분되어 서로 다르다는 것을 반드시 알고 있자.\n사용 이유 중첩 클래스를 사용하는 이유에 대한 오라클 페이지의 기술 내용이다. 개인적으로는 SRP나 OCP 등을 포함한 SOLID 원칙을 지키는데 오히려 악영향을 줄 수 있다고 판단해 중첩 클래스의 사용이 양면성이 있다고 판한다기 때문에 해당 문서 에서 제시하는 이유에 대해서는 동의를 하지 못하겠다.\n어쨌든 문서 내에서 기술하는 중첩 클래스를 사용하는 이유는 아래와 같다.\n논리적으로 한 곳에 클래스들을 모아놓을 방법 중 하나로서 사용 캡슐화 증가 가독성 및 유지보수에 유리 정적 클래스와 내부 클래스 class OuterClass { // 스태틱 클래스 public static class NestedStaticClass { public void printMessage () { System.out.println(\u0026#34;Message from nested static class\u0026#34;); } } // 내부 클래스(non-static nested class라고도 불린다.) public class InnerClass { public void display() { System.out.println(\u0026#34;Message from non-static inner class\u0026#34;); } } } 스코프에 따른 내부 클래스 구분 내부 클래스는 스코프에 따라 아래와 같이 구분할 수 있다.\nInner Class: 클래스 내부에 정의한 클래스이다. Method Local Inner Class: 메서드 내부에 정의한 클래스이다. Anonymous Inner Class: 익명 클래스를 이용하여 정의한 클래스이다. 보통 인터페이스를 구현한 클래스를 메서드 인자로 넘김과 동시에 정의할 때 사용한다. ... // 익명 내부 클래스 사용 예 public returnValue () { return new IAInterface({ @Override public methodA () { ... } }); } ... 출처 http://ccm3.net/archives/20638 https://docs.oracle.com/javase/tutorial/java/javaOO/nested.html ", 
        "url": "\/\/localhost:1313\/posts\/%EC%9E%90%EB%B0%94%EC%9D%98-%EC%A4%91%EC%B2%A9-%ED%81%B4%EB%9E%98%EC%8A%A4nested-class\/"
    },
    
    "\/\/localhost:1313\/tags\/apache_httpd\/": {
        "title": "Apache_httpd",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/apache_httpd\/"
    },
    
    "\/\/localhost:1313\/tags\/servlet_container\/": {
        "title": "Servlet_container",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/servlet_container\/"
    },
    
    "\/\/localhost:1313\/posts\/%EC%84%9C%EB%B8%94%EB%A6%BF-%EC%BB%A8%ED%85%8C%EC%9D%B4%EB%84%88%EC%99%80-%EC%84%9C%EB%B2%84%EC%99%80%EC%9D%98-%EA%B4%80%EA%B3%84\/": {
        "title": "서블릿 컨테이너와 서버와의 관계",
        "tags": ["apache_httpd","servlet_container",],
        "content": " 개요 아파치 서버에 톰캣(tomcat)을 이용하여 젠킨스와 같은 애플리케이션을 올려본 적은 있어도 직접 서블릿 컨테이너에서 사용할 수 있는 애플리케이션은 개발해 본 적이 없다. 때문에 서블릿이라는 용어 자체가 생소하고 웹 서버에서 이를 어떻게 관리하는지에 대해 궁금한 점을 중점으로 정리하고자 한다.\n정리하고자 하는 질문은 아래와 같다.\nApache나 Nginx에서 서블릿 컨테이너로 어떤 방식을 통해서 클라이언트 요청을 넘겨주는가? 서블릿 컨테이너가 만들어진 이유는 무엇인가? 애플리케이션의 서블릿은 서블릿 컨테이너가 가지고 있는 라이프사이클 중 언제 추가되는가? 애플리케이션이 가지는 서블릿은 서블릿 컨테이너에 어떤 형태로 추가되는가? 클라이언트 요청 시 서블릿 컨테이너는 어떤 방법을 통해 적절한 서블릿을 찾아내는가? 웹 서버, 웹 어플리케이션 서버에서 클라이언트 요청 방법 클라이언트 요청에 대한 서버의 처리 동작의 자세한 구현은 서버에 따라 달라진다. 하지만 웹 서버의 기본 작업은 관리자가 설정한 포트를 LISTENING하고 있다가 클라이언트로부터 요청이 들어오면 쓰레드풀로부터 쓰레드 하나를 반환받아 서블릿의 서비스 메서드를 호출하는 것이다.\n처리 방법 웹 서버는 브라우저(클라이언트)로부터 HTTP 요청을 받는다. 웹 서버는 클라이언트의 요청을 WAS(Web Application Server)에 전달한다. WAS는 관련된 서블릿을 메모리에 로드한다. WAS는 web.xml을 참조하여 해당 서블릿에 대한 쓰레드를 생성하며, 이 때 쓰레드 풀을 사용한다. HttpServletRequest와 HttpServletResponse 객체를 생성하여 서블릿에 전달한다. doGet() 또는 doPost() 메서드는 인자에 맞게 생성된 적절한 동적 페이지를 Response 객체에 담아 WAS에 전달한다. WAS는 Response 객체를 HttpResponse 형태로 바꾸어 웹 서버에 전달한다. 생성한 Thread를 종료하고 HttpServletrequest, HttpServletresponse 객체를 제거한다 서블릿 컨테이너에는 tomcat 외에 어떤 것들이 있는가? 서블릿 컨테이너 목록은 다음 링크 에서 확인할 수 있다.\n서블릿 컨테이너가 필요한 이유는? 서블릿 컨테이너는 웹 컨테이너 또는 Web Application Server(WAS) 라고도 불린다. WAS는 DB 조회나 다양한 로직 처리를 요구하는 동적 컨텐츠를 제공하기 위해 만들어진 애플리케이션 서버이다. 웹 서버의 기능들을 구조적으로 분리하여 처리하고자 하는 목적으로 제시되었다.\nWAS가 필요한 이유는 웹 페이지는 정적 컨텐츠와 동적 컨텐츠가 모두 존재하는데 웹 서버만을 이용한다면, 사용자가 원하는 요청에 대한 결과값을 미리 모두 준비해 놓고 서비스를 해야한다. 따라서 WAS를 통해 요청에 맞는 데이터를 DB에서 가져와 비지니스 로직에 맞게 필요 시 생성하여 제공함으로써 자원을 효율적으로 사용할 수 있다.\n출처 https://gmlwjd9405.github.io/2018/10/27/webserver-vs-was.html https://www.javatpoint.com/container ", 
        "url": "\/\/localhost:1313\/posts\/%EC%84%9C%EB%B8%94%EB%A6%BF-%EC%BB%A8%ED%85%8C%EC%9D%B4%EB%84%88%EC%99%80-%EC%84%9C%EB%B2%84%EC%99%80%EC%9D%98-%EA%B4%80%EA%B3%84\/"
    },
    
    "\/\/localhost:1313\/categories\/database\/": {
        "title": "Database",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/categories\/database\/"
    },
    
    "\/\/localhost:1313\/tags\/shard\/": {
        "title": "Shard",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/shard\/"
    },
    
    "\/\/localhost:1313\/posts\/%ED%8C%8C%ED%8B%B0%EC%85%94%EB%8B%9Dpartitioning%EA%B3%BC-%EC%83%A4%EB%94%A9sharding\/": {
        "title": "파티셔닝(Partitioning)과 샤딩(Sharding)",
        "tags": ["partition","shard",],
        "content": " 개요 데이터베이스에 관련된 문서를 보다보니 Partition, Sharding이라는 용어들이 등장하기 시작하기 시작했다. 이러한 것들은 말 그 대로 데이터베이스를 여러 개로 나누어 분산시키기 위한 기술 용어들인데 모두 서비스 크기 증가에 따른 DB 크기 증가, 성능 이슈에 따른 것이다. 일명 VLDB(Very Large DBMS)라 불리는, DBMS 하나로 전체 데이터베이스를 다룰 수 없는 데이터베이스가 자연스럽게 등장하였고 DBMS 한 개가 여러 개의 테이블을 관리하면서 성능 이슈도 생기게 되었는데 이를 해결하기 위한 것이 바로 파티셔닝(partitioning) 과 샤딩(Sharding)이다.\n파티셔닝(Partitioning) 큰 테이블이나 인덱스를 관리하기 쉬운 크기로 분리하는 방법이다.\n장점 가용성(Availability): 물리적인 노드 분리에 따라 전체 DB 내의 데이터 손상 가능성이 줄어들고, 데이터 가용성이 향상된다. 관리 용이성(Manageability): 큰 테이블을 제거하여 관리를 쉽게 할 수 있다. 성능(Performance): 특정 DML(Data Manipuation Language)1과 Query 성능을 향상시키며 대용량 데이터 write 환경에서 효율적이다. insert가 많은 OLTP(Online Transaction Processing)2 시스템에서 insert 작업들을 로드 밸런싱을 통해 분산시켜 성능상의 이점이 있다. 단점 테이블 간 join 비용 증가 파티션 제약: 테이블과 인덱스를 별도로 파티션 할 수 없다. 파티셔닝 범위 Range Partitioning 연속적인 숫자나 날짜 기준으로 파티셔닝한다. 손쉬운 관리 기법 제공에 따라 관리 시간을 단축할 수 있다. 우편번호, 일별, 월별, 분기별 등의 데이터에 적합하다. List Partitioning 특정 Partition에 저장될 데이터에 대한 명시적 제어가 가능하다. 분포도가 비슷하여 많은 SQL에서 해당 컬럼의 조건이 많이 들어오는 경우에 유용하다. Multi-Column Partition Key를 제공하기 힘들다. (한국, 일본, 중국 -\u0026gt; 아시아)(노르웨이, 스웨덴, 핀란드 -\u0026gt; 북유럽) 등의 예시 가능 Composite Partitioning Composite Partition은 파티션의 Sub-Partitioning을 말한다. 큰 파티션에 대한 I/O 요청을 여러 파티션으로 분산할 수 있다. Range Partitioning을 수행할 수 있는 컬럼이 있지만 파티셔닝을 수행한 결과의 파티션이 너무 커서 효과적으로 관리할 수 없을 때 유용하다. Range-List, Range-Hash Hash Partitioning Partition Key의 해시값에 의한 파티셔닝(데이터 균등 분할 가능) Select시 조건과 무관하게 병렬 Degree 제공 (질의 성능 향상) 특정 데이터가 어느 Hash Partition에 있는지 판단 불가 Hash Partition은 파티션을 위한 범위가 없는 데이터에 적합 파티셔닝 방법 Horizontal Partitioning 데이터 개수를 기준으로 나누어 Partitioning 하는 방법이다. Sharding이 Horizontal Partitioning과 관련되는데, 같은 테이블 스키마를 가진 데이터를 데이터베이스 여러 개에 분산하여 저장하는 방법을 일컫는 용어이기 때문이다. 때문에 Sharding을 Horizontal Partition`이라고 볼 수도 있다. Vertical Partitioning 테이블의 컬럼을 기준으로 데이터를 나눈다. 이미 정규화된 데이터를 분리하는 과정으로 생각해야 한다. 자주 사용하는 컬럼을 분리하여 성능 향상을 꾀할 수 있다. 샤딩, Sharding DBMS 한 개로 처리할 수 있는 데는 한계가 있으므로 데이터베이스 여러 개를 사용하는 방식으로 데이터 조회 한계를 극복해야 한다. 이를 위해 분산 환경을 고려해 만들어진 데이터베이스를 이용하는 방법도 있지만 범위 검색에 취약하거나 JOIN 연산을 사용할 수 없는 등 기능에 제약이 많다. 따라서 상대적으로 풍부한 기능을 사용하면서 데이터 확장을 꾀할 수 있는 방법은 RDBMS를 샤딩(sharding)하여 사용하는 것이다. 계속 증가하는 데이터에 장애 없이 효과적으로 대응할 수 있어야 하고 서비스마다 다른 데이터 특성과 모델에 어떻게 대처할 것인가가 샤딩 플랫폼의 핵심이다.\nHP(Horizontal Partitioning)과 샤딩(Sharding) 비교 수평 분할(Horizontal Partitioning)이란 스키마가 같은 데이터를 두 개 이상의 테이블에 나누어 저장하는 디자인을 말한다. 가령 같은 주민 데이터를 처리하기 위해 스키마가 같은 \u0026lsquo;서현동주민 테이블\u0026rsquo;과 \u0026lsquo;정자동주민 테이블\u0026rsquo;을 사용하는 것을 말한다. 인덱스의 크기를 줄이고, 작업 동시성을 늘리기 위한 것이다. 보통 수평 분할을 한다고 했을 때는 하나의 데이터베이스 안에서 이루어지는 경우를 지칭한다.\n이에 비해, 샤딩은 물리적으로 다른 데이터베이스에 데이터를 수평 분할 방식으로 분산 저장하고 조회하는 방법을 말한다. \u0026lsquo;주민\u0026rsquo; 테이블이 여러 DB에 있을 때 서현동 주민에 대한 정보는 A DB에, 정자동 주민에 대한 정보는 B DB에 저장되도록 하는 방식이다. 여러 데이터베이스를 대상으로 작업해야 하기 때문에 경우에 따라서는 (JOIN 등) 기능에 제약이 있을 수 있고 일관성(consistency)과 복제(replication) 등에서 불리한 점이 많다. 예전의 샤딩은 애플리케이션 서버 레벨에서 구현하는 경우가 많았지만 최근에는 이를 플랫폼 차원에서 제공한다.\n\u0026hellip; (중략)\n샤딩(Sharding)의 한계 샤딩의 대표적인 제약 사항은 아래와 같다.\n두 개 이상의 샤드에 대한 JOIN 연산을 할 수 없다. auto increment 등은 샤드 별로 달라질 수 있다. last_insert_id() 값은 유효하지 않다. shard key column 값은 update하면 안된다. 하나의 트랜잭션에서 두 개 이상의 샤드에 접근할 수 없다. 따라서, 샤딩을 사용할 때는 위와 같은 제약 사항에 문제가 되지 않도록 데이터 모델링을 하는 것이 중요하다.\n출처 https://nesoy.github.io/articles/2018-05/Database-Shard https://nesoy.github.io/articles/2018-02/Database-Partitioning https://en.wikipedia.org/wiki/Shard_(database_architecture) Sharding \u0026amp; Query Off Loading https://d2.naver.com/helloworld/14822 각주 \u0026lsquo;데이터 조작어\u0026rsquo;라고 부르며 SELECT, INSERT, UPDATE, DELETE 등의 명령어가 여기에 포함된다. 그 외에 \u0026lsquo;데이터 정의어\u0026rsquo;(DDL: CREATE/ALTER/DROP/RENAME/TRUNCATE), 데이터 제어어(GRANT/REVOKE), 트랜잭션 제어어(TLC: COMMIT/ROLLBACK/SAVEPOINT) 등이 있다. 자세한 것은 http://brownbears.tistory.com/180 참고\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n컴퓨터 시스템을 사용하는 트랜잭션 데이터 관리를 온라인 트랜잭션 처리(OLTP)라고 한다. OLTP 시스템은 조직의 일상적인 작업에서 발생하는 비지니스 상호작용을 기록하고 이 데이터를 쿼리하여 유추할 수 있도록 지원한다.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n", 
        "url": "\/\/localhost:1313\/posts\/%ED%8C%8C%ED%8B%B0%EC%85%94%EB%8B%9Dpartitioning%EA%B3%BC-%EC%83%A4%EB%94%A9sharding\/"
    },
    
    "\/\/localhost:1313\/tags\/restful\/": {
        "title": "Restful",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/restful\/"
    },
    
    "\/\/localhost:1313\/posts\/restful-%EC%84%B1%EC%88%99%EB%8F%84-%EB%AA%A8%EB%8D%B8-richardson-maturity-modelrmm\/": {
        "title": "RESTful 성숙도 모델, Richardson Maturity Model(RMM)",
        "tags": ["RMM","Restful",],
        "content": " 개요 이 문서는 진행하는 프로젝트에서 백엔드 서버에서 제공할 REST API를 어떻게 설계해야 하는가에 대해 공부하던 중 인터넷에서 찾은 문서를 정리한 것이다. 이 문서에서는 REST 소개부터 API 설계 방법, 그리고 설계한 API가 \u0026lsquo;RESTful\u0026rsquo; 이라는 형용사를 붙일 수 있는지 판단할 수 있는 \u0026lsquo;Richardson Maturity Model\u0026rsquo;이라는 성숙도 모델을 기술한다.\nREST 소개 이 절은 Microsoft의 REST 소개 를 발췌하여 정리한 것이다.\nREST는 하이퍼미디어 기반 분산 시스템을 구축하기 위한 아키텍처 스타일로서 프로토콜과는 관련이 없는 용어이다. (일각에서 REST와 SOAP를 비교하는 글들이 많아 언급하였다.) REST는 어떤 기본 프로토콜과도 독립적이며 HTTP에 연결될 필요가 없다. 그러나 대부분의 일반적인 REST 구현에서 응용 프로그램 프로토콜로 HTTP를 사용하고 이 지침에서는 HTTP를 위한 REST API 디자인에 중점을 둔다.\nREST가 HTTP보다 우수한 주요 장점은 개방형 표준을 사용하므로 API 또는 클라이언트 응용 프로그램의 구현이 특정 구현에 바인딩되지 않는다는 점이다. 예를 들어 REST 웹 서비스는 ASP.NET으로 작성할 수 있으며, 클라이언트 응용 프로그램은 HTTP 요청을 생성하고 HTTP 응답을 구문 분석할 수 있는 모든 언어 또는 도구 집합을 사용할 수 있다.\n아래는 HTTP를 사용하는 RESTful API의 몇 가지 디자인 원칙이며 잘 디자인된 웹 API는 아래에 언급되는 특성들을 지원해야 한다.\nHTTP기반 RESTful API 디자인 원칙 REST API는 리소스를 중심으로 디자인되며 클라이언트에서 액세스할 수 있는 모든 종류의 개체, 데이터 또는 서비스가 리소스에 포함된다. 리소스마다 해당 리소스를 고유하게 식별하는 URI인 식별자가 있다. 예를 들어 특정 고객 주문의 URI는 아래와 같다. https://adventure-works.com/orders/1 클라이언트가 리소스의 표현을 교환하여 서비스와 상호 작용한다. 많은 Web API가 교환 형식으로 JSON을 사용한다. 예를 들어 위에 나열된 URI에 대한 GET 요청은 아래의 응답을 반환할 수 있다. {\u0026#34;orderID\u0026#34;: 1, \u0026#34;orderValue\u0026#34;:99, \u0026#34;productId\u0026#34;: 1, \u0026#34;quantity\u0026#34;: 1} REST API는 균일한 인터페이스를 사용하므로 클라이언트와 서비스 구현을 분리하는데 도움이 된다. HTTP 기반의 REST API의 경우 리소스에 표준 HTTP 동사 수행 작업을 사용하는 것이 균일한 인터페이스에 포함된다. 가장 일반적인 작업은 GET, POST, PUT, PATCH, DELETE 등이다.\nREST API는 상태 비저장 요청 모델을 사용한다. HTTP 요청은 독립적이어야 하고 임의 순서대로 발생할 수 있으므로 요청 사이에 일시적인 상태 정보를 유지할 수 없다. 정보는 리소스 자체에만 저장되며 각 요청은 자동 작업이어야 한다. 결국 이러한 클라이언트와 서버 간의 상태 유지 제약 조건 덕분에 웹 서비스를 확장하는데 유리한 면이 생기게 되었다. (둘 간에 상태를 처리할 필요가 없기 때문에) 하지만 웹 서비스가 백 엔드 데이터 저장소에 데이터를 쓰는 경우 규모 확장이 어려울 수 있다.\nREST API는 표현에 포함된 하이퍼미디어 링크에 따라 구동된다. 예를 들어 아래는 주문 요청을 JSON 형식으로 나타낸 것이다. 해당 요청에는 주문과 관련된 고객 정보를 가져오고 업데이트하는 링크를 포함하고 있다.\n{ \u0026#34;orderId\u0026#34;: 3, \u0026#34;productId\u0026#34;: 2, \u0026#34;quantity\u0026#34;: 4, \u0026#34;orderValue\u0026#34;: 16, \u0026#34;links\u0026#34;: [ {\u0026#34;rel\u0026#34;:\u0026#34;product\u0026#34;, \u0026#34;href\u0026#34;:\u0026#34;https://adventure-works.com/customers/3\u0026#34;, \u0026#34;action\u0026#34;:\u0026#34;GET\u0026#34;}, {\u0026#34;rel\u0026#34;:\u0026#34;product\u0026#34;,\u0026#34;href\u0026#34;:\u0026#34;https://adventure-works.com/customers/3\u0026#34;, \u0026#34;action\u0026#34;:\u0026#34;PUT\u0026#34; } ] } 잘 디자인된 웹 API가 지원해야 하는 특성 플랫폼 독립성 모든 클라이언트는 내부에서 API가 구현되는 방법에 관계없이 API를 호출할 수 있어야 한다. 이를 위해서는 표준 프로토콜을 사용해야 하고 클라이언트 및 웹 서비스가 교환할 데이터 형식에 동의할 수 있는 매커니즘이 있어야 한다.\n서비스 진화 Web API는 클라이언트 응용 프로그램과 독립적으로 기능을 진화시키고 추가할 수 있어야 한다. API가 진화해도 기존 클라이언트 응용 프로그램은 수정 없이 계속 작동할 수 있어야 한다. 모든 기능은 클라이언트 응용 프로그램이 해당 기능을 완전히 이용할 수 있도록 검색이 가능하다.\nRichardson 성숙도 모델(Richardson Maturity Model) REST 방식의 주요 요소들을 3개의 단계로 나눈 모델이며 이 모델은 리소스, HTTP 메서드(verbs), 하이퍼미디어 컨트롤을 도입한다.\n레벨 0 이 모델의 시작점은 웹 매커니즘은 전혀 사용하지 않고 HTTP를 단순히 원격 통신을 위한 전송 시스템으로 사용하는 것이다. 여기서는 원격 프로시저 호출(Remote Precedure Invocation)에 기반한 원격 통신 매커니즘을 위한 터널링 매커니즘으로서 HTTP를 사용하는 것이다.\n| | | | o-----------------------------x AppointmentService | POST/OpenSlotRequest | | | x-----------------------------o | RESPONSE/OpenSlotList | | | | | o-----------------------------x AppointmentService | POST/RequestAppointment | | | x-----------------------------o | RESPONSE/Apointment | | | | | 레벨 0 단계에서는 단순히 POX(Plain Old XML)을 주고 받는 단순한 RPC 스타일 시스템에 불과하다.\n레벨 1: 리소스 사용 이 단계는 리소스를 도입하는 것이다. 레벨 0에서처럼 모든 요청을 단일 서비스 엔드포인트로 보내는 것이 아니라 개별 리소스와 통신하도록 설계하는 단계이다.\n| | | | | POST/OpenSlotRequest | O----------------------------------X doctors/mjones | | X----------------------------------O | RESPONSE/OpenSlotList | | | | | | | | POST/RequestAppointment | o----------------------------------X slots/1234 | | | | X----------------------------------o | RESPONSE/Appointment | | | 위 그림에서 첫 요청에서 해당 의사에 대한 리소스를 먼저 아래와 같이 POST로 요청하고 해당 결과 값을 받아올 것이다.\nPOST /doctors/mjones HTTP/1.1 [various other headers] \u0026lt;OpenSlotRequest date=\u0026#34;2018-01-01\u0026#34; /\u0026gt; 이 때, 중요한 것은 응답은 같은 기본 정보를 제공하지만 각 시간대는 이제 개별적으로 어드레싱이 가능한 리소스라는 점이다. 의사 정보를 요청하는 것과 마찬가지로 예약은 특정 시간대로 요청을 보내는 것이다.\nPOST /slots/1234 HTTP/1.1 [various other headers] \u0026lt;RequestAppointment\u0026gt; \u0026lt;patient id=\u0026#34;jsmith\u0026#34; /\u0026gt; \u0026lt;/RequestAppointment\u0026gt; 여기에 문제가 없다면 아래와 같은 응답을 받을 것이다.\nHTTP/1.1 200 OK [various headers] \u0026lt;appointment\u0026gt; \u0026lt;slot id=\u0026#34;1234\u0026#34; doctor=\u0026#34;mjones\u0026#34; start=\u0026#34;1400\u0026#34; end=\u0026#34;1450\u0026#34; /\u0026gt; \u0026lt;patient id=\u0026#34;jsmith\u0026#34; /\u0026gt; \u0026lt;/appointment\u0026gt; 레벨 2: HTTP 메서드 사용(Verbs) 레벨 0, 레벨 1의 모든 통신에서 HTTP POST 메서드를 사용했지만 레벨 2에서는 HTTP 사용법에 가능한 가깝게 HTTP 메서드를 사용하여 API를 구현한다.\n| | | GET/?date=20180101\u0026amp;status=open | o---------------------------------------x doctors/mjones/slots | | x---------------------------------------o | 200 OK/OpenSlotList | | | | POST/RequestAppointment | o---------------------------------------x slots/1234 | | x---------------------------------------o | 201 Created | | Location: slots/1234/appointments | | | 레벨 2는 HTTP 메서드와 HTTP 응답 코드의 사용을 도입한다. 여기에 일관성 문제가 생겨나는데 REST 옹호자들은 모든 HTTP 메서드 사용에 대해 언급하지만 웹의 존재에 의해 지지되는 핵심 요소들은 발생한 에러의 종류를 커뮤니케이션하기 위해 상태 코드를 사용하는 것과 안전한 오퍼레이션(GET 등)과 안전하지 않은 오퍼레이션 간의 분리를 제공하는 것이다.\n레벨 3: 하이퍼미디어 컨트롤 도입 마지막 레벨은 HATEOAS(Hypertext As The Engine Of Application state)라는 약어로 종종 언급되는 것을 도입하는 단계이다. 예시에 적용하여 설명하자면, 예약을 하기 위해 필요한 요청을, 가능한 시간대 목록을 가져오면서 알 수 있는 디자인 방법이다.\n먼저 아래의 다이어그램을 보자.\n| | o---------------------------------------\u0026gt; doctors/mjones/slots | GET ?date=20180101\u0026amp;status=open | | | \u0026lt;---------------------------------------o | 200 OK OpenSlotList | | ... | | \u0026lt;link rel=\u0026#34;/linkrels/slot/book\u0026#34; | | url=\u0026#34;/slots/1234\u0026#34; | | | | | o---------------------------------------\u0026gt; | POST RequestAppointment | | | \u0026lt;---------------------------------------o | 201 Created | | Location: /slots/1234/appointment | | | 레벨 2에서 보냈던 GET과 동일한 요청으로 시작한다.\nGET /doctors/mjones/slots?date=20100104\u0026amp;status=open HTTP/1.1 Host: royalhope.nhs.uk 하지만 여기에 대한 응답은 새로운 요소를 가진다.\nHTTP/1.1 200 OK [various headers] \u0026lt;openSlotList\u0026gt; \u0026lt;slot id = \u0026#34;1234\u0026#34; doctor = \u0026#34;mjones\u0026#34; start = \u0026#34;1400\u0026#34; end = \u0026#34;1450\u0026#34;\u0026gt; \u0026lt;link rel = \u0026#34;/linkrels/slot/book\u0026#34; uri = \u0026#34;/slots/1234\u0026#34;/\u0026gt; \u0026lt;/slot\u0026gt; \u0026lt;slot id = \u0026#34;5678\u0026#34; doctor = \u0026#34;mjones\u0026#34; start = \u0026#34;1600\u0026#34; end = \u0026#34;1650\u0026#34;\u0026gt; \u0026lt;link rel = \u0026#34;/linkrels/slot/book\u0026#34; uri = \u0026#34;/slots/5678\u0026#34;/\u0026gt; \u0026lt;/slot\u0026gt; \u0026lt;/openSlotList\u0026gt; 이제 각 시간대는 예약하는 방법을 알려주는 URI를 포함한 링크 요소를 가진다.\n하이퍼 미디어 컨트롤의 요점은 리소스로 다음에 무엇을 할 수 있는지, 그것을 하기 위해 다루어야 할 리소스의 URI를 함께 알려준다는 것이다. 우리가 예약 요청을 어디에 보낼지 알아야 하는 것이 아니라 응답 내에서 하이퍼미디어 컨트롤이 우리에게 그것을 어떻게 할지를 알려주는 것이다. 이러한 하이퍼미디어 컨트롤의 장점은 클라이언트 개발자가 프로토콜을 탐색할 수 있도록 돕는다. 링크는 개발자에게 다음에 무엇이 가능할지 힌트를 제공함과 동시에 새로운 기능이 추가된 경우에도 알릴 수 있다.\n레벨의 의미 레벨 1은 큰 서비스 엔드포인트를 복수개의 리소스로 나누는 분할 \u0026amp; 정복(divide and conquer)을 사용해서 복잡성을 다루는 문제를 처리한다. 레벨 2는 불필요한 다양성을 제거하고, 동일한 방식으로 유사한 환경을 처리할 수 있도록 메소드의 표준 집합을 도입한다. 레벨 3은 프로토콜을 더 스스로 문서화할 수 있는 방법을 제공함으로써 발견가능성(discoverability)을 도입한다. 출처 https://docs.microsoft.com/ko-kr/azure/architecture/best-practices/api-design http://jinson.tistory.com/190 ", 
        "url": "\/\/localhost:1313\/posts\/restful-%EC%84%B1%EC%88%99%EB%8F%84-%EB%AA%A8%EB%8D%B8-richardson-maturity-modelrmm\/"
    },
    
    "\/\/localhost:1313\/tags\/rmm\/": {
        "title": "RMM",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/rmm\/"
    },
    
    "\/\/localhost:1313\/tags\/folding\/": {
        "title": "Folding",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/folding\/"
    },
    
    "\/\/localhost:1313\/posts\/folding-in-haskell\/": {
        "title": "Folding in Haskell",
        "tags": ["folding",],
        "content": " Folding in Haskell 취업을 위한 포트폴리오 준비 작업으로 바쁜 가운데, Functional Programming에 대해 관심이 생겨 Haskell이라는 언어를 배우기 시작했다. (알고리즘이나 데이터베이스 등 배울 것이 많은 데 갈수록 태산이다.) 대학교 시절, xmonad 윈도우즈 매니저를 사용하면서 접한 언어를 이렇게 뒤늦게 배우게 될 줄은 꿈에도 몰랐다.\n대표적인 함수형 언어로 알려진 Haskell 에는 Folding 이라는 특별한 개념이 등장한다. Haskell Wiki에서는 Folding을 아래와 같이 설명하고 있다.\nIn functional programming, fold (or reduce) is a family of higher order functions that process a data structure in some order and build a return value. Typically, a fold deals with two things: a combining function, and a data structure (typically a list of elements). The fold then proceeds to combine elements of the data structure using the function in some systematic way.\n리스트와 같이 여러 원소들로 이루어진 데이터 구조의 각 원소들을 각각 인자(arguments)로 넘긴 함수에 대입하고, 그 결과를 원소 타입으로 반환하는 것이 fold 함수의 역할이다. 다시 말해서, spine 형태의 데이터 구조를 특정한 함수를 이용하여 접고(fold) 줄여서(reduce) 결과값을 얻는 것이 그 역할이라고 정리하면 이해하기 쉽다.\n먼저, 아래의 예제를 살펴보자.\n\u0026gt; foldr (+) 1 [1..3] 7 \u0026gt; :t foldr foldr :: Foldable t =\u0026gt; (a -\u0026gt; b -\u0026gt; b) -\u0026gt; b -\u0026gt; t a -\u0026gt; b foldr은 (a -\u0026gt; b -\u0026gt; b) 형태의 함수와 b, t a 인자 3 개를 입력 받아 b 타입을 반환한다. 즉, (1 + (2 + (3 + (1))) 과 같은 형식으로 fold 를 진행하여 결국 답은 7이 된다. 이 때, 각 괄호를 reduce한 결과는 foldr 대신 scanr 키워드를 사용하면 아래와 같이 얻을 수 있다.\n\u0026gt; scanr (+) 1 [1..3] [7,6,4,1] 그렇다면, 반대로 괄호를 움직여 evaluation의 방향을 왼쪽에서 오른쪽으로 바꿔보면 어떨까?\n\u0026gt; foldl (+) 1 [1..3] 7 \u0026gt; scanl (+) 1 [1..3] [1,2,4,7] \u0026gt; :t foldl foldl :: Foldable t =\u0026gt; (b -\u0026gt; a -\u0026gt; b) -\u0026gt; b -\u0026gt; t a -\u0026gt; b foldr과 달리 foldr은 전달한 함수에 인자로 (a -\u0026gt; b -\u0026gt; b)가 아닌 (b -\u0026gt; a -\u0026gt; b)를 전달한다. 이 때문에 결과값은 같더라도 평가(Evaluation)의 순서가 ((((1) + 1) + 2) + 3) 달라지게 된다. 이 때문에 scanl의 결과는 [1,2,4,7]이 되는 것이다.\n이러한 fold의 장점은 갖가지 재귀 함수들을 엄청나게 간단하게 만들 수 있다는 것이다. 예를 들면, reverse 함수의 경우 보통 아래와 같이 작성할 수 있다.\nmyReverse :: [a] -\u0026gt; [a] myReverse [] = [] myReverse (x:xs) = myReverse xs ++ [x] 하지만, fold를 사용하면, 아래와 같이 축약된 형태로 간단하게 작성할 수 있다.\n-- pointfree 함수 형태로 작성 myReverse :: [a] -\u0026gt; [a] myReverse = foldl (flip (:)) [] 이처럼 함수형 프로그래밍에서 fold 또는 reduce는 인자로 전달한 데이터 구조를 특정한 함수를 이용하여 반환값을 구성하기 위한 HOF (Higher Order Functions, 함수를 인자로 취하거나 반환값으로 함수를 반환하는 함수)의 한 가지이다. 문서 대부분이 영어로 되어 있고 함수형 언어에 대해서는 아직까지 관심을 가지는 사람이 많이 없어 공부하기에는 쉽지 않지만 하나씩 정리해 나가다 보면 언젠가는 문서를 만들 수 있을 정도로 자료가 많아질 것이라 기대한다.\n", 
        "url": "\/\/localhost:1313\/posts\/folding-in-haskell\/"
    },
    
    "\/\/localhost:1313\/posts\/http\/1.1%EA%B3%BC-http\/2%EC%9D%98-%EC%B0%A8%EC%9D%B4%EC%A0%90\/": {
        "title": "HTTP\/1.1과 HTTP\/2의 차이점",
        "tags": ["http1.1","http2",],
        "content": " HTTP/1.1 vs HTTP/2 HTTP 프로토콜에 대해서는 관심이 없다가 문득 텔레그램의 봇을 통해 우연히 HTTP/2의 개념을 처음으로 접하게 되었다. 이 문서는 HTTP/1.1과 HTTP/2 의 차이점을 설명하고 1.1에서 2로 변화하면서 어떻게 개선되었는지 기술한다.\nHTTP/1.1 동작 방식 HTTP(HyperText Transfer Protocol)은 웹에서 클라이언트(웹 브라우저)가 웹 서버(httpd, nginx, etc\u0026hellip;)와 통신하기 위한 프로토콜 중 하나이다. HTTP 1.1은 클라이언트와 서버 간의 통신을 위해 다음과 같은 과정을 거친다.\n위 그림에서 알 수 있듯이 HTTP/1.1은 기본적으로 Connection 한 개당 하나의 요청을 처리하도록 설계되어 있다. 이 때문에 동시에 여러개의 리소스를 주고받는 것이 불가능하고 요청과 응답이 위 그림처럼 순차적으로 이루어진다. 이런 설계 방식 때문에 HTTP 문서 내에 포함된 다수의 리소스 (image, css, script)를 처리하려면 요청할 리소스 개수에 비례하여 Latency(대기시간)이 길어진다.\n그렇다면 이러한 순차적인 처리에 따른 지연 외에 또 다른 단점이 있는지 좀 더 살펴보자.\n단점 1. HOL(Head Of Line) Blocking - 특정 응답의 지연 HTTP/1.1의 경우 HOL Blocking이 발생할 수 있다. HOL (Head-Of-Line) 블로킹이란 네트워크에서 같은 큐에 있는 패킷이 첫 번째 패킷에 의해 지연될 때 발생하는 성능 저하 현상을 말한다. 웹 환경에서의 HOL 블로킹은 패킷 처리 속도 지연 뿐만 아니라 최악의 경우 패킷 드랍까지 발생할 수 있다.\n아래는 스위치에서 발생할 수 있는 HOL Blocking 현상이다.\n보통의 스위치는 패킷을 받는 input, switching fabric, output 등 총 세 부분으로 구성된다. 그래서 패킷이 input으로 들어오면 switching fabric을 거쳐 output으로 도달하게 된다. 하지만 위의 예시처럼 첫 번째, 세 번째 입력이 동일한 인터페이스(Output 4)로 패킷을 보내려 하고 이에 switching fabric에서 세 번째 입력의 패킷을 먼저 처리해버리면 같은 클록 사이클 안에 첫번째 입력을 인터페이스로 보내지 못한다. 이렇게 순차적 처리와 output 스트림 제한으로 인한 블로킹 현상을 HOL Blocking이라 일컫는다.\n웹 환경에서의 HOLB는 두 가지 종류가 있다. 첫 번째는 HTTP의 HOLB, 두 번째는 TCP의 HOLB이다. 먼저 HTTP 프로토콜에서 발생할 수 있는 HOLB에 대해 살펴보자.\nHTTP에서의 Head-Of-Line Blocking HTTP/1.1의 \u0026lsquo;Connection 하나 당 하나의 요청을 처리하는\u0026rsquo; 방식을 개선할 수 있는 개선 방법 중 pipelining이라는 것이 존재한다. 이것은 하나의 connection을 통해 여러 개의 파일을 요청/응답할 수 있는 기법을 말하는데 이 기법을 통해 어느 정도의 성능 향상을 꾀할 수 있지만 한 가지 문제점이 있다. 아래 예시처럼 하나의 TCP 연결에서 3개의 이미지를 얻으려고 하는 경우 HTTP 요청 순서는 다음 그림과 같다.\n| --- a.png --- | | --- b.png --- | | --- c.png --- | 순서대로 첫 번째 이미지를 요청하고 응답받고 두 번째, 세 번째도 마찬가지로 처리하는데 첫 번째 이미지의 응답 처리가 완료하기 전까지 b.png, c.png에 대한 요청은 대기하게 된다. 이와 같은 현상을 HTTP/1.1의 Head of Line Blocking이라 부르며 파이프 라이닝의 가장 큰 문제점 중 하나이다.\nTCP에서의 Head-Of-Line Blocking TCP에서의 HOLB는 HTML/2에서도 나타나는 단점으로서 TCP의 고질적인 문제이다. TCP의 경우 패킷 LOSS가 발생하면 패킷을 재전송하게 되는데 패킷 전송 후 상대방으로부터 ACK 신호를 받지 못하면 전송한 다음 번에 패킷들을 전송하지 않고 모두 대기 상태로 두고 이전에 보냈던 패킷을 재전송한다. 이러한 특성 때문에 TCP를 사용하게 될 경우 어쩔 수 없이 Head-Of-Line Blocking 문제가 발생하게 된다.\n단점 2. RTT(Round Trip Time) 증가 HTTP/1.1의 경우 일반적으로 Connection 하나에 요청 한 개를 처리한다. 이렇다보니 매번 요청 별로 Connection을 만들게 되고 TCP 상에서 동작하는 HTTP의 특성상 3-way Handshake가 반복적으로 일어나며, 불필요한 RTT증가와 네트워크 지연을 초래하여 성능을 지연시킨다.\n단점 3. 무거운 Header 구조 HTTP/1.1의 헤더에는 많은 메타 정보들이 저장되어 있다. 클라이언트가 서버로 보내는 HTTP 요청은 매 요청 때마다 중복된 헤더 값을 전송하게 되며 서버 도메인에 관련된 쿠키 정보도 헤더에 함께 포함되어 전송된다. 이러한 반복적인 헤더 전송, 쿠키 정보로 인한 헤더 크기 증가가 HTTP/1.1의 단점이다.\nHTTP/1.1을 개선하기 위한 노력들 앞서 언급한 HTTP/1.1의 문제점들을 극복하는 몇 가지 트릭들이 있는데 그 중 참고 사이트에서 언급된 것들을 기술하겠다.\n개선방법 1. Image Spriting 웹 페이지를 구성하는 다양한 아이콘 이미지 파일의 요청 횟수를 줄이기 위해 아이콘을 하나의 큰 이미지로 만든 다음 CSS에서 해당 이미지의 좌표 값을 지정하여 표시한다.\n개선방법 2. Domain Sharding 요즘 브라우저들은 HTTP/1.1의 단점을 극복하기 위해 여러 개의 Connection을 생성해서 병렬로 요청을 보내기도 한다. 하지만 브라우저 별로 도메인당 Connection의 개수 제한이 존재하기 때문에 근본적인 해결책은 아니다.\n개선방법 3. Minified CSS/Javascript HTTP를 통해 전송되는 데이터의 용량을 줄이기 위해서 CSS, Javascript를 축소하여 적용한다. name.min.js, name.min.css 등이 그 예이다.\n개선방법 4. Load Faster (옛 방식과 최신 방식) 이 방법은 브라우저에서 문서를 어떻게 빨리 로드하는가에 대한 정보들이다. 이러한 개선 방법으로 옛 방식과 최신 방식으로 나누어 설명할 수 있는데, 먼저 옛 방식으로 해결하는 방법부터 설명하겠다.\n옛 방식은 스타일시트 파일을 HTML 문서의 상위에 배치하고 스크립트 파일을 HTML 문서 하단에 배치하는 방법으로 로드를 더 빨리 한다는 방법인데 이는 웹 브라우저가 페이지를 로드하는 방법과 관련이 있다.\n아래는 브라우저가 \u0026lt;script\u0026gt; 태그가 포함된 페이지를 로드하는 과정을 간략하게 설명한 것이다.\nindex.html과 같은 HTML 페이지를 가져온다. (fetch) 가져온 HTML 페이지를 파싱한다. 파서(parser)가 \u0026lt;script\u0026gt; 태그를 만나면 해당 태그가 가리키고 있는 스크립트 파일을 요청한다. 이 때 파서는 스크립트 파일을 받아오는 동안 HTML 파싱을 멈추거나 블락한다. 스크립트 파일이 모두 받아지면 파서는 파싱을 계속한다. HTML 파일의 남은 부분을 계속해서 파싱한다. 이러한 웹 브라우저의 접근 방법(스크립트 파일 수신 대기)은 스크립트 파일 내에서 DOM을 생성할 수 있는 가능성이 있기 때문인데 근래의 자바스크립트 개발자 대부분은 더이상 아래처럼 HTML 파일 로드 중에 자바스크립트를 이용해서 DOM을 생성하지 않는다. 하지만 브라우저 입장에서는 스크립트의 DOM 변경여부를 사전에 알 수 없기 때문에 스크립트 파일을 받아오는 동안 파싱을 멈출 수 밖에 없다.\ndocument.addEventListener(\u0026#34;DOMContentLoaded\u0026#34;, function () { document.getElementById(\u0026#34;user-greeting\u0026#34;).textContent = \u0026#34;Welcome!\u0026#34; }); 하지만 이러한 개선방법은 한 가지 문제점을 가지고 있는데, 전체 DOM을 파싱하기 전까지 스크립트 파일을 받아오지 못하는 단점이 있다. 한 예로, 옛날 \u0026lsquo;파병온라인\u0026rsquo;이라는 온라인 FPS 게임이 있었는데 당시 캐시 충전을 하는 페이지가 단순 웹페이지로 되어 있었다. 그리고 단순하게 오른쪽 마우스를 차단하는 방식으로만 페이지를 구성했는데 문제는 자바스크립트가 로드되는 시간을 늦추면(강제로 모뎀의 속도를 낮추면) 스크립트가 다운로드 되기 전에 페이지의 소스코드를 볼 수 있었다. 당시는 초등학생 때라 그러한 원리를 몰랐지만 이렇게 스크립트 배치와 브라우저의 파싱 방법에 대해 알고나니 당시의 문제점이 무엇인지 알게 되었다.\n때문에 요즘에는 아래와 같은 방법으로 로드 문제를 해결한다.\nasync \u0026lt;script type=\u0026#39;text/javascript\u0026#39; src=\u0026#39;path/to/s1.js\u0026#39; async\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#39;text/javascript\u0026#39; src=\u0026#39;path/to/s2.js\u0026#39; async\u0026gt;\u0026lt;/script\u0026gt; 이 속성(attribute)를 사용하면 스크립트 로드를 비동기적으로 받아오게 된다. 때문에 받아오는 동안 파싱을 블락하는 현상을 해결할 수 있다. http://caniuse.com/#feat=script-async 에 따르면 브라우저 중 94.57% 가 async 키워드를 지원한다.\ndefer \u0026lt;script type=\u0026#39;text/javascript\u0026#39; src=\u0026#39;path/to/s1.js\u0026#39; defer\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#39;text/javascript\u0026#39; src=\u0026#39;path/to/s2.js\u0026#39; defer\u0026gt;\u0026lt;/script\u0026gt; defer 속성은 순차적으로 실행된다(s1.js 파일이 먼저 로드되고 s2.js가 그 다음에 실행된다). 다만 defer 속성이 사용된 경우 브라우저의 파싱을 블락하지 않는다. 하지만 async 속성과 다르게 defer 스크립트들은 전체 문서가 로드된 후에 실행된다는 차이점이 있다.\n최근 트렌드는 head 태그에 자바스크립트를 삽입하고 async 키워드나 defer 키워드를 사용하는 것이다.\n개선방법 5. Data URI Scheme 이 방법은 HTML 문서 내에 이미지 리소스를 Base64로 인코딩된 이미지 데이터로 직접 기술하는 방법으로 이를 이용하여 서버로의 요청을 줄이는 방식이다.\n구글의 SPDY 위에서 언급된 노력들도 근본적인 HTTP/1.1의 단점을 해결할 수 없었다. 그래서 구글은 더 빠른 웹을 실행하기 위해 Throughput 관점이 아닌 Latency 관점에서 HTTP를 고속화한 SPDY(스피디)라 불리는 새로운 프로토콜을 구현하였다. 다만 SPDY는 HTTP를 대체하는 프로토콜이 아니가 HTTP를 통한 전송을 재정의하는 형태로 구현되었다. 그리고 이 SPDY는 HTTP/2 초안의 참고 규격이 되게 된다.\nHTTP/2 HTTP/2는 HTTP/1.1을 완전하게 재작성한 것이 아니라 프로토콜의 성능에 초점을 맞추어 수정한 버전이라 생각하면 된다. 특히 End-user가 느끼는 latency나 네트워크, 서버 리소스 사용량 등과 같은 성능 위주로 개선했다. 아래는 HTTP/2의 주요 특징들이다.\nMultiplexed Streams HTTP/2는 Multiplexed Streams를 이용하여 Connection 한 개로 동시에 여러 개의 메시지를 주고 받을 수 있으며 응답은 순서에 상관없이 Stream으로 주고 받는다. HTTP/1.1의 Connection Keep-Alive, Pipelining의 개선 버전이라 보면 된다.\nStream Prioritization 문서 내에 CSS 파일 1개와 이미지 파일 2개가 존재하고 이를 클라이언트가 요청하는 상황에서, 이미지 파일보다 CSS 파일의 수신이 늦어진다면 브라우저 렌더링에 문제가 생기게 된다. HTTP/2에서는 이러한 상황을 고려하여 리소스 간의 의존관계에 따른 우선순위를 설정하여 리소스 로드 문제를 해결한다.\nServer Push 서버는 클라이언트가 요청하지 않은 리소스를 사전에 푸쉬를 통해 전송할 수 있다. 이렇게 리소스 푸쉬가 가능해지면 클라이언트가 추후에 HTML 문서를 요청할 때 해당 문서 내의 리소스를 사전에 클라이언트에서 다운로드할 수 있도록 하여 클라이언트의 요청을 최소화할 수 있다.\nHeader Compression HTTP/2는 헤더 정보를 압축하기 위해 Header Table과 Huffman Encoding 기법을 사용하여 처리하는데 이를 HPACK 압축방식이라 부르며 별도의 명세서(RFC 7531)로 관리하고 있다.\n위 그림처럼 클라이언트가 요청을 두 번 보낸다고 가정할 때 HTTP/1.x의 경우 헤더 중복이 발생해도 중복 전송한다. 하지만 HTTP/2에서는 헤더에 중복이 있는 경우 Static/Dynamic Header Table 개념을 이용하여 중복을 검출해내고 해당 테이블에서의 index값 + 중복되지 않은 Header 정보를 Huffman Encoding 방식으로 인코딩한 데이터를 전송한다.\n![Huffman Encoding](/img/huffman encoding.png)\n출처 나만 모르고 있던 HTTP/2 HOL 블로킹 Where should I put \u0026lt;script\u0026gt; tags in HTML markup? ", 
        "url": "\/\/localhost:1313\/posts\/http\/1.1%EA%B3%BC-http\/2%EC%9D%98-%EC%B0%A8%EC%9D%B4%EC%A0%90\/"
    },
    
    "\/\/localhost:1313\/tags\/http1.1\/": {
        "title": "Http1.1",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/http1.1\/"
    },
    
    "\/\/localhost:1313\/tags\/http2\/": {
        "title": "Http2",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/http2\/"
    },
    
    "\/\/localhost:1313\/tags\/cors\/": {
        "title": "Cors",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/cors\/"
    },
    
    "\/\/localhost:1313\/posts\/corscross-origin-resource-sharing\/": {
        "title": "CORS(Cross-Origin Resource Sharing)",
        "tags": ["cors",],
        "content": "CORS(Cross-Origin Resource Sharing) 웹 보안 정책 중 Same-Origin Policy는 한 출처(Origin)에서 로드된 문서나 스크립트가 다른 출처 자원과 상호작용하지 못하도록 제약한다. 언급한 출처(Origin)는 두 페이지의 프로토콜, 호스트, 포트가 같으면 동일 출처로 간주한다.\n하지만 이러한 보안 정책으로 인해 타 사이트로부터 받아오는 리소스나 웹 폰트, CDN 등의 사용에 문제가 되고 있어 CORS(Cross-Origin Resource Sharing)이라는 추가 정책이 나오게 되었다.\nCORS 요청 CORS 요청에는 Simple/Preflight, Credential/Non-Credential의 조합으로 총 4가지 요청이 존재한다. 브라우저가 요청 내용을 분석하여 4가지 방식 중 해당하는 방식으로 서버에 요청을 날리므로 프로그래머가 목적에 맞는 방식을 선택해 그 조건에 맞게 코딩해야 한다.\nSimple Requests 몇몇 요청(Request)들은 CORS preflight를 트리거하지 않는다. MDN 자료와 티맥스 출처자료에서는 이를 두고 Simple Requests라고 구분하지만 CORS를 정의한 실제 Fetch 스펙에서는 Simple Requests라는 용어를 사용하지 않는다. CORS preflight를 트리거 하지 않는 요청(편의상 MDN에서 \u0026lsquo;simple requests\u0026rsquo;라고 명명했던)은 아래의 조건들을 모두 만족하는 요청을 가리킨다.\nGET/POST/HEAD 메서드만을 사용해야 한다. User Agent에 의해 자동으로 설정된 헤더, Fetch 스펙에서 \u0026ldquo;forbidden header name\u0026quot;이라고 정의된 헤더들을 제외하고 \u0026ldquo;CORS-safelisted request-header\u0026rdquo; 라고 Fetch 스펙에 정의된 아래의 헤더만이 직접적으로 요청 안에 설정될 수 있다. Accept Accept-Language Content-Language Content-Type DPR Downlink Save-Data Viewport-Width Width Content-Type이 아래 중 하나여야 한다. application/x-www-form/urlencoded multipart/form-data text/plain (따로 지정하지 않을 시에 default) Request 안에 ReadableStream 객체가 없어야 한다. 요청 안에 있는 XMLHttpRequestUpload 객체에 대한 이벤트 리스너가 없어야 한다. (해당 객체는 XMLHttpRequest.upload 프로퍼티를 이용해 접근 가능하다.) 이러한 Simple Request 방식에서 클라이언트는 서버로 요청을 한 번 보내고, 마찬가지로 서버도 회신을 한 번 보내는 것으로 요청에 대한 응답이 종료된다.\n아래는 Simple requests를 사용하는 자바스크립트 예제이다. 아래 코드가 http://foo.example 서버로부터 제공되어 http://bar.other라는 외부 도메인으로부터 리소스를 받아오려 한다는 상황을 가정해보자.\nvar invocation = new XMLHttpRequest(); var url = \u0026#39;http://bar.other/resources/public-data/\u0026#39;; function callOtherDomain() { if(invocation) { invocation.open(\u0026#39;GET\u0026#39;, url, true); invocation.onreadystatechange = handler; invocation.send(); } } 위 코드를 통해 웹 브라우저가 서버로 Request를 보내고 서버로부터 Response를 받는 과정을 아래와 같이 간략하게 나타낼 수 있다.\n이 때, 실제 Request와 Response가 어떤 식으로 오고 가는지 아래 텍스트를 보자. Request의 Origin, Response의 Access-Control-Allow-Origin 부분을 중심으로 살펴보자.\nGET /resources/public-data/ HTTP/1.1 Host: bar.other User-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.5; en-US; rv:1.9.1b3pre) Gecko/20081130 Minefield/3.1b3pre Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8 Accept-Language: en-us,en;q=0.5 Accept-Encoding: gzip,deflate Accept-Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.7 Connection: keep-alive Referer: http://foo.example/examples/access-control/simpleXSInvocation.html Origin: http://foo.example 먼저, 위는 예제 코드가 서버로 보내는 Request를 나타내며 Origin 헤더부분은 컨텐츠가 http://foo.example로부터 오는 것이라고 서버에게 알리는 역할을 한다.\nHTTP/1.1 200 OK Date: Mon, 01 Dec 2008 00:23:53 GMT Server: Apache/2.0.61 Access-Control-Allow-Origin: * Keep-Alive: timeout=2, max=100 Connection: Keep-Alive Transfer-Encoding: chunked Content-Type: application/xml 위는 서버가 클라이언트로 보내는 Response를 나타내며, Access-Control-Allow-Origin: *은 모든 도메인의 cross-site 방식으로부터 액세스가 가능하다고 클라이언트에게 알리는 역할을 한다. 하지만 만약 이 헤더가\nAccess-Control-Allow-Origin: http://foo.example 처럼 왔다면 http://foo.example을 제외한 다른 도메인들에서는 cross-site 방식으로 해당 리소스에 접근할 수 없다는 것을 의미한다.\nPreflighted Requests Simple Requests와 다르게 \u0026ldquo;preflighted\u0026rdquo; requests(사전 전달 요청)는 먼저 OPTIONS 메서드를 이용하여 HTTP request를 먼저 보내 실제 요청이 보내기에 안전한지 확인한다. 아래 조건들 중 하나라도 만족하면 Preflighted Requests로 간주한다.\nRequest가 아래 메서드를 사용한다. PUT DELETE CONNECT OPTIONS TRACE PATCH Simple requests와 마찬가지로 User Agent의 자동 설정된 헤더를 제외하고, \u0026ldquo;CORS-safelisted request-header\u0026quot;를 포함한다. Accept Accept-Language Content-Language Content-Type DPR Downlink Save-Data Viewport-Width Width Content-Type 헤더 값이 아래를 제외한 다른 값인 경우 application/x-www-form-url multipart/form-data text/plain Request 안에 있는 XMLHttpRequestUpload 객체에 한 개 이상의 이벤트 리스너가 등록된 경우 ReadableStream이 Request 안에서 사용된 경우 아래는 preflighted 요청을 위한 자바스크립트 예제이다.\nvar invocation = new XMLHttpRequest(); var url = \u0026#39;http://bar.other/resources/post-here/\u0026#39;; var body = \u0026#39;\u0026lt;?xml version=\u0026#34;1.0\u0026#34;?\u0026gt;\u0026lt;person\u0026gt;\u0026lt;name\u0026gt;Arun\u0026lt;/name\u0026gt;\u0026lt;/person\u0026gt;\u0026#39;; function callOtherDomain(){ if(invocation) { invocation.open(\u0026#39;POST\u0026#39;, url, true); invocation.setRequestHeader(\u0026#39;X-PINGOTHER\u0026#39;, \u0026#39;pingpong\u0026#39;); invocation.setRequestHeader(\u0026#39;Content-Type\u0026#39;, \u0026#39;application/xml\u0026#39;); invocation.onreadystatechange = handler; invocation.send(body); } } 위 예제 코드에서는 XML body를 보내기 위해 POST 방식을 사용하고 X-PINGOTHER: pingpong이라는 customized request 헤더를 사용했다. 또한, application/xml Content-Type을 사용함으로써 위에서 명시된 3가지 Content-Type 외에 해당하여 해당 request가 preflighted 타입이라는 것을 알 수 있다.\n이제, 이 preflighted request가 서버로 보내질 때 어떤 식으로 요청과 응답이 오고가는지 아래 그림을 통해 개괄적으로 살펴보자.\n위 그림에서 주의해야할 것은 아래의 실제 REQUEST/REPONSE 코드에서 보겠지만 실제 POST request 안에는 Access-Control-Request-* 헤더가 없다는 점이다. 해당 헤더들은 모두 OPTIONS request에서만 필요하다. 또한, preflighted request의 경우 메인 Request를 보내기 전에 Preflighted Request를 한번 더 보낸다는 점이 주의하자.\n아래는 위 다이어그램에 대한 실제 Request \u0026amp; Response 내용이다.\nOPTIONS /resources/post-here/ HTTP/1.1 Host: bar.other User-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.5; en-US; rv:1.9.1b3pre) Gecko/20081130 Minefield/3.1b3pre Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8 Accept-Language: en-us,en;q=0.5 Accept-Encoding: gzip,deflate Accept-Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.7 Connection: keep-alive Origin: http://foo.example Access-Control-Request-Method: POST Access-Control-Request-Headers: X-PINGOTHER, Content-Type HTTP/1.1 200 OK Date: Mon, 01 Dec 2008 01:15:39 GMT Server: Apache/2.0.61 (Unix) Access-Control-Allow-Origin: http://foo.example Access-Control-Allow-Methods: POST, GET Access-Control-Allow-Headers: X-PINGOTHER, Content-Type Access-Control-Max-Age: 86400 Vary: Accept-Encoding, Origin Content-Encoding: gzip Content-Length: 0 Keep-Alive: timeout=2, max=100 Connection: Keep-Alive Content-Type: text/plain 먼저, preflighted request와 그 응답에 대해 살펴보자. Access-Control-Request-Method 해더는 서버에게 실제 Request가 보내졌을 때 해당 Request의 메서드와 X-PINGOTHER, Content-Type 등의 custom header들을 함께 전송할 것이라고 미리 알린다. 서버는 클라이언트로부터 이러한 정보를 미리 preflighted request를 통해 전달받고 실제 request를 받을 것인지를 결정한 뒤 알려준다. 위에 나타난 Reponse 코드 중 유심해야할 부분은 다음과 같다.\nAccess-Control-Allow-Origin: http://foo.example Access-Control-Allow-Methods: POST, GET Access-Control-Allow-Headers: X-PINGOTHER, Content-Type Access-Control-Max-Age: 86400 서버는 preflighted request에 대한 응답을 통해 클라이언트로 사용 가능한 메서드와 헤더, 그리고 해당 리소스 접근을 위해 허용된 origin을 http://foo.example로 제한하여 보내주고 있다. 마지막으로 Access-Control-Max-Age는 해당 reponse가 또다른 preflight request를 보내지 않고 얼마 동안 캐시되어 있는지를 클라이언트에게 알려주는 역할을 한다. 여기서 86400은 86400초를 나타내어 24시간동안 cached response가 유효하다고 알린다.\nPOST /resources/post-here/ HTTP/1.1 Host: bar.other User-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.5; en-US; rv:1.9.1b3pre) Gecko/20081130 Minefield/3.1b3pre Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8 Accept-Language: en-us,en;q=0.5 Accept-Encoding: gzip,deflate Accept-Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.7 Connection: keep-alive X-PINGOTHER: pingpong Content-Type: text/xml; charset=UTF-8 Referer: http://foo.example/examples/preflightInvocation.html Content-Length: 55 Origin: http://foo.example Pragma: no-cache Cache-Control: no-cache \u0026lt;?xml version=\u0026#34;1.0\u0026#34;?\u0026gt;\u0026lt;person\u0026gt;\u0026lt;name\u0026gt;Arun\u0026lt;/name\u0026gt;\u0026lt;/person\u0026gt; HTTP/1.1 200 OK Date: Mon, 01 Dec 2008 01:15:40 GMT Server: Apache/2.0.61 (Unix) Access-Control-Allow-Origin: http://foo.example Vary: Accept-Encoding, Origin Content-Encoding: gzip Content-Length: 235 Keep-Alive: timeout=2, max=99 Connection: Keep-Alive Content-Type: text/plain [Some GZIP\u0026#39;d payload] Requests with credentials 마지막으로 Credential, Non-Credential을 구분할 CORS Request 종류에 대해 기술한다. 이 \u0026ldquo;credentialed\u0026rdquo; requests는 HTTP Cookie와 HTTP Authentication information의 취약점에 대비하여 만들어진 request 타입이다. 기본으로 웹 브라우저는 cross-site XMLHttpRequest와 Fetch invocation에서 credential을 보내지 않는다.\n아래 코드를 예로 들어보자. http://bar.other/로부터 받은 컨텐츠가 쿠키를 설정하는 리소스라고 가정하고, 아래 자바스크립트 코드가 http://foo.example내에서 동작하는 코드라고 생각하자.\nvar invocation = new XMLHttpRequest(); var url = \u0026#39;http://bar.other/resources/credentialed-content/\u0026#39;; function callOtherDomain(){ if(invocation) { invocation.open(\u0026#39;GET\u0026#39;, url, true); invocation.withCredentials = true; invocation.onreadystatechange = handler; invocation.send(); } } 위처럼 새로 가져올 컨텐츠에서 캐시를 설정하는 등의 행위를 할 때 반드시 withCredentials를 설정해줘야 하며, 해당 헤더가 설정되었다면 웹 브라우저는 서버로부터 받은 response 안에 Access-Control-Allow-Credentials: true가 없는 경우는 모두 거절해버린다.\n출처 CORS in MDN CORS(Cross-Origin Resource Sharing) 및 관련 내용 - Tmaxsoft ", 
        "url": "\/\/localhost:1313\/posts\/corscross-origin-resource-sharing\/"
    },
    
    "\/\/localhost:1313\/tags\/cdn\/": {
        "title": "Cdn",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/cdn\/"
    },
    
    "\/\/localhost:1313\/posts\/%EC%BB%A8%ED%85%90%EC%B8%A0-%EC%A0%84%EC%86%A1-%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC-cdncontent-delivery-network\/": {
        "title": "컨텐츠 전송 네트워크, CDN(Content Delivery Network)",
        "tags": ["cdn",],
        "content": "CDN(Content Delivery Network) Content Delivery Network 또는 Content Distribution Network라고 불리는 네트워크는 컨텐츠를 효율적으로 전달하기 위해 여러 노드를 가진 네트워크에 데이터를 저장하여 제공하는 시스템을 말한다. 관련된 질문으로는 \u0026lsquo;CDN vs. Cache, 무엇이 더 효율적인가?\u0026lsquo;라는 것이 있다. CDN은 오늘날 텐스트, 그래픽, 스크립트, 미디어 파일, 소프트웨어, 문서 등의 다운로드가 가능한 객체들 뿐만 아니라 어플리케이션, 라이브 스트리밍 미디어 등의 다양한 컨텐츠들을 망라하여 제공하고 있다.\n아래의 그림을 살펴보자.\n왼쪽의 그림은 컨텐츠가 End User들에게 직접 제공되는 반면에 오른쪽 그림은 End-User에서 가장 가까운 CDN 서버를 통해 간접적으로 전달된다. 즉, 사용자가 인터넷 상에서 가장 가까운 곳의 서버로 컨텐츠를 전송받아 트래픽이 특정 서버로 집중되지 않고 각 서버로 분산되도록 하는 기술이다. 때문에 외쪽 그림처럼 하나의 서버만 사용할 경우 모든 클라이언트의 요청이 한 곳으로 집중될 뿐만 아니라 클라이언트와 서버 위치와의 거리에 따라 네트워크 지연속도는 비례하기 때문에 응답이 늦어질 수 있다. 하지만 CDN을 경유하게 되면 지역적으로 먼 곳이 아닌 가까운 네트워크에 요청하기 때문에 물리적인 이유로 발생하는 네트워크 속도 지연과 같은 문제를 해결할 수 있다.\n\u0026lt;script\u0026gt; 태그와 CDN JQuery 를 사용할 때, 아래처럼 URL을 이용하여 JQuery를 로드하여 사용하거나 자신의 서버에 올려사용한다. 보통 아래와 같이 URL을 통해 로드하는 방식을 CDN 방식이라 일컫는데, 제공하고자 하는 컨텐츠(스크립트의 일부)를 외부 노드로부터 로드하는 방식이므로 CDN이라 할 수 있겠다.\n\u0026lt;script src=\u0026#34;//ajax.googleapis.com/ajax/libs/jquery/1.8.2/jquery.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; ", 
        "url": "\/\/localhost:1313\/posts\/%EC%BB%A8%ED%85%90%EC%B8%A0-%EC%A0%84%EC%86%A1-%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC-cdncontent-delivery-network\/"
    },
    
    "\/\/localhost:1313\/tags\/host-header\/": {
        "title": "Host Header",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/host-header\/"
    },
    
    "\/\/localhost:1313\/tags\/virtual-host\/": {
        "title": "Virtual Host",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/virtual-host\/"
    },
    
    "\/\/localhost:1313\/posts\/%EA%B0%80%EC%83%81%ED%98%B8%EC%8A%A4%ED%8A%B8virtual-host%EC%99%80-host-header\/": {
        "title": "가상호스트(Virtual Host)와 Host Header",
        "tags": ["virtual host","host header",],
        "content": "Virtual Host and Host Header 문득 HTTP/2에 대한 책을 읽다가 가상호스트(Virtual host)와 호스트 헤더(Host header)에 대한 얘기가 나왔다. 대체 가상 호스트와 호스트 헤더 간에는 무슨 관계가 있는 것일까. 이 문서에서는 가상 호스트와 가상호스트 종류, 그리고 마지막으로 호스트 헤더와 어떤 식으로 연관되는지에 대해 설명하겠다.\n가상 호스트(Virtual Host) 가상호스트란 싱글 서버(또는 서버 풀)에서 여러 개의 도메인 이름으로 호스팅하기 위한 방법이다. 가상 호스트를 사용함으로써 얻는 이점은 하나의 서버로 해당 서버의 리소스(메모리, 프로세서 사이클 등)를 공유할 수 있다는 점이다. 이러한 특징을 잘 살린 것이 Shared Web Hosting이라는 서비스이다. 서버 한 대를 통째로 소비자에게 빌려주는 방식이 아닌 서버 하나에 가상 호스트를 이용하여 여러 개의 웹 서비스를 구축하는 방식이다.\n가상 호스트에는 Name-based 방식과 IP-based 방식 등 두 가지 방식이 있다.\n이름 기반(Name-based) Name-based 방식의 가상 호스트는 같은 IP 주소를 가지고 여러 개의 호스트명을 가진다. 이 때, 기술적인 요구사항은 HTTP/1.1 프로토콜에서의 REQUEST 메시지 안에 들어가 있는 호스트 헤더이다. 서버는 클라이언트가 보낸 요청의 호스트 헤더를 통해 어떤 가상 호스트로 요청을 전달해야 할지를 파악할 수 있기 때문이다.\n참고로, 호스트 헤더의 문법은 다음과 같다.\nHost: \u0026lt;host\u0026gt;:\u0026lt;port\u0026gt; 아래는 실제 Request 안에 들어있는 호스트 헤더 예시이다.\nGET /pub/WWW/TheProject.html HTTP/1.1 Host: www.w3.org 하지만 이러한 Name-based 방식은 한 가지 문제점이 있는데 SSL/TLS를 사용하는 HTTPS 기반의 웹 사이트들의 경우에는 이러한 가상 호스트를 적용하기 힘들다는 것이다. 왜냐하면 SSL/TLS handshake를 위해서 인증서를 사용하게 되는데 서버 입장에서는 클라이언트가 보내는 encrypted 된 handshake 단계의 패킷들이 어떠한 서비스의 인증서인지 알 길이 없다. 때문에 이러한 문제를 해결하기 위해 [SNI(Server Name Indication)](#/wiki/detail/Server Name Indication(SNI))이라는 TLS의 확장이 나오게 되었는데 이를 이용하여 handshake 시작부터 서버의 어떤 호스트를 가리키는지 이름을 사용하여 문제를 해결할 수 있게 되었다.\n또 한가지 문제점은 DNS(Domain Name System)이 제대로 설정되지 않았을 때, 서버의 IP 주소를 알고 있다고 해도 가상 호스트 웹사이트에 접속할 수 없는 문제가 생긴다. 특정 도메인으로 접속하여 fall back이 발생하면 웹 브라우저는 IP 주소를 호스트이름으로 보내기 때문이다. 웹 서버가 웹 브라우저가 어떤 서버 이름(vhost)을 사용할 것인지를 요청으로 서버에게 알리는 방식이기 때문에 DNS로 인해 fallback이 발생하는 상황까지 서버에서 전부 처리해줄 수 없다. 이러한 문제의 해결 방법은 클라이언트 시스템의 호스트 파일에 서버 IP 주소와 호스트 이름을 함께 직접 추가해주는 것이다.\nIP 주소 기반(IP-based) IP 기반의 가상호스팅 환경에서 각 사이트들은 고유한 IP주소를 갖는다. 이 때 웹 서버 환경은 물리적 네트워크 인터페이스를 여러개 갖고 있거나, 하나의 물리 네트워크 인터페이스 위에 여러 개의 가상 네트워크 인터페이스를 갖고 있거나, 하나의 물리 인터페이스가 여러 개의 아이피 주소를 갖고 있는 경우 중 하나에 해당한다. 또한 웹 서버는 각 IP 주소별로 LISTENING 소켓을 갖고 있거나 하나의 소켓만으로 모든 인터페이스를 LISTEN하도록 설정할 수도 있다. Name-based 방식과는 다르게 클라이언트가 어떤 가상 호스트를 선택할 것인지 결정하는 과정이 없기 때문에 Compatibility 이슈는 없다. 하지만 각 웹 사이트들마다 서로 다른 IP 주소가 필요하고 이에 따라 사이트 관리 오버헤드가 증가하게 되는 것이 단점이다.\n포트 기반(Port-based) 기본 HTTP 포트는 80번이다. 하지만 LISTEN 포트를 80번에서 다른 번호로 바꿈으로써 하나의 IP 주소로 여러 개의 웹 사이트를 명시적으로 운영할 수 있다.\n출처 위키피디아 - 가상호스트 검색 결과 ", 
        "url": "\/\/localhost:1313\/posts\/%EA%B0%80%EC%83%81%ED%98%B8%EC%8A%A4%ED%8A%B8virtual-host%EC%99%80-host-header\/"
    },
    
    "\/\/localhost:1313\/posts\/server-name-indicationsni\/": {
        "title": "Server Name Indication(SNI)",
        "tags": ["SNI",],
        "content": "Server Name Indication Server Name Indication(줄여서 SNI)는 SSL/TLS 기반의 HTTPS에 기반한 Name-based 가상 호스팅 환경에서 일어날 수 있는 문제점을 해결하기 위해 구현된 HTTPS의 Extension이다. 해당 문제점에 대해 구체적으로 설명하자면 Name-based 가상 호스팅 환경에서 클라이언트는 서버로 어떤 vhost를 사용할 것인지 Request 메시지에 같이 보낸다. 이 때, 사용하는 프로토콜이 HTTP가 아닌 HTTPS라면 서버는 클라이언트로 Server Hello 패킷을 보낼 때 패킷 내에 인증서 데이터를 함께 전송한다. 클라이언트가 다시 서버로 Client Hello를 보낼 때 클라이언트는 서버에서 보내준 인증서의 공개키로 암호화하여 패킷을 보내게 되는데 서버 입장에서는 클라이언트가 사용한 인증서가 어떤 가상 호스트에서 보낸 인증서인지 알 길이 없다. 이러한 문제를 해결하기 위해 Server Name Indication(SNI)라는 Extension을 사용하여 아래와 같이 문제를 해결할 수 있다.\n서버에서 SNI 설정 방법(예시. Apache 사용) SNI 설정을 위해서는 두 가지 방법이 있다. (설정 파일 변경 방법)\n첫 번째는 하나의 가상 호스트만 정의하고 나머지는 SSLSNIMap을 통해 호스트 이름과 인증서 레이블을 맵핑하는 방법이다.\n\u0026lt;virtualhost *:443\u0026gt; ServerName example.com SSLEnable SNI SSLServerCert default SSLSNIMap a.example.com sni1-rsa SSLSNIMap a.example.com sni1-ecc SSLSNIMap b.example.com sni2 \u0026lt;/virtualhost\u0026gt; 두 번째는 가상호스트에 맵핑할 호스트명과 인증서 레이블을 각각 나누어 명시해주는 방법이다.\n\u0026lt;virtualhost *:443\u0026gt; ServerName example.com SSLEnable SNI \u0026lt;/virtualhost\u0026gt; \u0026lt;virtualhost *:443\u0026gt; ServerName a.example.com SSLEnable SSLServerCert sni1 \u0026lt;/virtualhost\u0026gt; \u0026lt;virtualhost *:443\u0026gt; ServerName b.example.com ServerAlias other.example.com SSLEnable SSLServerCert sni2 \u0026lt;/virtualhost\u0026gt; 실제 SNI 패킷 const unsigned char good_data_2[] = { // TLS record 0x16, // Content Type: Handshake 0x03, 0x01, // Version: TLS 1.0 0x00, 0x6c, // Length (use for bounds checking) // Handshake 0x01, // Handshake Type: Client Hello 0x00, 0x00, 0x68, // Length (use for bounds checking) 0x03, 0x03, // Version: TLS 1.2 // Random (32 bytes fixed length) 0xb6, 0xb2, 0x6a, 0xfb, 0x55, 0x5e, 0x03, 0xd5, 0x65, 0xa3, 0x6a, 0xf0, 0x5e, 0xa5, 0x43, 0x02, 0x93, 0xb9, 0x59, 0xa7, 0x54, 0xc3, 0xdd, 0x78, 0x57, 0x58, 0x34, 0xc5, 0x82, 0xfd, 0x53, 0xd1, 0x00, // Session ID Length (skip past this much) 0x00, 0x04, // Cipher Suites Length (skip past this much) 0x00, 0x01, // NULL-MD5 0x00, 0xff, // RENEGOTIATION INFO SCSV 0x01, // Compression Methods Length (skip past this much) 0x00, // NULL 0x00, 0x3b, // Extensions Length (use for bounds checking) // Extension 0x00, 0x00, // Extension Type: Server Name (check extension type) 0x00, 0x0e, // Length (use for bounds checking) /* SNI 설정 부분: 아래에서 \u0026#34;localhost\u0026#34;라고 서버의 이름을 직접 명시한다. */ 0x00, 0x0c, // Server Name Indication Length 0x00, // Server Name Type: host_name (check server name type) 0x00, 0x09, // Length (length of your data) // \u0026#34;localhost\u0026#34; (data your after) 0x6c, 0x6f, 0x63, 0x61, 0x6c, 0x68, 0x6f, 0x73, 0x74, // Extension 0x00, 0x0d, // Extension Type: Signature Algorithms (check extension type) 0x00, 0x20, // Length (skip past since this is the wrong extension) // Data 0x00, 0x1e, 0x06, 0x01, 0x06, 0x02, 0x06, 0x03, 0x05, 0x01, 0x05, 0x02, 0x05, 0x03, 0x04, 0x01, 0x04, 0x02, 0x04, 0x03, 0x03, 0x01, 0x03, 0x02, 0x03, 0x03, 0x02, 0x01, 0x02, 0x02, 0x02, 0x03, // Extension 0x00, 0x0f, // Extension Type: Heart Beat (check extension type) 0x00, 0x01, // Length (skip past since this is the wrong extension) 0x01 // Mode: Peer allows to send requests }; 출처 SNI(Server Name Indication) - IBM Knowledge Center Extract Server Name Indication (SNI) from TLS client hello ", 
        "url": "\/\/localhost:1313\/posts\/server-name-indicationsni\/"
    },
    
    "\/\/localhost:1313\/tags\/sni\/": {
        "title": "SNI",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/sni\/"
    },
    
    "\/\/localhost:1313\/tags\/http\/": {
        "title": "Http",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/http\/"
    },
    
    "\/\/localhost:1313\/tags\/https\/": {
        "title": "Https",
        "tags": [],
        "content": "", 
        "url": "\/\/localhost:1313\/tags\/https\/"
    },
    
    "\/\/localhost:1313\/posts\/http%EC%99%80-https%EC%9D%98-%EC%B0%A8%EC%9D%B4%EC%A0%90\/": {
        "title": "HTTP와 HTTPS의 차이점",
        "tags": ["http","https",],
        "content": "HTTP와 HTTPS의 차이점 HTTP와 HTTPS의 차이점은 그 용어에서부터 단번에 알아챌 수 있다. 바로 끝 글자 \u0026lsquo;S\u0026rsquo;가 나타내는 Secure라는 의미로부터 HTTP에 \u0026lsquo;어떤 것\u0026rsquo;을 추가하여 보안을 강화한 프로토콜이라는 것을 짐작할 수 있다. 여기서 중요한 것은 HTTP에 추가한\u0026rsquo;어떤 것\u0026rsquo;인데 이 것이 바로 TLS(SSL라고도 부름)가 되겠다. HTTPS를 이해하기 위해서는 TLS에 대해 알아야 하므로 이 문서에서는 TLS에 대한 기본적인 개념과 HTTPS가 등장하게 된 배경에 대한 설명으로 HTTP와 HTTPS의 차이점을 설명하도록 하겠다. 먼저, HTTP, HTTPS와 TLS 프로토콜부터 살펴보자.\nHTTP(HyperText Transfer Protocol)와 HTTPS(HyperText Transfer Protocol Secure) HTTP(HyperText Transfer Protocol) HTTP는 웹 브라우저와 같은 클라이언트가 웹 서버에 요청하는 프로토콜 중 하나로서 암호화 되어 있지 않은 Plain 텍스트 형태의 메시지를 주고받도록 구현되어 있다. 때문에 클라이언트와 서버 간의 데이터를 가로챈다면 해당 내용을 읽을 수 있는 취약점을 가지고 있다.\nTLS(Transport Layer Security) HTTPS를 설명하기에 앞서 TLS 프로토콜에 대한 설명이 필요하다. TLS 프로토콜은 데이터를 암호화해서 송수신하는 프로토콜로서 넷스케이프가 개발한 SSL에 기반한 기술이다. 기반 기술인 SSL은 넷스케이프에서 1995년 2.0 버전으로 처음 공개하였다(1.0은 애초에 공개되지도 않았다). 하지만 몇 가지 취약점들이 발견되면서 넷츠케이프는 1년 만인 1996년에 다시 SSL 3.0을 공개하였다. (SSL 2.0, SSL 3.0은 SSLv2, SSLv3으로 언급되기도 한다.) 그리고 1996년도 버전인 SSLv3을 기반으로 하여 1999년에 표준으로 공개한 것이 TLS 1.0이다. 즉, TLS 1.0은 SSL 3.1이라고 생각하면 이해하기 쉽다.\n실제로 TLS 1.0과 SSL 3.0의 차이는 많지 않지만 중요한 점이 이 둘이 상호운용이 안된다는 점이다. 그렇다면 둘 중 어떤 것을 써야 하는가에 대한 질문이 나올 수 있는데, 여기에 대한 답은 \u0026lsquo;TLS를 사용해야 한다\u0026rsquo;이다. SSLv2와 SSLv3은 각각 2011년, 2015년 IETF에서 depcrecated 되었으며 매년 POODLE, DROWN과 같은 취약점이 발견되고 있다. 최근 웹 브라우저들은 특정 서버에서 이러한 deprecated된 프로토콜을 사용하고 있다면 알아서 보안 경고를 내보내도록 구현되어 있다.\n이러한 가운데 대다수는 아니지만 여전히 많은 사람들이 TLS 대신에 SSL이라는 용어를 혼용해서 사용하고 있다. 이 때문에 많은 곳에서 TLS 대신 \u0026lsquo;SSL/TLS\u0026rsquo; 라고 표기하고 있는데 서버 설정 시에는 반드시 이 둘을 구분하여 deprecated된 SSL은 비활성화하고 TLS만 활성화하도록 해야한다.\nHTTPS HTTPS의 S는 Over Secure Socket Layer의 약자이다. 즉, HTTP 프로토콜에서 보안이 강화된 버전(HTTP + TLS)으로 이해하면 간단하다. 때문에 클라이언트와 서버 간의 데이터를 중간에서 가로챈다고 해도 Plain 텍스트가 아니기 때문에 제3자가 곧바로 내용을 확인할 수 없다.\n그렇다면 두 프로토콜의 차이점인 암호화 프로토콜인 TLS는 어떤 방식으로 이루어지는 것일까? 아래는 TLS 방식으로 클라이언트와 서버가 핸드쉐이킹하는 과정을 간략하게 설명한 것이다.\n클라이언트(웹 브라우저)가 웹 서버에 접속한다. 서버는 클라이언트에 인증서를 제공한다. 클라이언트는 서버로부터 받은 인증서가 신뢰할 수 있는 인증서인지 검증한다. 인증서가 신뢰할 수 있다면, 클라이언트는 서버로 pre master secret key 값을 암호화하여 전송한다. 서버는 클라이언트로부터 받은 pre master secret key을 복호화한다. 클라이언트와 서버는 동일한 pre master secret key를 가지고 master secret key를 만들어 세션 키를 공유한다. 인증서란 무엇이고 클라이언트와 서버는 어떻게 서로 암호화/복호화를 할 수 있는 것일까? 이를 위해서는 공개키와 대칭키의 개념부터 알아야 한다. HTTPS 프로토콜에서 서버와 클라이언트 간의 주고받는 모든 데이터들이 공개키와 대칭키를 응용한 것들이기 때문이다.\n공개키(Public-Key Cryptosystem)와 대칭키(Symmetric-Key Cryptosystem) 대칭키(Symmetric-Key Cryptosystem) 암호를 만드는 행위인 암호화를 할 때 사용하는 비밀번호를 키(key)라고 한다. 이 키에 따라 암호화 결과가 달라지기 때문에 키를 정확하게 알지 않는 한 복호화가 불가능하다. 쉽게 설명하면 동일한 키로 암호화 또는 복호화를 할 수 있으므로 어떤 값을 암호화 할 때 1234라는 값을 사용했다면 복호화를 할 때도 1234라는 값을 입력해야 한다는 것이다. 아래는 이러한 공개키를 실제로 사용하는 예시이다.\n$ echo \u0026#39;this is the plain text\u0026#39; \u0026gt; plaintext.txt $ openssl enc -e -des3 -salt -in plaintext.txt -out ciphertext.bin enter des-ede3-cbc encryption password: Verifying - enter des-ede3-cbc encryption password: $ openssl enc -d -des3 -in ciphertext.bin -out plaintext2.txt enter des-ede3-cbc decryption password: $ diff plaintext.txt plaintext2.txt # 서로 같음 $ # 두 파일의 차이점이 없음 공개키(Public-Key Cryptosystem) 대칭키 방식의 경우 키가 한 번 유출되면 키를 획득한 공격자가 암호 내용을 복호화할 수 있다는 취약점 때문에 노드 간에 온라인으로 키를 주고받기가 어렵다. 이러한 대칭키의 단점을 극복하기 위해서 만들어진 것이 바로 공개키이다.\n공개키 방식은 공개키(public key)와 비공개키(private key, 개인키라고도 부름)로 구성된다. 여기서 중요한 특징은 비밀키로 암호화를 하면 공개키로 복호화할 수 있고 반대로 공개키로 암호화하면 비밀키로 복호화할 수 있다는 점이다. 공개키 방식에서 비공개키는 자신이 가지고 있고 타인에게는 공개키를 제공한다. 이로써 비공개키를 가지고 있는 쪽에서는 비공개키를 이용하여 데이터를 암호화하여 타인에게 전달하거나, 타인이 전달한 암호화된 데이터를 비공개키를 이용하여 복호화할 수 있다.\n이러한 공개키의 특징은 서버 - 클라이언트 간의 \u0026lsquo;인증서\u0026rsquo;로서 활용될 수 있다.\n서버는 CA(Certificate Authority)라는 공인된 기관으로부터 비밀키를 발급받는다. 클라이언트가 서버로 접속했을 때 서버는 발급받은 비밀키에 대한 공개키를 클라이언트로 전송한다. 클라이언트는 서버로부터 받은 공개키를 가지고 CA라는 제 3자에게 전송하여 연결하고자 하는 서버가 신뢰할 수 있는 서버인지 검증한다. 이 과정에서 공개키가 유출된다면 공격자에 의해 데이터가 복호화될 위험이 있지만, 위의 인증 단계에서 서버가 비공개키를 이용해 암호화하는 이유가 데이터 보호가 아닌 신원 보장에 있기 때문에 문제되지 않는다. 이러한 \u0026lsquo;전자 서명\u0026rsquo;은 암호화된 데이터가 공개키를 가지고 복호화 할 수 있다는 특징을 활용해 데이터를 제공한 사람의 신원을 보장해준다.\n인증서 위에서 언급한 인증서의 역할은 클라이언트가 접속한 서버가 클라이언트가 의도한 서버가 맞는지를 보장하는 역할을 한다. 이 역할을 하는 민간 기업들이 있는데 이들을 CA(Certificate Authority) 또는 Root Certificate라고 부른다. 이들 CA 리스트는 브라우저 내부에 내장되어 있으며 브라우저가 미리 파악하고 있는 CA 리스트에 포함되어야만 공인된 인증 기관이 될 수 있다.\nTLS Handshake 이제 마지막으로 HTTPS에서 사용하는 SSL/TLS의 Handshake에 대해 살펴보자.\n![TLS Handshake](/img/TLS handshake.svg)\n클라이언트가 서버에 접속한다. 이 단계를 Client Hello라고 한다. 이 단계에서 주고 받는 데이터는 아래와 같다. 클라이언트 측에서 생성한 랜덤 데이터(Client\u0026rsquo;s random values) 클라이언트가 지원하는 암호화 방식들(Supported cipher suites): 클라이언트와 서버가 지원하는 암호화 방식이 서로 다를 수 있기 때문에 상호간에 어떤 암호화 방식을 사용할 것인지에 대한 협상을 해야 한다. 이 것을 위해 클라이언트는 서버로 자신이 사용할 수 있는 암호화 방식을 전송한다. 세션 아이디: 이미 한 번 handshake를 했다면 비용을 절약하기 위해 기존 세션을 재활용하게 되는데 이 때 사용할 연결에 대한 식별자를 서버 측으로 전송한다. 서버는 Client Hello에 대한 응답으로 Server Hello를 클라이언트로 전송한다. 이 단계에서의 송수신 데이터는 아래와 같다. 서버가 생성한 랜덤 데이터(Server\u0026rsquo;s random values) 서버가 선택한 클라이언트의 암호화 방식 인증서 서버가 클라이언트로 Server hello done 메시지를 보낸다. 클라이언트는 서버의 인증서가 CA에 의해서 발급된 것인지 확인하기 위해 클라이언트에 내장된 CA 리스트를 확인하고 리스트에 인증서가 없다면 경고 메시지를 출력한다. 클라이언트는 서버로부터 받은 랜덤 데이터와 클라이언트가 생성한 랜덤 데이터를 조합하여 pre master secret을 생성한 후 인증서에 들어 있는 공개키로 암호화한 후 서버로 전송한다. 서버는 클라이언트로부터 pre master secret을 수신하고 클라이언트와 서버 모두 공통으로 갖고 있는 pre master secret을 토대로 master secret과 session keys를 생성한다. 클라이언트는 Change cipher spec 알림을 서버로 보내 앞으로 새로운 세션 키를 이용하여 메세지 암호화/복호화를 할 것이라고 서버에 알린다. 서버는 클라이언트로부터 Change cipher spec 수신한다. 그리고 record layer security state를 session keys를 이용하는 비대칭 암호화 방식(symmetric encryption)으로 변경한다. 변경이 완료되면 클라이언트에게 Server finished 메시지를 보낸다. 이제 클라이언트와 서버는 세션키를 비대칭 방식으로 주고 받는 메시지를 암호화/복호화 할 수 있게 되었다. 세션 종료 이 후, 데이터 전송이 끝나면 SSL/TLS 통신이 끝났음을 서로에게 알리고 사용한 대칭키인 session keys를 폐기한다.\n출처 SSL vs. TLS - What\u0026rsquo;s the Difference? HTTPS와 SSL 인증서 What\u0026rsquo;s the difference between SSL, TLS, and HTTPS? TLS Handshake Protocol ", 
        "url": "\/\/localhost:1313\/posts\/http%EC%99%80-https%EC%9D%98-%EC%B0%A8%EC%9D%B4%EC%A0%90\/"
    },
    
}
</script>
<script defer src="/js/lunr.js"></script>
<script defer src="/js/lunr.stemmer.support.js"></script>
<script defer src="/js/lunr.ko.js"></script>
<script defer src="/js/search.js"></script>

</footer>

</body>
</html>

